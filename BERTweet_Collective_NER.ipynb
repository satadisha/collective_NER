{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZKMubBHEgZM"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygYoSkp_Wq-j",
        "outputId": "85dab83f-593f-4b6d-9d29-e3fe442f3229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ],
      "source": [
        "!python3 --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LQiJy6bX52T",
        "outputId": "28f4fd0f-b2d9-47f7-ed5a-46b8dec9270f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/BERTweet-ner\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "%cd gdrive/My Drive/BERTweet-ner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba-jv0cUX9R2"
      },
      "outputs": [],
      "source": [
        "!pip3 install datasets\n",
        "!pip3 install transformers\n",
        "!pip3 install -U sentence-transformers\n",
        "!pip3 install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w4J4mdnYD2o"
      },
      "outputs": [],
      "source": [
        "!pip3 install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht5nhSRpgxn4"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_D9peakfkT9",
        "outputId": "e2b0db3c-394c-4d55-b44b-e7348d7f3495"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.cuda.amp.autocast_mode.autocast at 0x7f43c39d4610>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import copy\n",
        "\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "torch.cuda.amp.autocast(enabled=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usDw-ReUiklS"
      },
      "source": [
        "## **Setting some global stuff**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lZKZmPKizRT",
        "outputId": "885d187e-16ac-4b04-ab5d-519db6cf4428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Set device\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-uyPRRBJmEe"
      },
      "outputs": [],
      "source": [
        "# example_2D_list = [[0.2423,-0.6679,0.8277],[0.2423,-0.6679,0.8277],[0.2423,-0.6679,0.8277]]\n",
        "# twodTensor = torch.tensor(example_2D_list)\n",
        "\n",
        "# example_1D_list = [0.2245,-2.2268,0.4577]\n",
        "# origTensor = torch.tensor(example_1D_list)\n",
        "\n",
        "# # modifiedTensor = origTensor.repeat(3, 1)\n",
        "\n",
        "# # print(twodTensor.size(),origTensor.size())\n",
        "# # print(modifiedTensor)\n",
        "\n",
        "# cosine_layer = torch.nn.CosineSimilarity(dim=1)\n",
        "# output = cosine_layer(origTensor, twodTensor)\n",
        "# print(output)\n",
        "# print(torch.mul(output, -1/100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1_FuBfIU9E5"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "class PhraseEmbeddingI(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size,output_size,device,mentions_dict,index_candidate_dict):\n",
        "        super(PhraseEmbeddingI, self).__init__()\n",
        "        self.print_once=True\n",
        "        self.dense_layer = nn.Linear(input_size,output_size)\n",
        "        self.batchnorm = nn.BatchNorm1d(output_size)\n",
        "        self.non_linear_layer = nn.LeakyReLU() #nn.Tanh()\n",
        "        self.cosine_layer = nn.CosineSimilarity(dim=1)\n",
        "        self.device = device\n",
        "        self.mentions_dict_train = mentions_dict\n",
        "        self.mentions_embedding_dict_train = {}\n",
        "        self.index_candidate_dict_train = index_candidate_dict\n",
        "        self.temperature = 2\n",
        "        return\n",
        "\n",
        "    def encode(self, input_embedding):\n",
        "\n",
        "        # print(input_embedding.size())\n",
        "\n",
        "        #Average Pooling has already been done before\n",
        "        input_embedding = input_embedding.to(device=self.device)\n",
        "        x = self.dense_layer(input_embedding)\n",
        "        x = self.batchnorm(x)\n",
        "        out = self.non_linear_layer(x)\n",
        "        # print(out.size())\n",
        "        return out\n",
        "\n",
        "    def compute(self, input_tuple):\n",
        "        # Soft Nearest Neighbours\n",
        "        # print(input_tuple)\n",
        "        anchor_mention_index = input_tuple[0]\n",
        "        input_anchor = self.index_candidate_dict_train[input_tuple[1]]\n",
        "        input_negative1 = self.index_candidate_dict_train[input_tuple[2]]\n",
        "        input_negative2 = self.index_candidate_dict_train[input_tuple[3]]\n",
        "        input_negative3 = self.index_candidate_dict_train[input_tuple[4]]\n",
        "        input_negative4 = self.index_candidate_dict_train[input_tuple[5]]\n",
        "\n",
        "        # print(anchor_mention_index,input_anchor,input_negative1,input_negative2,input_negative3,input_negative4)\n",
        "\n",
        "        positive_embeddings_list = []\n",
        "        all_embeddings_list = []\n",
        "\n",
        "        anchor_embedding = self.encode(self.mentions_dict_train[input_anchor][anchor_mention_index])\n",
        "\n",
        "        positive_embeddings_list = [self.mentions_dict_train[input_anchor][ind] for ind in range(len(self.mentions_dict_train[input_anchor])) if ind != anchor_mention_index]\n",
        "        \n",
        "        all_embeddings_list += positive_embeddings_list\n",
        "        all_embeddings_list += self.mentions_dict_train[input_negative1] + self.mentions_dict_train[input_negative2] + self.mentions_dict_train[input_negative3] + self.mentions_dict_train[input_negative4]\n",
        "\n",
        "        pos_emb_tensors = self.encode(torch.stack(positive_embeddings_list).to(self.device)) #\n",
        "        all_emb_tensors = self.encode(torch.stack(all_embeddings_list).to(self.device)) #\n",
        "\n",
        "        # print(len(positive_embeddings_list),len(all_embeddings_list))\n",
        "        # print(pos_emb_tensors.size(),all_emb_tensors.size())\n",
        "\n",
        "        # #Cosine sims\n",
        "        pos_sim_tensor = torch.mul(1-self.cosine_layer(anchor_embedding, pos_emb_tensors), -1/self.temperature)\n",
        "        all_sim_tensor = torch.mul(1-self.cosine_layer(anchor_embedding, all_emb_tensors), -1/self.temperature)\n",
        "\n",
        "        ret = torch.sum(torch.exp(pos_sim_tensor))/torch.sum(torch.exp(all_sim_tensor)).to(self.device)\n",
        "        # print('==>',output_anchor.size(),output_positive.size(),output_negative.size())\n",
        "        return ret\n",
        "\n",
        "    def forward(self,batch_data):\n",
        "        output_arr = []\n",
        "        printn=0\n",
        "\n",
        "        # # print('start')\n",
        "        # for key in self.mentions_dict_train.keys():\n",
        "        #     mentions_embedding_list = []\n",
        "        #     for emb in self.mentions_dict_train[key]:\n",
        "        #         mentions_embedding_list.append(self.encode(emb))\n",
        "        #     self.mentions_embedding_dict_train[key] = mentions_embedding_list\n",
        "        # # print('end')\n",
        "\n",
        "        anchor_mention_indices = batch_data[0]\n",
        "        anchor_candidate_indices = batch_data[1]\n",
        "        neg_candidate1_indices = batch_data[2]\n",
        "        neg_candidate2_indices = batch_data[3]\n",
        "        neg_candidate3_indices = batch_data[4]\n",
        "        neg_candidate4_indices = batch_data[5]\n",
        "\n",
        "        for index in range(len(anchor_mention_indices)):\n",
        "            x_tup = (int(anchor_mention_indices[index].item()), int(anchor_candidate_indices[index].item()), int(neg_candidate1_indices[index].item()), int(neg_candidate2_indices[index].item()), int(neg_candidate3_indices[index].item()), int(neg_candidate4_indices[index].item()))\n",
        "            out = self.compute(x_tup)\n",
        "            # if(printn%10==0):\n",
        "            #     print(printn)\n",
        "            printn+=1\n",
        "            output_arr.append(out)\n",
        "        output_tensor = torch.stack(output_arr).to(self.device)\n",
        "        return output_tensor\n",
        "\n",
        "    def getEmbedding(self, input_embeddings):\n",
        "        with torch.no_grad():\n",
        "            return self.encode(input_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6thsaSRLjH0e"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "class PhraseEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size,output_size,device):\n",
        "        super(PhraseEmbedding, self).__init__()\n",
        "        self.print_once=True\n",
        "        self.dense_layer1 = nn.Linear(input_size,output_size)\n",
        "        # self.batchnorm1 = nn.BatchNorm1d(350)\n",
        "        # self.dense_layer2 = nn.Linear(350,output_size)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(output_size)\n",
        "        self.non_linear_layer = nn.Tanh()#nn.LeakyReLU() \n",
        "        self.cosine_layer = nn.CosineSimilarity(dim=1)\n",
        "        self.device = device\n",
        "        return\n",
        "\n",
        "    \n",
        "\n",
        "    def encode(self, input_embedding):\n",
        "\n",
        "        # print(input_embedding.size())\n",
        "\n",
        "        #Average Pooling has already been done before\n",
        "        input_embedding = input_embedding.to(device=self.device)\n",
        "        # print(average_pooled_embedding.size())\n",
        "        x = self.dense_layer1(input_embedding)\n",
        "        # x = self.batchnorm1(x)\n",
        "        # x = self.non_linear_layer(x)\n",
        "        # x = self.dense_layer2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        out = self.non_linear_layer(x)\n",
        "        # print(out.size())\n",
        "        return out\n",
        "\n",
        "    def forward(self, input_tuple):\n",
        "        # Triplet\n",
        "        input_anchor = input_tuple[0]\n",
        "        input_positive = input_tuple[1]\n",
        "        input_negative = input_tuple[2]\n",
        "\n",
        "        output_anchor = self.encode(input_anchor)\n",
        "        output_positive = self.encode(input_positive)\n",
        "        output_negative = self.encode(input_negative)\n",
        "\n",
        "        assert torch.isnan(output_anchor).any() == False\n",
        "        assert torch.isnan(output_positive).any() == False\n",
        "        assert torch.isnan(output_negative).any() == False\n",
        "\n",
        "        # #L2 distances\n",
        "        # positive_distance = torch.linalg.norm(output_anchor - output_positive)\n",
        "        # negative_distance = torch.linalg.norm(output_anchor - output_negative)\n",
        "\n",
        "        #Cosine distances\n",
        "        positive_distance = 1-self.cosine_layer(output_anchor, output_positive)\n",
        "        negative_distance = 1-self.cosine_layer(output_anchor, output_negative)\n",
        "\n",
        "        # print('==>',output_anchor.size(),output_positive.size(),output_negative.size())\n",
        "        return (positive_distance,negative_distance)\n",
        "\n",
        "    def getEmbedding(self, input_embeddings):\n",
        "        with torch.no_grad():\n",
        "            return self.encode(input_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz6WlopQquia"
      },
      "outputs": [],
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "    \n",
        "    def forward(self, input_tup):\n",
        "        distance_positive = input_tup[0]\n",
        "        distance_negative = input_tup[1]\n",
        "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "        # print(distance_positive.size(),losses.size())\n",
        "        return losses.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgXNBca_AM7G"
      },
      "outputs": [],
      "source": [
        "class TupletLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TupletLoss, self).__init__()\n",
        "    \n",
        "    def forward(self, input_tup):\n",
        "        expdiff1 = input_tup[0]\n",
        "        expdiff2 = input_tup[1]\n",
        "        expdiff3 = input_tup[2]\n",
        "        expdiff4 = input_tup[3]\n",
        "        expdiffsum = expdiff1 + expdiff2 + expdiff3 + expdiff4\n",
        "        losses = torch.log(torch.add(expdiffsum,1))\n",
        "        \n",
        "        # print(expdiff1.size(),expdiffsum.size(),losses.size())\n",
        "        return losses.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ9JZrFdpqyU"
      },
      "outputs": [],
      "source": [
        "class SoftKNNLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SoftKNNLoss, self).__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        losses = torch.mul(torch.log(input),-1)\n",
        "        # print(input.size(),losses.size())\n",
        "        # final_loss = Variable(final_loss, requires_grad = True)’\n",
        "        return losses.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ENBgHBFm9fb"
      },
      "outputs": [],
      "source": [
        "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "model_checkpoint = \"vinai/bertweet-base\"\n",
        "batch_size = 32\n",
        "datasets = load_dataset(\"wnut_17\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhnAJCnbjKIm"
      },
      "source": [
        "## **Initialization of some components**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "\n",
        "# from entityEmbedding import phraseEmbedding\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model, optimizer, checkpoint['epoch']"
      ],
      "metadata": {
        "id": "Yq3wS_ptXd79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNnKiVrjjQ3J"
      },
      "outputs": [],
      "source": [
        "# Initialize network\n",
        "output_embedding_size = 300\n",
        "# output_embedding_size = 768\n",
        "learning_rate = 0.001\n",
        "\n",
        "phraseEmbeddingModel = PhraseEmbedding(768, output_embedding_size, device).to(device) #triplet\n",
        "# phraseEmbeddingModel = PhraseEmbeddingI(768, output_embedding_size, device).to(device) #soft-knn\n",
        "\n",
        "#Loss and Optimizer\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOtV2RzzVgt2"
      },
      "outputs": [],
      "source": [
        "# define checkpoint saved path for entity phrase embedder\n",
        "\n",
        "# ckp_path = \"entityEmbedding/model_checkpoints_ner/checkpoint_model300.pt\" #300 triplet\n",
        "# # ckp_path = \"entityEmbedding/model_checkpoints_ner/checkpoint_model300_softknn.pt\" #300 tuplet\n",
        "\n",
        "# if(path.exists(ckp_path)):\n",
        "#     # load the saved checkpoint\n",
        "#     entityPhraseEmbedder, optimizer, start_epoch = load_ckp(ckp_path, phraseEmbeddingModel, optimizer)\n",
        "\n",
        "#     print(\"starting with model at epoch:\", start_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3e11xA_kKRa",
        "outputId": "128b23cf-b420-4f6c-915c-dcbc4d9edbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl7BOt4tkLA1"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "\n",
        "gutenberg_text = \"\"\n",
        "for file_id in gutenberg.fileids():\n",
        "    gutenberg_text += gutenberg.raw(file_id)\n",
        "tokenizer_trainer = PunktTrainer()\n",
        "tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "\n",
        "sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "sentence_tokenizer._params.abbrev_types.add('ret')\n",
        "sentence_tokenizer._params.abbrev_types.add('rep')\n",
        "sentence_tokenizer._params.abbrev_types.add('mr')\n",
        "sentence_tokenizer._params.abbrev_types.add('ms')\n",
        "sentence_tokenizer._params.abbrev_types.add('mrs')\n",
        "sentence_tokenizer._params.abbrev_types.add('v')\n",
        "sentence_tokenizer._params.abbrev_types.add('vs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5qm2iIc8zzq"
      },
      "source": [
        "## **External fine-tuning of BERTweet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5PzYXA7YglU"
      },
      "outputs": [],
      "source": [
        "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "model_checkpoint = \"vinai/bertweet-base\"\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIjXWgKNYp6x"
      },
      "outputs": [],
      "source": [
        "datasets = load_dataset(\"wnut_17\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM4XcV19Yt0l"
      },
      "outputs": [],
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNlRZ-enYyuq"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
        "label_all_tokens = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HofMbhhY3My"
      },
      "outputs": [],
      "source": [
        "expanded_label_dict={0:'O', 1:'B-corporation', 2:'I-corporation', 3:'B-creative-work', 4:'I-creative-work', 5:'B-group', 6:'I-group', 7:'B-location', 8:'I-location', 9:'B-person', 10:'I-person', 11:'B-product', 12:'I-product'}\n",
        "BIO_dict={'O':0,'B':1,'I':2}\n",
        "BIO_type_dict={'O':0, 'B-ORG':1, 'I-ORG':2, 'B-MISC':3, 'I-MISC':4, 'B-LOC':5, 'I-LOC':6, 'B-PER':7, 'I-PER':8}\n",
        "label_map_dict={'O':'O', 'B-corporation':'B-ORG', 'I-corporation':'I-ORG', 'B-creative-work':'B-MISC', 'I-creative-work':'I-MISC', 'B-group':'B-MISC', 'I-group':'I-MISC', 'B-location':'B-LOC', 'I-location':'I-LOC', 'B-person':'B-PER', 'I-person':'I-PER', 'B-product':'B-MISC', 'I-product':'I-MISC'}\n",
        "\n",
        "def tokenize_and_align_labels(example):\n",
        "        \n",
        "    tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "    inputId_to_token_dict={}\n",
        "    for index, token in enumerate(example[\"tokens\"]):\n",
        "        values=tokenizer.encode(token, add_special_tokens=False, truncation=True)\n",
        "        for value in values:\n",
        "            try:\n",
        "                inputId_to_token_dict[value].append(index)\n",
        "            except KeyError:\n",
        "                inputId_to_token_dict[value]=[index]\n",
        "    labels=[]\n",
        "    for inputID in tokenized_input['input_ids']:\n",
        "        try:\n",
        "            index_list=copy.deepcopy(inputId_to_token_dict[inputID])\n",
        "            index_to_address=index_list.pop(0)\n",
        "\n",
        "            # label=BIO_dict[expanded_label_dict[example['ner_tags'][index_to_address]][0]] #Just BIO\n",
        "            # label = example['ner_tags'][index_to_address]\n",
        "            label = BIO_type_dict[label_map_dict[expanded_label_dict[example['ner_tags'][index_to_address]]]]\n",
        "\n",
        "            labels.append(label)\n",
        "            inputId_to_token_dict[inputID]=index_list\n",
        "        except KeyError:\n",
        "            labels.append(-100)\n",
        "\n",
        "    assert (len(tokenized_input['input_ids']) == len(labels))\n",
        "    tokenized_input['labels']=labels\n",
        "    \n",
        "    return tokenized_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G81rx9xnaxr"
      },
      "outputs": [],
      "source": [
        "# tokenize_and_align_labels(datasets['train'][2])\n",
        "tokenized_datasets = datasets.map(tokenize_and_align_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86M5oot7pnfz"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50ZRrI4JqT3R"
      },
      "outputs": [],
      "source": [
        "!pip3 install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCeuNMxFqhqi"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAWz_QrzqlAU"
      },
      "outputs": [],
      "source": [
        "# label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
        "\n",
        "# label_list = ['O','B','I']\n",
        "\n",
        "label_list = ['O','B-ORG','I-ORG','B-MISC','I-MISC','B-LOC','I-LOC','B-PER','I-PER']\n",
        "print(label_list)\n",
        "print(len(label_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MmRvoZrrfwH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    # print(p.shape)\n",
        "    output, labels = p\n",
        "\n",
        "    # print(len(predictions))\n",
        "    # print(predictions[0].shape)\n",
        "    # for elem in predictions[1]:\n",
        "    #   print(elem.shape)\n",
        "\n",
        "    predictions, _ = output\n",
        "    \n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkcj6DSGrspe"
      },
      "outputs": [],
      "source": [
        "alt_model = AutoModelForTokenClassification.from_pretrained(\"vinai/bertweet-base\", output_hidden_states=True, num_labels=len(label_list))\n",
        "# alt_model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RBOsjlvsBqY"
      },
      "outputs": [],
      "source": [
        "alt_training_args = TrainingArguments(\n",
        "    f\"test-{task}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# alt_training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03zFk8aEmAkW"
      },
      "outputs": [],
      "source": [
        "# alt_training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouXBNnSqsE7g"
      },
      "outputs": [],
      "source": [
        "alt_trainer = Trainer(\n",
        "    alt_model,\n",
        "    alt_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJaSNPccsIis"
      },
      "outputs": [],
      "source": [
        "alt_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDEYZT668rNp"
      },
      "source": [
        "## **Pre-processing on custom test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4GEp-qlzNGZ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW6tkKJGIAEN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import emoji\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string\n",
        "\n",
        "string.punctuation=string.punctuation+'…‘’'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HcqhLmMIVo8"
      },
      "outputs": [],
      "source": [
        "def collate_token_labels(tweetWordList, token_dict, prediction_labels):\n",
        "    #this function is also a valid alt implementation of the one in LocalNER module\n",
        "    counter=0\n",
        "    collated_labels=[]\n",
        "    for word in tweetWordList:\n",
        "    # for key in token_dict.keys():\n",
        "        \n",
        "        vals=token_dict[word]\n",
        "        if(counter<len(prediction_labels)):\n",
        "            labels=prediction_labels[counter:counter+len(vals)]\n",
        "            boundary_labels=[label[0] for label in labels]\n",
        "            if('I' in boundary_labels):\n",
        "                label=labels[boundary_labels.index('I')]\n",
        "                collated_labels.append(label)\n",
        "            elif('B' in boundary_labels):\n",
        "                label=labels[boundary_labels.index('B')]\n",
        "                collated_labels.append(label)\n",
        "            else:\n",
        "                collated_labels.append('O')\n",
        "            counter+=len(vals)\n",
        "        else:\n",
        "            collated_labels.append('O')\n",
        "    return collated_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnIqT-daIZK2"
      },
      "outputs": [],
      "source": [
        "def get_entities(word_tag_tuples):\n",
        "    \n",
        "    mentions=[]\n",
        "    candidateMention=''\n",
        "    type_tag=''\n",
        "    #emoji.get_emoji_regexp().sub(u'', candidateMention)\n",
        "    for tup in word_tag_tuples:\n",
        "        candidate=tup[0]\n",
        "        tag=tup[1]\n",
        "\n",
        "        if(tag=='O'):\n",
        "            if(candidateMention):\n",
        "                if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                    mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                    if mention_to_add.endswith(\"'s\"):\n",
        "                        li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    elif mention_to_add.endswith(\"’s\"):\n",
        "                        li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    else:\n",
        "                        mention_to_add=mention_to_add\n",
        "                    if(mention_to_add!=''):\n",
        "                        mentions.append((mention_to_add,type_tag))\n",
        "            candidateMention=''\n",
        "            type_tag=''\n",
        "        else:\n",
        "            boundary_tag = tag.split('-')[0]\n",
        "            type_tag = tag.split('-')[1]\n",
        "            if (boundary_tag=='B'):\n",
        "                if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                    mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                    if mention_to_add.endswith(\"'s\"):\n",
        "                        li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    elif mention_to_add.endswith(\"’s\"):\n",
        "                        li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    else:\n",
        "                        mention_to_add=mention_to_add\n",
        "                    if(mention_to_add!=''):\n",
        "                        mentions.append((mention_to_add,type_tag))\n",
        "                candidateMention=candidate\n",
        "            else:\n",
        "                candidateMention+=\" \"+candidate\n",
        "        # if (tag=='B'):\n",
        "        #     if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))):\n",
        "        #         mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "        #         if(mention_to_add):\n",
        "        #             mentions.append(mention_to_add)\n",
        "        #     candidateMention=candidate\n",
        "        # else:\n",
        "        #     candidateMention+=\" \"+candidate\n",
        "    if(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).strip()):\n",
        "        if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "            mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "            if(mention_to_add!=''):\n",
        "                mentions.append((mention_to_add,type_tag))\n",
        "        # mentions.append(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip())\n",
        "    # print('extracted mentions:', mentions)\n",
        "    return mentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kp4wv0yIj7F"
      },
      "outputs": [],
      "source": [
        "# def get_encoding_seq(tweet_word_list, mentions):\n",
        "#     print(tweet_word_list)\n",
        "#     print(mentions)\n",
        "#     tweet_word_index=0\n",
        "#     encoded_tag_sequence=[]\n",
        "#     while(mentions):\n",
        "#         current_mention=[token.strip() for token in mentions.pop(0).split(' ')]\n",
        "#         while(normalize(current_mention[0])!=normalize(tweet_word_list[tweet_word_index])):\n",
        "#             encoded_tag_sequence.append('O')\n",
        "#             tweet_word_index+=1\n",
        "#         if(normalize(current_mention[0])==normalize(tweet_word_list[tweet_word_index])):\n",
        "#             for token_index, token in enumerate(current_mention):\n",
        "#                 if(token_index==0):\n",
        "#                     encoded_tag_sequence.append('B')\n",
        "#                 else:\n",
        "#                     encoded_tag_sequence.append('I')\n",
        "#                 tweet_word_index+=1\n",
        "#     while(tweet_word_index<len(tweet_word_list)):\n",
        "#         encoded_tag_sequence.append('O')\n",
        "#         tweet_word_index+=1\n",
        "        \n",
        "#     print(encoded_tag_sequence)\n",
        "#     return encoded_tag_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uHnEzVDJN2F"
      },
      "outputs": [],
      "source": [
        "gutenberg_text = \"\"\n",
        "for file_id in gutenberg.fileids():\n",
        "    gutenberg_text += gutenberg.raw(file_id)\n",
        "tokenizer_trainer = PunktTrainer()\n",
        "tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "my_sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('ret')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('rep')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('mr')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('ms')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('mrs')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('v')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('vs')\n",
        "\n",
        "\n",
        "def normalize_to_sentences(text):\n",
        "    tweetSentences=list(filter (lambda sentence: len(sentence)>1, text.split('\\n')))\n",
        "    tweetSentenceList_inter=custom_flatten(list(map(lambda sentText: my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
        "    tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
        "    return tweetSentenceList\n",
        "\n",
        "def custom_flatten(mylist, outlist,ignore_types=(str, bytes, int)):\n",
        "    \n",
        "    if (mylist !=[]):\n",
        "        for item in mylist:\n",
        "            #print not isinstance(item, ne.NE_candidate)\n",
        "            if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                custom_flatten(item, outlist)\n",
        "            else:\n",
        "                item=item.strip(' \\t\\n\\r')\n",
        "                outlist.append(item)\n",
        "    return outlist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv4aARohiJm9"
      },
      "outputs": [],
      "source": [
        "def preprocess(filename):\n",
        "    \"\"\"save a file with token, label and prediction in each row\"\"\"\n",
        "    tweet_to_sentences_w_annotation={}\n",
        "    sentenceID=0\n",
        "    test=pd.read_csv(\"data/\"+filename,sep =',',keep_default_na=False)\n",
        "    # outputfilename=\"data/covid/covid_2K.txt\"\n",
        "    \n",
        "    all_annotated_ne=[]\n",
        "    tweetsentences=[]\n",
        "    tokenizedsentences=[]\n",
        "    \n",
        "    for row in test.itertuples():\n",
        "        tweetID=str(row.Index)\n",
        "        text=str(row.TweetText)\n",
        "        row_sentences = normalize_to_sentences(text)\n",
        "        tweetsentences += row_sentences\n",
        "        tokenizedsentences += [tokenizer(sentence, is_split_into_words=True) for sentence in row_sentences]\n",
        "        # print(tweetID,text)\n",
        "        \n",
        "        mentions=[]\n",
        "        # print(row.mentions_other)\n",
        "        for sentence_level in str(row.mentions_other).split(';'):\n",
        "            if(sentence_level):\n",
        "                for elem in sentence_level.split(','):\n",
        "                    if(elem):\n",
        "                        mention_record = elem.split('|')\n",
        "                        # print(mention_record)\n",
        "                        mention, entity_type =  mention_record[0], mention_record[1].lower()\n",
        "                        if(mention):\n",
        "                            if((mention !='')&(mention !='nan')):\n",
        "                                mention = mention.lower().strip(string.punctuation).strip()\n",
        "                                if(mention.startswith('the ')):\n",
        "                                    mention = mention[4:]\n",
        "                                mentions.append((mention, entity_type))\n",
        "        # mentions=list(filter(lambda element: ((element !='')&(element !='nan')), mentions))\n",
        "        # all_annotated_ne.extend(mentions)\n",
        "        \n",
        "        # if(row_sentences):\n",
        "        tweet_to_sentences_w_annotation[tweetID]=((sentenceID,sentenceID+len(row_sentences)),mentions)\n",
        "        sentenceID+=len(row_sentences)\n",
        "        # else:\n",
        "        #     tweet_to_sentences_w_annotation[tweetID]=((sentenceID,sentenceID+1),mentions)\n",
        "        #     sentenceID+=1\n",
        "        # print(sentenceID,len(row_sentences))\n",
        "    return tweetsentences, tokenizedsentences, tweet_to_sentences_w_annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChZgr__bv3M9"
      },
      "outputs": [],
      "source": [
        "def calculate_f1_ner(tweet_to_sentences_w_annotation, tweetsentences, true_predictions):\n",
        "    \n",
        "    entity_types = ['org','misc', 'loc', 'per']\n",
        "    confusion_matrices = {entity_type: {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0} for entity_type in entity_types}\n",
        "    scores = {entity_type: {'p': 0, 'r': 0, 'f1': 0} for entity_type in entity_types}\n",
        "\n",
        "    ner_arrays=[]\n",
        "    for n, tweet in enumerate(tweetsentences):\n",
        "        # tweet_data = list(zip(tweet, true_labels[n], predictions[i:i + len(tweet)]))\n",
        "        # word_tag_tuples=zip(tweet,predictions[i:i + len(tweet)])\n",
        "        \n",
        "        assert (len(tweet)==len(true_predictions[n]))\n",
        "        word_tag_tuples=zip(tweet,true_predictions[n])\n",
        "        entities_from_sentence=get_entities(word_tag_tuples)\n",
        "        ner_arrays.append(entities_from_sentence)\n",
        "\n",
        "    print('tally:',len(tweetsentences),len(ner_arrays))\n",
        "\n",
        "    for tweetID in tweet_to_sentences_w_annotation.keys():\n",
        "        unrecovered_annotated_mention_list=[]\n",
        "        annotated_mention_list=tweet_to_sentences_w_annotation[tweetID][1]\n",
        "        output_mentions_list=[]\n",
        "        true_positive_list=[]\n",
        "        idRange=tweet_to_sentences_w_annotation[tweetID][0]\n",
        "\n",
        "        for sentID in range(idRange[0],idRange[1]):\n",
        "            output_mentions_list+=ner_arrays[sentID]\n",
        "        output_mentions_list=[(elem[0],elem[1].lower()) for elem in output_mentions_list]\n",
        "        print(tweetID,annotated_mention_list,output_mentions_list)\n",
        "\n",
        "        while(annotated_mention_list):\n",
        "            if(len(output_mentions_list)):\n",
        "                annotated_candidate= annotated_mention_list.pop()\n",
        "                if(annotated_candidate in output_mentions_list):\n",
        "                    output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                    # tp_counter_inner+=1 # for emd\n",
        "                    true_positive_list.append(annotated_candidate)\n",
        "                    # #for ner\n",
        "                    # entity_type=annotated_candidate[1]\n",
        "                    # confusion_matrices[entity_type]['TP']+=1\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "            else:\n",
        "                unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                break\n",
        "        # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "\n",
        "        # fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "        # fp_counter_inner=all_postitive_counter_inner - tp_counter_inner\n",
        "        # print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
        "\n",
        "        print('true positives:',true_positive_list)\n",
        "        print('false positives:',output_mentions_list)\n",
        "        print('false negatives:',unrecovered_annotated_mention_list)\n",
        "\n",
        "        for mention_tup in true_positive_list:\n",
        "            entity_type=mention_tup[1]\n",
        "            if(mention_tup[0]):\n",
        "                confusion_matrices[entity_type]['TP']+=1\n",
        "\n",
        "        for mention_tup in unrecovered_annotated_mention_list:\n",
        "            entity_type=mention_tup[1]\n",
        "            if(mention_tup[0]):\n",
        "                confusion_matrices[entity_type]['FN']+=1\n",
        "\n",
        "        for mention_tup in output_mentions_list:\n",
        "            entity_type=mention_tup[1]\n",
        "            if(mention_tup[0]):\n",
        "                confusion_matrices[entity_type]['FP']+=1\n",
        "    \n",
        "    print('========Named Entity Recognition========')\n",
        "    for entity_type in entity_types:\n",
        "        print(entity_type.upper())\n",
        "        precision = confusion_matrices[entity_type]['TP']/(confusion_matrices[entity_type]['TP']+confusion_matrices[entity_type]['FP'])\n",
        "        recall = confusion_matrices[entity_type]['TP']/(confusion_matrices[entity_type]['TP']+confusion_matrices[entity_type]['FN'])\n",
        "        f_measure = 2*precision*recall/(precision+recall)\n",
        "\n",
        "        scores[entity_type]['p'] = precision\n",
        "        scores[entity_type]['r'] = recall\n",
        "        scores[entity_type]['f1'] = f_measure\n",
        "\n",
        "        print('precision: ',precision)\n",
        "        print('recall: ',recall)\n",
        "        print('f_measure: ',f_measure)\n",
        "\n",
        "        print('------------------------')\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOsWhlXRJRVw"
      },
      "outputs": [],
      "source": [
        "def calculate_f1_emd(tweet_to_sentences_w_annotation, tweetsentences, true_predictions):\n",
        "    \n",
        "    # dataset, i = [], 0\n",
        "    ner_arrays=[]\n",
        "    file_write_text=''\n",
        "    all_detected_ne=[]\n",
        "    all_annotated_ne=[]\n",
        "    \n",
        "    for n, tweet in enumerate(tweetsentences):\n",
        "        # tweet_data = list(zip(tweet, true_labels[n], predictions[i:i + len(tweet)]))\n",
        "        # word_tag_tuples=zip(tweet,predictions[i:i + len(tweet)])\n",
        "        \n",
        "        assert (len(tweet)==len(true_predictions[n]))\n",
        "        word_tag_tuples=zip(tweet,true_predictions[n])\n",
        "        entities_from_sentence=get_entities(word_tag_tuples)\n",
        "        # line_text='\\t'.join(entities_from_sentence)\n",
        "        # file_write_text+=line_text+'\\n'\n",
        "        # print(entities_from_sentence)\n",
        "        all_detected_ne.extend(entities_from_sentence)\n",
        "        ner_arrays.append(entities_from_sentence)\n",
        "        # dataset += tweet_data + [()]\n",
        "    \n",
        "    print('tally:',len(tweetsentences),len(ner_arrays))\n",
        "    system_output_mention_list=list(set(all_detected_ne))\n",
        "    # file_write_text='\\n'.join(system_output_mention_list)\n",
        "    # f1= open(outputfilename, \"w\")\n",
        "    # f1.write(file_write_text)\n",
        "    # f1.close()\n",
        "    \n",
        "    true_positive_count=0\n",
        "    false_positive_count=0\n",
        "    false_negative_count=0\n",
        "    total_mentions=0\n",
        "    total_annotation=0\n",
        "    \n",
        "    \n",
        "    for tweetID in tweet_to_sentences_w_annotation.keys():\n",
        "        unrecovered_annotated_mention_list=[]\n",
        "        tp_counter_inner=0\n",
        "        fp_counter_inner=0\n",
        "        fn_counter_inner=0\n",
        "        \n",
        "        annotated_mention_list=tweet_to_sentences_w_annotation[tweetID][1]\n",
        "        all_annotated_ne.extend(annotated_mention_list)\n",
        "        output_mentions_list=[]\n",
        "        output_mentions_w_type_list=[]\n",
        "        idRange=tweet_to_sentences_w_annotation[tweetID][0]\n",
        "        for sentID in range(idRange[0],idRange[1]):\n",
        "            output_mentions_w_type_list+=ner_arrays[sentID]\n",
        "            \n",
        "        print(tweetID,annotated_mention_list,output_mentions_w_type_list)\n",
        "        output_mentions_list=[elem[0] for elem in output_mentions_w_type_list]\n",
        "        print(output_mentions_list)\n",
        "        all_postitive_counter_inner=len(output_mentions_list)\n",
        "        while(annotated_mention_list):\n",
        "            if(len(output_mentions_list)):\n",
        "                annotated_candidate= annotated_mention_list.pop()\n",
        "                if(annotated_candidate in output_mentions_list):\n",
        "                    output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                    tp_counter_inner+=1\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "            else:\n",
        "                unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                break\n",
        "        # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "        fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "        fp_counter_inner=all_postitive_counter_inner - tp_counter_inner\n",
        "        \n",
        "        print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
        "        \n",
        "        true_positive_count+=tp_counter_inner\n",
        "        false_positive_count+=fp_counter_inner\n",
        "        false_negative_count+=fn_counter_inner\n",
        "        \n",
        "    print('true_positive_count,false_positive_count,false_negative_count:')\n",
        "    print(true_positive_count,false_positive_count,false_negative_count)\n",
        "    \n",
        "    precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "    recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "    f_measure=2*(precision*recall)/(precision+recall)\n",
        "            \n",
        "    print('========Entity Mention Detection========')\n",
        "    print('precision: ',precision)\n",
        "    print('recall: ',recall)\n",
        "    print('f_measure: ',f_measure)\n",
        "\n",
        "    # print('========Entity Detection========')\n",
        "    # true_positive_entities =  len(list(set(all_detected_ne).intersection(set(all_annotated_ne))))\n",
        "    # false_positive_entities = len(list(set(all_annotated_ne)-set(all_detected_ne)))\n",
        "    # false_negative_entities = len(list(set(all_detected_ne)-set(all_annotated_ne)))\n",
        "\n",
        "    # precision= (true_positive_entities)/(true_positive_entities+false_positive_entities)\n",
        "    # recall= (true_positive_entities)/(true_positive_entities+false_negative_entities)\n",
        "    # f_measure = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "    # print('precision: ',precision)\n",
        "    # print('recall: ',recall)\n",
        "    # print('f_measure: ',f_measure)   \n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK4fkrxjLQZj"
      },
      "source": [
        "## **Entity Classifier Alt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-44Vn9b5LVdZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHrCHHreLbla"
      },
      "outputs": [],
      "source": [
        "class CandidateDataset(Dataset):\n",
        "    def __init__(self, df, candidateEmbeddingDict):\n",
        "        self.samples= []\n",
        "        self.max_freq=-1\n",
        "        self.freq_arr = []\n",
        "        # self.embeddings_repo = []\n",
        "        self.output = []\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            candidate_record = row['candidate'].split('||')\n",
        "            # print(candidate_record)\n",
        "            candidate, cluster_id = candidate_record[0], candidate_record[1]\n",
        "            normalized_length = row['normalized_length']\n",
        "            cumulative= row['cumulative']\n",
        "            if(cumulative>self.max_freq):\n",
        "                self.max_freq=cumulative\n",
        "\n",
        "            output_val= row['class']\n",
        "\n",
        "            local_embedding_list= candidateEmbeddingDict[(candidate,cluster_id)]\n",
        "            assert len(local_embedding_list) !=0\n",
        "            assert cumulative == len(local_embedding_list)\n",
        "\n",
        "            tup=(candidate,cluster_id,normalized_length,cumulative)\n",
        "\n",
        "            self.freq_arr.append(cumulative)\n",
        "            self.samples.append(tup)\n",
        "            self.output.append(output_val)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tup2=(self.freq_arr[idx]/self.max_freq,)\n",
        "        tup=self.samples[idx]+tup2\n",
        "\n",
        "        return tup,self.output[idx]\n",
        "        # return self.len_arr[idx],self.cum_freq_arr[idx],self.output[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d3CEsOhLhU9"
      },
      "outputs": [],
      "source": [
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self,input_size,device):\n",
        "        super(NN, self).__init__()\n",
        "        self.device=device\n",
        "        self.input_size = input_size\n",
        "\n",
        "        #customized weighted pooling\n",
        "        self.pooling_layer1 = nn.Linear(input_size,50)\n",
        "        self.pooling_layer2 = nn.Linear(50,1)\n",
        "\n",
        "        self.linear1 = nn.Linear(input_size+1,500)\n",
        "        self.batchnorm = nn.BatchNorm1d(500)\n",
        "        self.linear2 = nn.Linear(500,200)\n",
        "        self.linear3 = nn.Linear(200,50)\n",
        "        self.output_layer = nn.Linear(50,5)\n",
        "      \n",
        "    def nxn_cos_sim(A, B, dim=1, eps=1e-8):\n",
        "      numerator = A @ B.T\n",
        "      A_l2 = torch.mul(A, A).sum(axis=dim)\n",
        "      B_l2 = torch.mul(B, B).sum(axis=dim)\n",
        "      denominator = torch.max(torch.sqrt(torch.outer(A_l2, B_l2)), torch.tensor(eps))\n",
        "      return torch.div(numerator, denominator)\n",
        "\n",
        "    \n",
        "    def compute(self, x_tup):\n",
        "\n",
        "        x_len=x_tup[0]\n",
        "        # x_List=x_tup[1]\n",
        "        local_embedding_list=x_tup[1]\n",
        "        x_cum_freq=x_tup[2]\n",
        "        x_normalized_freq=x_tup[3]\n",
        "        \n",
        "        # x_tensor = torch.FloatTensor(np.array([x_len,x_normalized_freq],dtype=float)).to(self.device)\n",
        "        x_tensor = torch.FloatTensor(np.array([x_len],dtype=float)).to(self.device)\n",
        "\n",
        "        \n",
        "        all_local_embedding = torch.stack(local_embedding_list).to(self.device)\n",
        "\n",
        "        pooling_output_1=F.relu(self.pooling_layer1(all_local_embedding))\n",
        "        pooling_output_2=self.pooling_layer2(pooling_output_1)\n",
        "        weights = pooling_output_2.reshape(-1)\n",
        "        # print(all_local_embedding.size(),len(weights))\n",
        "\n",
        "        # cos_sim_matrix = self.nxn_cos_sim(all_local_embedding,all_local_embedding)\n",
        "        exp_tensor = torch.exp(weights)\n",
        "        sum_tensor = torch.sum(exp_tensor)\n",
        "        weights = torch.div(exp_tensor,sum_tensor).tolist()\n",
        "\n",
        "\n",
        "        aggregated_input = torch.zeros(self.input_size, requires_grad=True).to(self.device)\n",
        "        for ind, local_embedding in enumerate(all_local_embedding):\n",
        "            aggregated_input=aggregated_input.add(torch.mul(local_embedding, weights[ind]))\n",
        "        \n",
        "        # aggregated_input=torch.mul(aggregated_input, float(1/x_cum_freq))\n",
        "        x = torch.cat((x_tensor,aggregated_input), 0).to(self.device)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, batch_data):\n",
        "        global_embeddings_arr=[]\n",
        "        for x_tup in batch_data:\n",
        "            global_embeddings_arr.append(self.compute(x_tup))\n",
        "        # global_embeddings_arr = np.array(global_embeddings_arr)\n",
        "        global_embeddings_batch = torch.stack(global_embeddings_arr).to(self.device)\n",
        "        # global_embeddings_batch.requires_grad = True\n",
        "        # print(global_embeddings_batch.size())\n",
        "        x = F.relu(self.linear1(global_embeddings_batch))\n",
        "        x = self.batchnorm(x)\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        out = self.output_layer(x)\n",
        "        return out\n",
        "\n",
        "class EntityClassifierAlt():\n",
        "\n",
        "    def __init__(self, to_train, device, candidateBase_train, candidateEmbeddingDict):\n",
        "\n",
        "\n",
        "        self.candidateEmbeddingDict_train = candidateEmbeddingDict\n",
        "        self.candidateBase_train = candidateBase_train\n",
        "        self.combined_feature_list=['length']\n",
        "\n",
        "        self.device=device\n",
        "\n",
        "        self.input_size = 300 #len(['normalized_cf_'+str(i) for i in range(300)])\n",
        "\n",
        "        self.entity_types = {0:'ne', 1:'org', 2:'misc', 3:'loc', 4:'per'}\n",
        "        # self.confusion_matrices = {entity_type: {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0} for entity_type in self.entity_types.keys()}\n",
        "        # self.scores = {entity_type: {'p': 0, 'r': 0, 'f1': 0} for entity_type in self.entity_types.keys()}\n",
        "\n",
        "        # self.relevant_columns = ['candidate','normalized_length','cumulative']\n",
        "        self.relevant_columns = ['candidate','normalized_length','cumulative','class']\n",
        "        # self.relevant_columns = ['normalized_cf_'+str(i) for i in range(768)]\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(self.input_size,device).to(device)\n",
        "        #Loss and Optimizer\n",
        "        self.ec_criterion = nn.CrossEntropyLoss()\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.01, weight_decay=1e-8)\n",
        "        self.ec_batch_size = 32\n",
        "        self.ec_num_epochs = 200\n",
        "        self.patience = 10\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            # define checkpoint saved path\n",
        "            ckp_path = \"entityClassifierAlt/model_checkpoints_ner/classifierAlt_checkpoint_model300_wnut.pt\" #300\n",
        "\n",
        "            if(path.exists(ckp_path)):\n",
        "                # load the saved checkpoint\n",
        "                self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "            max_candidate_length = self.candidateBase_train['length'].max()+1\n",
        "            # print(self.train['length'].tolist())\n",
        "            self.candidateBase_train['normalized_length'] = self.candidateBase_train['length']/max_candidate_length\n",
        "\n",
        "            print(self.candidateBase_train['normalized_length'].max())\n",
        "            \n",
        "            #Loading the data\n",
        "            # candidate_array = self.train['candidate'].tolist()\n",
        "            dataset_df = self.candidateBase_train[self.relevant_columns]\n",
        "\n",
        "            dataset = CandidateDataset(dataset_df, self.candidateEmbeddingDict_train)\n",
        "\n",
        "            print(dataset.__getitem__(0))\n",
        "\n",
        "            train_len=int(math.ceil(len(dataset_df)*0.85))\n",
        "            val_len=len(dataset_df)-train_len\n",
        "\n",
        "            print('train_len',train_len)\n",
        "            print('val_len',val_len)\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, val_len, shuffle=True) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            checkpoint_dir = \"entityClassifierAlt/model_checkpoints_ner\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            # define checkpoint saved path\n",
        "            ckp_path = \"entityClassifierAlt/model_checkpoints_ner/classifierAlt_checkpoint_model300.pt\" #300\n",
        "            # ckp_path = \"entityClassifierAlt/model_checkpoints_ner/classifierAlt_checkpoint_model300_wnut.pt\" #300\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        # f_path = checkpoint_dir + '/classifierAlt_checkpoint768.pt' #768\n",
        "        f_path = checkpoint_dir + '/classifierAlt_checkpoint_model300.pt' #300\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def custom_loss(self, out, targets, freq):\n",
        "        print('verifying shapes:')\n",
        "        print(out.shape, targets.shape)\n",
        "        softmaxloss_arr=[]\n",
        "        for i, out_i in enumerate(out):\n",
        "            print(out_i.shape, targets[i].shape)\n",
        "            loss_i = self.ec_criterion(out_i, targets[i])\n",
        "            softmaxloss_arr.append(loss_i)\n",
        "        # print(type(softmaxloss))\n",
        "        softmaxloss = torch.stack(softmaxloss_arr).to(self.device)\n",
        "        print(softmaxloss, freq)\n",
        "        print(softmaxloss.shape, freq.shape)\n",
        "        vec=softmaxloss*freq\n",
        "        print(vec)\n",
        "        final_loss = torch.sum(vec)/torch.sum(freq)\n",
        "        print(vec.shape, loss.shape)\n",
        "        return final_loss\n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data_inter, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                # data = data.to(device=device)\n",
        "\n",
        "                # print(len(data_inter),len(targets),type(data_inter))\n",
        "                # print(len(data_inter[0]),len(data_inter[1]),len(data_inter[2]))\n",
        "\n",
        "                #(candidate,cluster_id,normalized_length,cumulative,normalized_cumulative)\n",
        "                candidates = data_inter[0]\n",
        "                cluster_ids = data_inter[1]\n",
        "                length_arr = data_inter[2]\n",
        "                cumulative_arr = data_inter[3]\n",
        "                normalized_freq_arr = data_inter[4]\n",
        "\n",
        "                data = [(length_arr[index], self.candidateEmbeddingDict_train[(candidate,cluster_ids[index])], cumulative_arr[index], normalized_freq_arr[index]) for index, candidate in enumerate(candidates)]\n",
        "                # targets = targets.unsqueeze(1).to(device=self.device)\n",
        "                targets = targets.type(torch.LongTensor)\n",
        "                targets = targets.to(device=self.device)\n",
        "\n",
        "                self.ec_optimizer.zero_grad()\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "                # out = out.to(torch.float32)\n",
        "                # targets = targets.to(torch.float32)\n",
        "\n",
        "                # #custom cross-entropy loss factoring in frequency\n",
        "                # loss = self.custom_loss(out, targets, torch.FloatTensor(np.array(cumulative_arr)).unsqueeze(1).to(device=self.device))\n",
        "\n",
        "                loss = self.ec_criterion(out, targets)\n",
        "\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            print('combined_training_loss:',combined_training_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            all_candidates = []\n",
        "            predictions = []\n",
        "            self.confusion_matrices = {entity_type: {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0} for entity_type in self.entity_types.keys()}\n",
        "            self.scores = {entity_type: {'p': 0, 'r': 0, 'f1': 0} for entity_type in self.entity_types.keys()}\n",
        "            with torch.no_grad():\n",
        "                self.classifier.eval()\n",
        "                for batch_idx, (val_data_inter, val_targets) in enumerate(self.val_loader):\n",
        "                    # val_data = val_data.to(device=device)\n",
        "                    val_candidates = val_data_inter[0]\n",
        "                    val_cluster_ids = val_data_inter[1]\n",
        "                    val_length_arr = val_data_inter[2]\n",
        "                    val_cumulative_arr = val_data_inter[3]\n",
        "                    val_normalized_freq_arr = val_data_inter[4]\n",
        "\n",
        "                    val_data = [(val_length_arr[index], self.candidateEmbeddingDict_train[(val_candidate,val_cluster_ids[index])], val_cumulative_arr[index], val_normalized_freq_arr[index]) for index, val_candidate in enumerate(val_candidates)]\n",
        "                    # val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    val_targets = val_targets.type(torch.LongTensor)\n",
        "                    val_targets = val_targets.to(device=device)\n",
        "\n",
        "                    val_out = self.classifier(val_data)\n",
        "\n",
        "                    # y_pred = out.to(torch.float32)\n",
        "                    # val_targets = val_targets.to(torch.float32)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    \n",
        "                    y_pred_softmax = torch.log_softmax(val_out, dim = 1)\n",
        "                    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
        "                    batch_predictions = y_pred_tags.reshape(-1)\n",
        "                    # print(y_pred_tags.shape)\n",
        "                    # print(batch_predictions.shape)\n",
        "\n",
        "                    predictions+=batch_predictions.tolist()\n",
        "\n",
        "                    labels+=val_targets.tolist()\n",
        "                    all_candidates+=candidates\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    val_loss = self.ec_criterion(val_out, val_targets)\n",
        "                    # loss = self.custom_loss(out, val_targets, torch.FloatTensor(np.array(cumulative_arr)).unsqueeze(1).to(device=self.device)) #custom l1 loss factoring in frequency\n",
        "                    validation_batch_loss.append(val_loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "                assert len(predictions)==len(labels)\n",
        "                # print(len(predictions),len(labels),type(predictions[0]))\n",
        "                # print(predictions)\n",
        "                \n",
        "\n",
        "                #EMD training objective\n",
        "                for indx in range(len(predictions)):\n",
        "                    predicted_class = predictions[indx]\n",
        "                    true_class = labels[indx]\n",
        "\n",
        "                    if(predicted_class==true_class):\n",
        "                        self.confusion_matrices[true_class]['TP']+=1\n",
        "                    else:\n",
        "                        self.confusion_matrices[true_class]['FN']+=1\n",
        "                        self.confusion_matrices[true_class]['FP']+=1\n",
        "\n",
        "                macrof1=0\n",
        "                for entity_type in self.entity_types:\n",
        "                    # print(entity_type.upper())\n",
        "                    try:\n",
        "                        precision = self.confusion_matrices[entity_type]['TP']/(self.confusion_matrices[entity_type]['TP']+self.confusion_matrices[entity_type]['FP'])\n",
        "                    except ZeroDivisionError:\n",
        "                        precision = 0\n",
        "                    try:\n",
        "                        recall = self.confusion_matrices[entity_type]['TP']/(self.confusion_matrices[entity_type]['TP']+self.confusion_matrices[entity_type]['FN'])\n",
        "                    except ZeroDivisionError:\n",
        "                        recall = 0\n",
        "                    try:\n",
        "                        f_measure = 2*precision*recall/(precision+recall)\n",
        "                    except ZeroDivisionError:\n",
        "                        f_measure = 0\n",
        "\n",
        "                    self.scores[entity_type]['p'] = precision\n",
        "                    self.scores[entity_type]['r'] = recall\n",
        "                    self.scores[entity_type]['f1'] = f_measure\n",
        "                    macrof1+=f_measure\n",
        "                    print(self.entity_types[entity_type], 'precision: ',precision, 'recall: ',recall, 'f_measure: ',f_measure)\n",
        "                    \n",
        "                macrof1 = macrof1/len(self.entity_types)\n",
        "                print('macrof1:',str(macrof1))\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_validation_loss)\n",
        "                \n",
        "                \n",
        "                # if(combined_validation_loss<best_loss):\n",
        "                if(macrof1>best_f1):\n",
        "                    # best_loss = combined_validation_loss\n",
        "                    best_f1 = macrof1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                print('------------------------')\n",
        "                print('\\n')\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase,candidateEmbeddingDict):\n",
        "\n",
        "        candidateBase['class']=-1\n",
        "        max_length=candidateBase['length'].max()+1\n",
        "        candidateBase['normalized_length']= candidateBase['length']/max_length\n",
        "\n",
        "        test_dataset = CandidateDataset(candidateBase, candidateEmbeddingDict)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        predictions=[]\n",
        "        with torch.no_grad():\n",
        "            self.classifier.eval()\n",
        "            for batch_idx, (data_inter, targets) in enumerate(test_loader):\n",
        "                # data = data.to(device=device)\n",
        "                candidates = data_inter[0]\n",
        "                cluster_ids = data_inter[1]\n",
        "                length_arr = data_inter[2]\n",
        "                cumulative_arr = data_inter[3]\n",
        "                normalized_freq_arr = data_inter[4]\n",
        "\n",
        "                data = [(length_arr[index], candidateEmbeddingDict[(candidate,cluster_ids[index])], cumulative_arr[index], normalized_freq_arr[index]) for index, candidate in enumerate(candidates)]\n",
        "                y_pred = self.classifier(data)\n",
        "                # print(out.shape)\n",
        "                y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "                _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
        "                predictions = y_pred_tags.reshape(-1)\n",
        "        print(predictions.shape,len(candidateBase))\n",
        "\n",
        "        candidateBase['class'] = predictions.tolist()\n",
        "        # print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47Do-RR4nsYt"
      },
      "source": [
        "## **Extracting Embeddings to train the Phrase Embedder on Contrastive Loss and EntityClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3akU16hcnywM"
      },
      "outputs": [],
      "source": [
        "from emoji import demojize\n",
        "from collections import defaultdict\n",
        "from random import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATZ1u0R_IcpH",
        "outputId": "7a42e208-3d03-4d06-a8a0-86d403e81971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "tweetTokenizer = TweetTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2a9q8a64LAV"
      },
      "outputs": [],
      "source": [
        "def normalizeToken(token):\n",
        "    lowercased_token = token.lower()\n",
        "    if token.startswith(\"@\"):\n",
        "        return \"@USER\"\n",
        "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
        "        return \"HTTPURL\"\n",
        "    elif len(token) == 1:\n",
        "        return demojize(token)\n",
        "    else:\n",
        "        if token == \"’\":\n",
        "            return \"'\"\n",
        "        elif token == \"…\":\n",
        "            return \"...\"\n",
        "        else:\n",
        "            return token\n",
        "\n",
        "def normalizeTweet(tweet):\n",
        "    tokens = tweetTokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
        "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
        "\n",
        "    normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n",
        "    normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n",
        "    normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n",
        "\n",
        "    normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n",
        "    normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n",
        "    normTweet = re.sub(r\"([0-9]{1,3}) - ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n",
        "    \n",
        "    return normTweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrotodBggktV"
      },
      "outputs": [],
      "source": [
        "def collate_token_label_embedding(tweetWordList, token_dict, prediction_labels, entity_embeddings):\n",
        "    counter=0\n",
        "    collated_labels=[]\n",
        "    collated_entity_embeddings=[]\n",
        "    for word in tweetWordList:\n",
        "        vals=token_dict[word]\n",
        "        # print(word,vals)\n",
        "        if(counter<len(prediction_labels)):\n",
        "            labels=prediction_labels[counter:counter+len(vals)]\n",
        "            token_entity_embeddings=entity_embeddings[counter:counter+len(vals)]\n",
        "    #         print(token_entity_embeddings.shape)\n",
        "            mean_tensor = torch.mean(token_entity_embeddings,dim=0)\n",
        "            mean_tensor[torch.isnan(mean_tensor)] = 0\n",
        "            collated_entity_embeddings.append(mean_tensor)\n",
        "    #         print(collated_entity_embeddings)\n",
        "            if('I-PER' in labels):\n",
        "                collated_labels.append('I-PER')\n",
        "            elif('I-LOC' in labels):\n",
        "                collated_labels.append('I-LOC')\n",
        "            elif('I-ORG' in labels):\n",
        "                collated_labels.append('I-ORG')\n",
        "            elif('I-MISC' in labels):\n",
        "                collated_labels.append('I-MISC')\n",
        "            elif('B-PER' in labels):\n",
        "                collated_labels.append('B-PER')\n",
        "            elif('B-LOC' in labels):\n",
        "                collated_labels.append('B-LOC')\n",
        "            elif('B-ORG' in labels):\n",
        "                collated_labels.append('B-ORG')\n",
        "            elif('B-MISC' in labels):\n",
        "                collated_labels.append('B-MISC')\n",
        "            else:\n",
        "                collated_labels.append('O')\n",
        "            counter+=len(vals)\n",
        "        else:\n",
        "            collated_labels.append('O')\n",
        "            collated_entity_embeddings.append(torch.zeros(768).to(device))\n",
        "    assert len(collated_labels)==len(collated_entity_embeddings)\n",
        "    return collated_labels,collated_entity_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTSrRtJbzNTt"
      },
      "outputs": [],
      "source": [
        "# trainset, tokenizedtrainset, tweet_to_sentences_w_annotation = preprocess('wnut17test_ner.csv')\n",
        "# trainset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('wnut17train_ner.csv')\n",
        "trainset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('deduplicated_test_WTypes.csv')\n",
        "\n",
        "candidateBaseHeaders_alt=['candidate', 'batch', 'length','cumulative','class']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ86SLX8Idct"
      },
      "outputs": [],
      "source": [
        "predictions=[]\n",
        "tokenized_sentences=[]\n",
        "count=0\n",
        "with torch.no_grad():\n",
        "    for train_record in trainset:\n",
        "        \n",
        "        sentence = normalizeTweet(train_record)\n",
        "        tweetWordList = sentence.split()\n",
        "        enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "\n",
        "        tokenized_input=tokenizer(sentence)\n",
        "        initial_input_ids = torch.tensor([tokenizer.encode(sentence)])\n",
        "\n",
        "        initial_input_ids = initial_input_ids[:,:128]\n",
        "        token_dict = {x : tokenizer.encode(x, add_special_tokens=False) for x in sentence.split()}\n",
        "\n",
        "        input_ids = initial_input_ids.to(device)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "        output = alt_model(input_ids)\n",
        "        \n",
        "        token_embeddings=output.hidden_states[-1].squeeze()[1:-1] # we dont need embeddings for CLS and EOS\n",
        "\n",
        "        assert torch.isnan(token_embeddings).any() == False\n",
        "\n",
        "        prediction = (torch.argmax(output.logits, axis=2))\n",
        "        prediction = prediction.cpu().numpy().reshape(-1)\n",
        "        if(count<=5):\n",
        "            print(prediction)\n",
        "        # prediction_labels=[label_list[l].split('-')[0] for l in prediction]\n",
        "        prediction_labels=[label_list[l] for l in prediction]\n",
        "        # prediction_labels=collate_token_labels(token_dict, prediction_labels[1:-1])\n",
        "        prediction_labels, entity_aware_embeddings = collate_token_label_embedding(tweetWordList, token_dict, prediction_labels[1:-1], token_embeddings)\n",
        "\n",
        "        # print(len(enumerated_tweetWordList),len(token_dict.keys()),len(entity_aware_embeddings),len(prediction_labels))\n",
        "\n",
        "        assert len(enumerated_tweetWordList)==len(entity_aware_embeddings)\n",
        "        assert len(prediction_labels)==len(enumerated_tweetWordList)\n",
        "\n",
        "        predictions.append(prediction_labels)\n",
        "        tokenized_sentences.append((tweetWordList,entity_aware_embeddings))\n",
        "\n",
        "        if(count<=5):\n",
        "            print(len(output.hidden_states))\n",
        "            print(token_dict.keys())\n",
        "            print(initial_input_ids)\n",
        "            print(input_ids)\n",
        "            print(prediction_labels)\n",
        "            print('===============')\n",
        "        count+=1\n",
        "\n",
        "print(len(predictions),len(tokenized_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXTEOoQPHZYt"
      },
      "outputs": [],
      "source": [
        "local_ner_arrays=[]\n",
        "for n, sentence_tup in enumerate(tokenized_sentences):  \n",
        "    sentence = sentence_tup[0]\n",
        "    assert (len(sentence)==len(predictions[n]))\n",
        "    word_tag_tuples=zip(sentence,predictions[n])\n",
        "    entities_from_sentence=get_entities(word_tag_tuples)\n",
        "    local_ner_arrays.append(entities_from_sentence)\n",
        "print('tally:',len(tokenized_sentences),len(local_ner_arrays))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hc4iew2YzB0"
      },
      "outputs": [],
      "source": [
        "canonical_candidate_df=pd.read_csv(\"data/candidate_train_records_altClassifier_new.csv\",sep =',', keep_default_na=False)\n",
        "canonical_ne_list = canonical_candidate_df[(canonical_candidate_df['class']==0)&(canonical_candidate_df['cumulative']>=5)].candidate.tolist()\n",
        "\n",
        "\n",
        "canonical_ne_list = set(canonical_ne_list)\n",
        "canonical_ne_list.remove('ex-trump')\n",
        "canonical_ne_list.remove('ex-trump adviser')\n",
        "canonical_ne_list.remove('jr')\n",
        "canonical_ne_list.remove('dt')\n",
        "canonical_ne_list.remove('ic')\n",
        "canonical_ne_list.remove('al')\n",
        "canonical_ne_list.remove('report')\n",
        "canonical_ne_list.remove('impeachment')\n",
        "canonical_ne_list.remove('missiles')\n",
        "canonical_ne_list.remove('america')\n",
        "canonical_ne_list.remove('military')\n",
        "canonical_ne_list.remove('congressional')\n",
        "canonical_ne_list.remove('secretary of state')\n",
        "canonical_ne_list.remove('america first')\n",
        "canonical_ne_list.remove('easter bunny')\n",
        "canonical_ne_list.remove('west')\n",
        "canonical_ne_list.remove('east')\n",
        "canonical_ne_list.remove('states')\n",
        "canonical_ne_list.remove('white')\n",
        "canonical_ne_list.remove('science')\n",
        "\n",
        "print(canonical_ne_list)\n",
        "print(len(canonical_ne_list))\n",
        "\n",
        "entity_types_dict = {'ne':0, 'org':1, 'misc':2, 'loc':3, 'per':4}\n",
        "type_candidate_dict = {entity_type:{} for entity_type in entity_types_dict}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSQL5lqsIqzo"
      },
      "outputs": [],
      "source": [
        "# #comment this out if collecting samples for training the entity phrase embedder\n",
        "# entityPhraseEmbedder.eval()\n",
        "\n",
        "candidate_embedding_dict = defaultdict(list)\n",
        "candidateBase_dict_alt = {} #<batch, length, cumulative, class>\n",
        "non_entity_count = 0\n",
        "entity_count = 0\n",
        "for tweetID in tweet_to_sentences_w_annotation.keys():\n",
        "    annotated_token_list = []\n",
        "    annotated_mention_list=tweet_to_sentences_w_annotation[tweetID][1]\n",
        "    tweet_token_list=[]\n",
        "    token_embedding_list=[]\n",
        "    local_mentions_list=[]\n",
        "    idRange=tweet_to_sentences_w_annotation[tweetID][0]\n",
        "\n",
        "    for sentID in range(idRange[0],idRange[1]):\n",
        "        tweet_token_list+=tokenized_sentences[sentID][0]\n",
        "        token_embedding_list+=tokenized_sentences[sentID][1]\n",
        "        local_mentions_list+=local_ner_arrays[sentID]\n",
        "    \n",
        "    assert len(tweet_token_list)==len(token_embedding_list)\n",
        "    print(tweetID, tweet_token_list)\n",
        "    print('annotated_mention_list:',annotated_mention_list)\n",
        "\n",
        "    \n",
        "    annotated_candidates = set([mention_tup[0].lower().strip() for mention_tup in annotated_mention_list])\n",
        "    for mention_tup in annotated_mention_list:\n",
        "        annotated_token_list+= mention_tup[0].lower().strip().split()\n",
        "    annotated_token_list = set(annotated_token_list)\n",
        "\n",
        "    #------------------------ negative candidate mining ------------------------\n",
        "    non_entity_list = list(set([elem.strip().lower() for elem in tweet_token_list]).intersection(canonical_ne_list)-annotated_candidates)\n",
        "\n",
        "    \n",
        "    if(non_entity_list):\n",
        "        print('non_entity_list:',non_entity_list)\n",
        "    \n",
        "    tweet_token_ind = 0\n",
        "    while((len(non_entity_list)>0)&(tweet_token_ind < len(tweet_token_list))):\n",
        "        #must pop from the left\n",
        "        non_entity_candidate = non_entity_list.pop(0)\n",
        "        non_entity_candidate, nonentity_type = non_entity_candidate.lower(),'ne'\n",
        "        non_entity_candidate_tokens = non_entity_candidate.split()\n",
        "        while(tweet_token_ind < len(tweet_token_list)):\n",
        "            if((' '.join(tweet_token_list[tweet_token_ind:tweet_token_ind+len(non_entity_candidate_tokens)])).strip().lower()!=non_entity_candidate):\n",
        "                # print(tweet_token_ind)\n",
        "                tweet_token_ind+=1\n",
        "            else:\n",
        "                ne_candidate=(' '.join(tweet_token_list[tweet_token_ind:tweet_token_ind+len(non_entity_candidate_tokens)])).strip().lower()\n",
        "                ne_candidate_token_embeddings = torch.stack(token_embedding_list[tweet_token_ind:tweet_token_ind+len(non_entity_candidate_tokens)]) #2d tensor of ne candidate token embeddings\n",
        "                \n",
        "                tensor_inter = torch.mean(ne_candidate_token_embeddings,dim=0)\n",
        "                tensor_inter_norm = torch.norm(tensor_inter, p=2,dim=0)\n",
        "                ne_candidate_tokens_avg_embedding = tensor_inter/tensor_inter_norm\n",
        "\n",
        "                assert torch.isnan(ne_candidate_tokens_avg_embedding).any() == False\n",
        "\n",
        "                # print(ne_candidate, ne_candidate_tokens_avg_embedding.shape)\n",
        "                if((non_entity_candidate, nonentity_type) not in candidate_embedding_dict):\n",
        "                    non_entity_count+=1\n",
        "                    candidateBase_dict_alt[non_entity_candidate+'||'+nonentity_type] = [0,len(non_entity_candidate_tokens),1,entity_types_dict[nonentity_type]]\n",
        "                else:\n",
        "                    candidateBase_dict_alt[non_entity_candidate+'||'+nonentity_type][2] += 1\n",
        "                \n",
        "                #---------------------- When training the entity phrase embedder ----------------------\n",
        "                # !!no need to unsqueeze here since not training on STS padded data\n",
        "                candidate_embedding_dict[(non_entity_candidate, nonentity_type)].append(ne_candidate_tokens_avg_embedding)\n",
        "                type_candidate_dict[nonentity_type][non_entity_candidate]=True\n",
        "\n",
        "                # #---------------------- When collecting mention phrase embeddings to train the entity classifier ----------------------\n",
        "                # # !! must unsqueeze at test time with embedder else batchnorm throws error\n",
        "                # ne_candidate_tokens_avg_embedding = ne_candidate_tokens_avg_embedding.unsqueeze(0)\n",
        "                # ne_candidate_phrase_embedding = (entityPhraseEmbedder.getEmbedding(ne_candidate_tokens_avg_embedding)).squeeze(0)\n",
        "                # # print(ne_candidate_tokens_avg_embedding.shape, ne_candidate_phrase_embedding.shape)\n",
        "                # candidate_embedding_dict[(non_entity_candidate, nonentity_type)].append(ne_candidate_phrase_embedding)\n",
        "\n",
        "                tweet_token_ind+=len(non_entity_candidate_tokens)\n",
        "                break\n",
        "    #------------------------ negative candidate mining ------------------------\n",
        "\n",
        "    #------------------------ positive candidate mining ------------------------\n",
        "    tweet_token_ind = 0\n",
        "    while((len(annotated_mention_list)>0)&(tweet_token_ind < len(tweet_token_list))):\n",
        "        #must pop from the left\n",
        "        annotated_candidate_tuple = annotated_mention_list.pop(0)\n",
        "        annotated_candidate, entity_type = annotated_candidate_tuple[0].lower(),annotated_candidate_tuple[1].lower()\n",
        "        candidate_tokens = annotated_candidate.split()\n",
        "\n",
        "        print(annotated_candidate)\n",
        "        while((' '.join(tweet_token_list[tweet_token_ind:tweet_token_ind+len(candidate_tokens)])).strip().lower()!=annotated_candidate):\n",
        "            tweet_token_ind+=1\n",
        "        candidate=(' '.join(tweet_token_list[tweet_token_ind:tweet_token_ind+len(candidate_tokens)])).strip().lower()\n",
        "        candidate_token_embeddings = torch.stack(token_embedding_list[tweet_token_ind:tweet_token_ind+len(candidate_tokens)]) #2d tensor of candidate token embeddings\n",
        "        tensor_inter = torch.mean(candidate_token_embeddings,dim=0)\n",
        "        tensor_inter_norm = torch.norm(tensor_inter, p=2,dim=0)\n",
        "        candidate_tokens_avg_embedding = tensor_inter/tensor_inter_norm\n",
        "\n",
        "        assert torch.isnan(candidate_tokens_avg_embedding).any() == False\n",
        "        # print(candidate,annotated_candidate,candidate_token_embeddings.shape)\n",
        "\n",
        "        if((annotated_candidate, entity_type) not in candidate_embedding_dict):\n",
        "            entity_count+=1\n",
        "            candidateBase_dict_alt[annotated_candidate+'||'+entity_type] = [0,len(candidate_tokens),1,entity_types_dict[entity_type]]\n",
        "        else:\n",
        "            candidateBase_dict_alt[annotated_candidate+'||'+entity_type][2] += 1\n",
        "\n",
        "        #---------------------- When training the entity phrase embedder ----------------------\n",
        "        # !!no need to unsqueeze here since not training on STS padded data\n",
        "        candidate_embedding_dict[(annotated_candidate, entity_type)].append(candidate_tokens_avg_embedding)\n",
        "        type_candidate_dict[entity_type][annotated_candidate]=True\n",
        "\n",
        "        # #---------------------- When collecting mention phrase embeddings to train the entity classifier ----------------------\n",
        "        # # !! must unsqueeze at test time with embedder else batchnorm throws error\n",
        "        # candidate_tokens_avg_embedding = candidate_tokens_avg_embedding.unsqueeze(0)\n",
        "        # candidate_phrase_embedding = (entityPhraseEmbedder.getEmbedding(candidate_tokens_avg_embedding)).squeeze(0)\n",
        "        # # print(candidate_tokens_avg_embedding.shape, candidate_phrase_embedding.shape)\n",
        "        # candidate_embedding_dict[(annotated_candidate, entity_type)].append(candidate_phrase_embedding)\n",
        "\n",
        "        tweet_token_ind+=len(candidate_tokens)\n",
        "    #------------------------ positive candidate mining ------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_VMi-RUegkN"
      },
      "outputs": [],
      "source": [
        "print('Found '+str(len(candidate_embedding_dict))+' candidates in training set')\n",
        "print('Found '+str(entity_count)+' entities in training set')\n",
        "print('Found '+str(non_entity_count)+' non entities in training set')\n",
        "print('========')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmj4tewsyPCv"
      },
      "source": [
        "## **Training the Entity Phrase Embedder**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z65DSXfnjdRb"
      },
      "outputs": [],
      "source": [
        "print(candidateBase_dict_alt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37h_Y1iM1NJm"
      },
      "outputs": [],
      "source": [
        "# #softknn TUPLET MINING\n",
        "# printonce=0\n",
        "# candidate_index_dict = {}\n",
        "# index_candidate_dict = {}\n",
        "\n",
        "# for ind, elem in enumerate(list(candidate_embedding_dict.keys())):\n",
        "#     candidate_index_dict[elem] = ind\n",
        "#     index_candidate_dict[ind] = elem\n",
        "\n",
        "# datalist=[] #schema: anchor, positive, n-negatives\n",
        "# for candidate_tup in candidate_embedding_dict.keys():\n",
        "#     if(len(candidate_embedding_dict[candidate_tup])>1):\n",
        "#         candidate, candidate_type = candidate_tup[0], candidate_tup[1]\n",
        "#         mention_embeddings_list = candidate_embedding_dict[candidate_tup]\n",
        "#         if(len(mention_embeddings_list)>300):\n",
        "#             mention_index_random = random.sample(list(range(len(mention_embeddings_list))), 300)\n",
        "#         else:\n",
        "#             mention_index_random = list(range(len(mention_embeddings_list)))\n",
        "        \n",
        "#         for ind in range(len(mention_embeddings_list)):\n",
        "#             negative_candidates_list=[]\n",
        "#             for entity_type in entity_types_dict:\n",
        "#                 if(candidate_type!=entity_type):\n",
        "#                     if(candidate in type_candidate_dict[entity_type]):\n",
        "#                         print('=>',candidate,candidate_type,entity_type)\n",
        "#                         neg_candidate_tup = (candidate,entity_type)\n",
        "#                     else:\n",
        "#                         negative_candidate = random.choice(list(type_candidate_dict[entity_type]))\n",
        "#                         neg_candidate_tup = (negative_candidate,entity_type)\n",
        "#                     negative_candidates_list.append(candidate_index_dict[neg_candidate_tup])\n",
        "#             tuplet = tuple([ind,candidate_index_dict[candidate_tup]]+negative_candidates_list)\n",
        "#             if(printonce<1):\n",
        "#                 print(tuplet)\n",
        "#                 printonce+=1\n",
        "#             datalist.append(tuplet)\n",
        "# print('dataset length is ',len(datalist))\n",
        "# shuffle(datalist)\n",
        "# print('dataset length is ',len(datalist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlDQucSgULYi"
      },
      "outputs": [],
      "source": [
        "# #N+1 TUPLET MINING\n",
        "# datalist=[] #schema: anchor, positive, n-negatives\n",
        "# for candidate_tup in candidate_embedding_dict.keys():\n",
        "#     if(len(candidate_embedding_dict[candidate_tup])>1):\n",
        "#         candidate, candidate_type = candidate_tup[0], candidate_tup[1]\n",
        "#         mention_embeddings_list = candidate_embedding_dict[candidate_tup]\n",
        "#         if(len(mention_embeddings_list)>300):\n",
        "#             mention_embeddings_list_random = random.sample(mention_embeddings_list, 300)\n",
        "#         else:\n",
        "#             mention_embeddings_list_random = mention_embeddings_list\n",
        "\n",
        "#         tuplets_size = 1\n",
        "#         negatives_arr = []\n",
        "#         for entity_type in entity_types_dict:\n",
        "#             if(candidate_type!=entity_type):\n",
        "#                 if(candidate in type_candidate_dict[entity_type]):\n",
        "#                     print('=>',candidate,candidate_type,entity_type)\n",
        "#                     neg_candidate_tup = (candidate,entity_type)\n",
        "#                 else:\n",
        "#                     negative_candidate = random.choice(list(type_candidate_dict[entity_type]))\n",
        "#                     neg_candidate_tup = (negative_candidate,entity_type)\n",
        "#                 negative_embedding_list = candidate_embedding_dict[neg_candidate_tup]\n",
        "\n",
        "#                 if(len(negative_embedding_list)>350):\n",
        "#                     negative_embedding_list_random = random.sample(negative_embedding_list, 350)\n",
        "#                 else:\n",
        "#                     negative_embedding_list_random = negative_embedding_list\n",
        "#                 tuplets_size *= len(negative_embedding_list_random)\n",
        "#                 negatives_arr.append(negative_embedding_list_random)\n",
        "        \n",
        "#         limit = min(500,tuplets_size)\n",
        "#         negatives_list = []\n",
        "#         while(limit>0):\n",
        "#             inner=[]\n",
        "#             for i in range(4):\n",
        "#                 inner.append(random.choice(negatives_arr[i]))\n",
        "#             negatives_list.append(inner)\n",
        "#             limit-=1\n",
        "\n",
        "#         for ind1, elem in enumerate(mention_embeddings_list_random):\n",
        "#             for ind2, elem in enumerate(mention_embeddings_list_random):\n",
        "#                 if(ind1!=ind2):\n",
        "#                     for neg_tuplet in negatives_list:\n",
        "#                         tuplet = tuple([mention_embeddings_list_random[ind1],mention_embeddings_list_random[ind2]]+ neg_tuplet)\n",
        "#                         datalist.append(tuplet)\n",
        "\n",
        "# print('dataset length is ',len(datalist))\n",
        "# shuffle(datalist)\n",
        "# print('dataset length is ',len(datalist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UormWm1mbmf"
      },
      "outputs": [],
      "source": [
        "#TRIPLET MINING\n",
        "\n",
        "datalist=[] #schema: anchor, positive, negative\n",
        "for candidate_tup in candidate_embedding_dict.keys():\n",
        "    if(len(candidate_embedding_dict[candidate_tup])>1):\n",
        "        candidate, candidate_type = candidate_tup[0], candidate_tup[1]\n",
        "        mention_embeddings_list = candidate_embedding_dict[candidate_tup]\n",
        "        if(len(mention_embeddings_list)>350):\n",
        "            mention_embeddings_list_random = random.sample(mention_embeddings_list, 350)\n",
        "        else:\n",
        "            mention_embeddings_list_random = mention_embeddings_list\n",
        "        \n",
        "        for ind1, elem in enumerate(mention_embeddings_list_random):\n",
        "            for ind2, elem in enumerate(mention_embeddings_list_random):\n",
        "                if(ind1!=ind2):\n",
        "                    for entity_type in entity_types_dict:\n",
        "                        if(candidate_type!=entity_type):\n",
        "                            if(candidate in type_candidate_dict[entity_type]):\n",
        "                                print('=>',candidate,candidate_type,entity_type)\n",
        "                                neg_candidate_tup = (candidate,entity_type)\n",
        "                            else:\n",
        "                                negative_candidate = random.choice(list(type_candidate_dict[entity_type]))\n",
        "                                neg_candidate_tup = (negative_candidate,entity_type)\n",
        "                            negative_embedding_list = candidate_embedding_dict[neg_candidate_tup]\n",
        "\n",
        "                            if(len(negative_embedding_list)>350):\n",
        "                                # negative_embedding_list_random = random.shuffle(negative_embedding_list)\n",
        "                                negative_embedding_list_random = random.sample(negative_embedding_list, 350)\n",
        "                            else:\n",
        "                                negative_embedding_list_random = negative_embedding_list\n",
        "                            \n",
        "                            #For triplets\n",
        "                            interlist = [(mention_embeddings_list_random[ind1],mention_embeddings_list_random[ind2],negative_elem) for negative_elem in negative_embedding_list_random]\n",
        "                            datalist.extend(interlist)\n",
        "print('dataset length is ',len(datalist))\n",
        "shuffle(datalist)\n",
        "print('dataset length is ',len(datalist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTlGFN2zUgsh"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "learning_rate = 0.001 #0.001\n",
        "num_epochs = 50\n",
        "patience = 8\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqSqRAJo3Um-"
      },
      "outputs": [],
      "source": [
        "class PhraseEmbeddingDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, input):\n",
        "\n",
        "        self.input = input\n",
        "        #input schema is a triplet of anchor|positive|negative examples\n",
        "        print(type(self.input))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.input[idx]\n",
        "        # y = self.output[idx]\n",
        "        # return X,y\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3X828UaLfTq"
      },
      "outputs": [],
      "source": [
        "full_dataset = PhraseEmbeddingDataset(datalist)\n",
        "train_size = int(0.75 * len(full_dataset))\n",
        "validation_size = len(full_dataset) - train_size\n",
        "print('Training set:', train_size)\n",
        "print('Validation set:', validation_size)\n",
        "\n",
        "# print(type(training_set))\n",
        "val_batch=int(validation_size/11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA9vkk_ewmdW"
      },
      "outputs": [],
      "source": [
        "training_set, validation_set = torch.utils.data.random_split(full_dataset, [train_size, validation_size])\n",
        "\n",
        "training_generator = torch.utils.data.DataLoader(training_set, batch_size=2048, shuffle=False)# 512 for soft-knn and 2048 for triplet\n",
        "validation_generator = torch.utils.data.DataLoader(validation_set, batch_size=val_batch, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7JYfopWPAOP"
      },
      "outputs": [],
      "source": [
        "def save_ckp(state, is_best, checkpoint_dir):\n",
        "    # f_path = checkpoint_dir + '/checkpoint.pt' \n",
        "    f_path = checkpoint_dir + '/checkpoint_model300.pt'\n",
        "    # f_path = checkpoint_dir + '/checkpoint_model300_softknn.pt'\n",
        "    torch.save(state, f_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3vb4DntcFbQ"
      },
      "outputs": [],
      "source": [
        "# # soft knn\n",
        "# candidate_tup = index_candidate_dict[datalist[1430][1]]\n",
        "# embeddingSize = list(candidate_embedding_dict[candidate_tup][datalist[1430][0]].shape)[0]\n",
        "\n",
        "# triplet\n",
        "embeddingSize = datalist[1430][0].shape[0]\n",
        "output_embedding_size = 300\n",
        "print(embeddingSize,output_embedding_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc0WRh-yhWrU"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.BatchNorm1d):\n",
        "        nn.init.constant_(m.weight.data, 1)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_uniform_(m.weight.data)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dmz6tPuP52R"
      },
      "outputs": [],
      "source": [
        "# Initialize network\n",
        "phraseEmbeddingModel = PhraseEmbedding(embeddingSize, output_embedding_size, device).to(device)\n",
        "# phraseEmbeddingModel.apply(initialize_weights)\n",
        "\n",
        "#Loss and Optimizer\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "criterion = TripletLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MskESAKT7p3y"
      },
      "outputs": [],
      "source": [
        "# phraseEmbeddingModel = PhraseEmbeddingI(embeddingSize, output_embedding_size, device,candidate_embedding_dict,index_candidate_dict).to(device)\n",
        "\n",
        "# #Loss and Optimizer\n",
        "# optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0005)\n",
        "# criterion = SoftKNNLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyyjJ1lpaITg"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir='entityEmbedding/model_checkpoints_ner'\n",
        "ckp_path = \"entityEmbedding/model_checkpoints_ner/checkpoint_model300.pt\" #300\n",
        "# ckp_path = \"entityEmbedding/model_checkpoints_ner/checkpoint_model300_softknn.pt\" #300\n",
        "\n",
        "# if(path.exists(ckp_path)):\n",
        "#     # load the saved checkpoint\n",
        "#     phraseEmbeddingModel, optimizer, start_epoch = load_ckp(ckp_path, phraseEmbeddingModel, optimizer)\n",
        "\n",
        "#     print(\"starting with model at epoch:\", start_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO4FgsQJQ5vp"
      },
      "outputs": [],
      "source": [
        "# Train Network\n",
        "history_training= []\n",
        "history_validation = []\n",
        "best_loss = np.float('inf')\n",
        "best_checkpoint = {}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_batch_loss=[]\n",
        "    mbcount=0\n",
        "    for batch_idx, batch_data in enumerate(training_generator):\n",
        "        # print(type(data),len(data),len(data[0]),len(data[1]),len(data[2]))\n",
        "        optimizer.zero_grad()\n",
        "        out = phraseEmbeddingModel(batch_data)\n",
        "        # print('network output type:',type(out))\n",
        "        loss = criterion(out)\n",
        "        # print('mini batch loss:',loss.item())\n",
        "        training_batch_loss.append(loss.item())\n",
        "         \n",
        "         \n",
        "\n",
        "        # backward\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(phraseEmbeddingModel.parameters(), max_norm=2.0, norm_type=2)\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "        mbcount+=1\n",
        "        if(mbcount%1000==0):\n",
        "            print(mbcount//1000)\n",
        "    combined_training_loss = np.mean(training_batch_loss)\n",
        "    print('\\nEpoch',str(epoch+1),' Training Loss:', combined_training_loss)\n",
        "    history_training.append(combined_training_loss)\n",
        "\n",
        "    #Validation: DO NOT BACKPROPAGATE HERE\n",
        "    validation_batch_loss = []\n",
        "    print_only_one=True\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(validation_generator):\n",
        "            # if(print_only_one):\n",
        "                # print(len(data[0]),len(data[1]))\n",
        "                # print(type(data))\n",
        "                # print_only_one=False\n",
        "            out = phraseEmbeddingModel(data)\n",
        "            loss = criterion(out)\n",
        "            validation_batch_loss.append(loss.item())\n",
        "            # print(validation_batch_loss)\n",
        "    combined_validation_loss= np.mean(validation_batch_loss)\n",
        "    history_validation.append(combined_validation_loss)\n",
        "    # if(((epoch+1)%10==0)|(epoch == (num_epochs-1))):\n",
        "    print('\\nEpoch',str(epoch+1),' Validation Loss:',combined_validation_loss)\n",
        "    print('=====================================================================\\n')\n",
        "    \n",
        "    if(combined_validation_loss<best_loss):\n",
        "        best_loss = combined_validation_loss\n",
        "        print('saving this checkpoint')\n",
        "        best_checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': phraseEmbeddingModel.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "        no_improvement_counter=0\n",
        "    else:\n",
        "        no_improvement_counter+=1\n",
        "        if(no_improvement_counter>patience):\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0YGzUWQ12A1"
      },
      "outputs": [],
      "source": [
        "save_ckp(best_checkpoint, True, checkpoint_dir) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjyU4EUATYTr"
      },
      "outputs": [],
      "source": [
        "# # tensor = -1*torch.Tensor([3,2,1])\n",
        "# # sum = torch.sum(tensor) \n",
        "# # sum_int = int(sum.item())\n",
        "# # print(tensor, type(sum), sum_int)\n",
        "\n",
        "# def nxn_cos_sim(A, B, dim=1, eps=1e-8):\n",
        "#       numerator = A @ B.T\n",
        "#       A_l2 = torch.mul(A, A).sum(axis=dim)\n",
        "#       B_l2 = torch.mul(B, B).sum(axis=dim)\n",
        "#       denominator = torch.max(torch.sqrt(torch.outer(A_l2, B_l2)), torch.tensor(eps))\n",
        "#       return torch.div(numerator, denominator)\n",
        "\n",
        "# example_2D_list1 = [[0.2423,-0.6679,0.8277],[0.2423,-0.6679,0.8277],[0.2423,-0.6679,0.8277]]\n",
        "# example_2D_Tensor1 = torch.tensor(example_2D_list1)\n",
        "\n",
        "# example_2D_list2 = [[0.2245,-2.2268,0.4577],[0.2245,-2.2268,0.3295],[0.2245,-2.2268,0.4577]]\n",
        "# example_2D_Tensor2 = torch.tensor(example_2D_list2)\n",
        "\n",
        "\n",
        "# sim_matrix = nxn_cos_sim(example_2D_Tensor1,example_2D_Tensor2)\n",
        "# print(sim_matrix)\n",
        "\n",
        "# for i in range(sim_matrix.size()[0]):\n",
        "#     print(i,sim_matrix[i])\n",
        "#     exp_tensor = torch.exp(sim_matrix[i])\n",
        "#     print(exp_tensor)\n",
        "#     sum_tensor = torch.sum(exp_tensor)\n",
        "#     print(sum_tensor)\n",
        "#     weights = torch.div(exp_tensor,sum_tensor)\n",
        "#     print('weights:',weights)\n",
        "#     print(weights[:,None]*example_2D_Tensor2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gcq0b510200"
      },
      "source": [
        "## **Training the Entity Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vybekKdR3jVO"
      },
      "outputs": [],
      "source": [
        "trainset_ec, tokenizedtestset_ec, tweet_to_sentences_w_annotation_ec = preprocess('deduplicated_test_WTypes.csv')\n",
        "candidateBaseHeaders_alt=['candidate', 'batch', 'length','cumulative','class']\n",
        "\n",
        "predictions=[]\n",
        "tokenized_sentences=[]\n",
        "count=0\n",
        "with torch.no_grad():\n",
        "    for train_record in trainset_ec:\n",
        "        \n",
        "        sentence = normalizeTweet(train_record)\n",
        "        tweetWordList = sentence.split()\n",
        "        enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "\n",
        "        tokenized_input=tokenizer(sentence)\n",
        "        initial_input_ids = torch.tensor([tokenizer.encode(sentence)])\n",
        "\n",
        "        initial_input_ids = initial_input_ids[:,:128]\n",
        "        token_dict = {x : tokenizer.encode(x, add_special_tokens=False) for x in sentence.split()}\n",
        "\n",
        "        input_ids = initial_input_ids.to(device)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "        output = alt_model(input_ids)\n",
        "        \n",
        "        token_embeddings=output.hidden_states[-1].squeeze()[1:-1] # we dont need embeddings for CLS and EOS\n",
        "\n",
        "        prediction = (torch.argmax(output.logits, axis=2))\n",
        "        prediction = prediction.cpu().numpy().reshape(-1)\n",
        "        # prediction_labels=[label_list[l].split('-')[0] for l in prediction]\n",
        "        prediction_labels=[label_list[l] for l in prediction]\n",
        "        # prediction_labels=collate_token_labels(token_dict, prediction_labels[1:-1])\n",
        "        prediction_labels, entity_aware_embeddings = collate_token_label_embedding(tweetWordList, token_dict, prediction_labels[1:-1], token_embeddings)\n",
        "\n",
        "        # print(len(enumerated_tweetWordList),len(token_dict.keys()),len(entity_aware_embeddings),len(prediction_labels))\n",
        "\n",
        "        assert len(enumerated_tweetWordList)==len(entity_aware_embeddings)\n",
        "        assert len(prediction_labels)==len(enumerated_tweetWordList)\n",
        "\n",
        "        predictions.append(prediction_labels)\n",
        "        tokenized_sentences.append((tweetWordList,entity_aware_embeddings))\n",
        "        count+=1\n",
        "\n",
        "print(len(predictions),len(tokenized_sentences))\n",
        "\n",
        "local_ner_arrays=[]\n",
        "for n, sentence_tup in enumerate(tokenized_sentences):  \n",
        "    sentence = sentence_tup[0]\n",
        "    assert (len(sentence)==len(predictions[n]))\n",
        "    word_tag_tuples=zip(sentence,predictions[n])\n",
        "    entities_from_sentence=get_entities(word_tag_tuples)\n",
        "    local_ner_arrays.append(entities_from_sentence)\n",
        "print('tally:',len(tokenized_sentences),len(local_ner_arrays))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz-gnTm84tAV"
      },
      "outputs": [],
      "source": [
        "phraseEmbeddingModel = PhraseEmbedding(768, output_embedding_size, device).to(device) #triplet\n",
        "# # phraseEmbeddingModel = PhraseEmbeddingI(768, output_embedding_size, device).to(device) #soft-knn\n",
        "# phraseEmbeddingModel.apply(initialize_weights)\n",
        "\n",
        "# #Loss and Optimizer\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# # define checkpoint saved path for entity phrase embedder\n",
        "ckp_path = \"entityEmbedding/model_checkpoints_ner/checkpoint_model300.pt\" #300 triplet\n",
        "# # ckp_path = \"entityEmbedding/model_checkpoints_ner/checkpoint_model300_softknn.pt\" #300 tuplet\n",
        "\n",
        "if(path.exists(ckp_path)):\n",
        "    # load the saved checkpoint\n",
        "    entityPhraseEmbedder, optimizer, start_epoch = load_ckp(ckp_path, phraseEmbeddingModel, optimizer)\n",
        "    print(\"starting with model at epoch:\", start_epoch)\n",
        "\n",
        "#comment this out if collecting samples for training the entity phrase embedder\n",
        "entityPhraseEmbedder.eval()\n",
        "\n",
        "candidate_embedding_dict = defaultdict(list)\n",
        "candidateBase_dict_alt_ec = {} #<batch, length, cumulative, class>\n",
        "non_entity_count = 0\n",
        "entity_count = 0\n",
        "for tweetID in tweet_to_sentences_w_annotation_ec.keys():\n",
        "    annotated_token_list = []\n",
        "    annotated_mention_list=tweet_to_sentences_w_annotation_ec[tweetID][1]\n",
        "    tweet_token_list=[]\n",
        "    token_embedding_list=[]\n",
        "    local_mentions_list=[]\n",
        "    idRange=tweet_to_sentences_w_annotation_ec[tweetID][0]\n",
        "\n",
        "    for sentID in range(idRange[0],idRange[1]):\n",
        "        tweet_token_list+=tokenized_sentences[sentID][0]\n",
        "        token_embedding_list+=tokenized_sentences[sentID][1]\n",
        "        local_mentions_list+=local_ner_arrays[sentID]\n",
        "    \n",
        "    assert len(tweet_token_list)==len(token_embedding_list)\n",
        "    print(tweetID, tweet_token_list)\n",
        "    print('annotated_mention_list:',annotated_mention_list)\n",
        "\n",
        "    \n",
        "    annotated_candidates = set([mention_tup[0].lower().strip() for mention_tup in annotated_mention_list])\n",
        "    for mention_tup in annotated_mention_list:\n",
        "        annotated_token_list+= mention_tup[0].lower().strip().split()\n",
        "    annotated_token_list = set(annotated_token_list)\n",
        "\n",
        "    #------------------------ negative candidate mining ------------------------\n",
        "    non_entity_list = list(set([elem.strip().lower() for elem in tweet_token_list]).intersection(canonical_ne_list)-annotated_candidates)\n",
        "\n",
        "    \n",
        "    if(non_entity_list):\n",
        "        print('non_entity_list:',non_entity_list)\n",
        "    \n",
        "    tweet_token_ind = 0\n",
        "    while((len(non_entity_list)>0)&(tweet_token_ind < len(tweet_token_list))):\n",
        "        #must pop from the left\n",
        "        non_entity_candidate = non_entity_list.pop(0)\n",
        "        non_entity_candidate, nonentity_type = non_entity_candidate.lower(),'ne'\n",
        "        non_entity_candidate_tokens = non_entity_candidate.split()\n",
        "        while(tweet_token_ind < len(tweet_token_list)):\n",
        "            if((' '.join(tweet_token_list[tweet_token_ind:tweet_token_ind+len(non_entity_candidate_tokens)])).strip().lower()!=non_entity_candidate):\n",
        "                # print(tweet_token_ind)\n",
        "                tweet_token_ind+=1\n",
        "            else:\n",
        "                ne_candidate=(' '.join(tweet_token_list[tweet_token_ind:tweet_token_ind+len(non_entity_candidate_tokens)])).strip().lower()\n",
        "                ne_candidate_token_embeddings = torch.stack(token_embedding_list[tweet_token_ind:tweet_token_ind+len(non_entity_candidate_tokens)]) #2d tensor of ne candidate token embeddings\n",
        "                \n",
        "                tensor_inter = torch.mean(ne_candidate_token_embeddings,dim=0)\n",
        "                tensor_inter_norm = torch.norm(tensor_inter, p=2,dim=0)\n",
        "                ne_candidate_tokens_avg_embedding = tensor_inter/tensor_inter_norm\n",
        "\n",
        "                \n",
        "\n",
        "                # print(ne_candidate, ne_candidate_tokens_avg_embedding.shape)\n",
        "                if((non_entity_candidate, nonentity_type) not in candidate_embedding_dict):\n",
        "                    non_entity_count+=1\n",
        "                    candidateBase_dict_alt_ec[non_entity_candidate+'||'+nonentity_type] = [0,len(non_entity_candidate_tokens),1,entity_types_dict[nonentity_type]]\n",
        "                else:\n",
        "                    candidateBase_dict_alt_ec[non_entity_candidate+'||'+nonentity_type][2] += 1\n",
        "                \n",
        "\n",
        "                #---------------------- When collecting mention phrase embeddings to train the entity classifier ----------------------\n",
        "                # !! must unsqueeze at test time with embedder else batchnorm throws error\n",
        "                ne_candidate_tokens_avg_embedding = ne_candidate_tokens_avg_embedding.unsqueeze(0)\n",
        "                assert torch.isnan(ne_candidate_tokens_avg_embedding).any() == False\n",
        "                ne_candidate_phrase_embedding = (entityPhraseEmbedder.getEmbedding(ne_candidate_tokens_avg_embedding)).squeeze(0)\n",
        "                assert torch.isnan(ne_candidate_phrase_embedding).any() == False\n",
        "                # print(ne_candidate_tokens_avg_embedding.shape, ne_candidate_phrase_embedding.shape)\n",
        "                candidate_embedding_dict[(non_entity_candidate, nonentity_type)].append(ne_candidate_phrase_embedding)\n",
        "\n",
        "                tweet_token_ind+=len(non_entity_candidate_tokens)\n",
        "                break\n",
        "    #------------------------ negative candidate mining ------------------------\n",
        "\n",
        "    #------------------------ positive candidate mining ------------------------\n",
        "    tweet_token_ind = 0\n",
        "    while((len(annotated_mention_list)>0)&(tweet_token_ind < len(tweet_token_list))):\n",
        "        #must pop from the left\n",
        "        annotated_candidate_tuple = annotated_mention_list.pop(0)\n",
        "        annotated_candidate, entity_type = annotated_candidate_tuple[0].lower(),annotated_candidate_tuple[1].lower()\n",
        "        candidate_tokens = annotated_candidate.split()\n",
        "\n",
        "        print(annotated_candidate)\n",
        "        while((' '.join(tweet_token_list[tweet_token_ind:tweet_token_ind+len(candidate_tokens)])).strip().lower()!=annotated_candidate):\n",
        "            tweet_token_ind+=1\n",
        "        candidate=(' '.join(tweet_token_list[tweet_token_ind:tweet_token_ind+len(candidate_tokens)])).strip().lower()\n",
        "        candidate_token_embeddings = torch.stack(token_embedding_list[tweet_token_ind:tweet_token_ind+len(candidate_tokens)]) #2d tensor of candidate token embeddings\n",
        "        tensor_inter = torch.mean(candidate_token_embeddings,dim=0)\n",
        "        tensor_inter_norm = torch.norm(tensor_inter, p=2,dim=0)\n",
        "        candidate_tokens_avg_embedding = tensor_inter/tensor_inter_norm\n",
        "        # print(candidate,annotated_candidate,candidate_token_embeddings.shape)\n",
        "\n",
        "        if((annotated_candidate, entity_type) not in candidate_embedding_dict):\n",
        "            entity_count+=1\n",
        "            candidateBase_dict_alt_ec[annotated_candidate+'||'+entity_type] = [0,len(candidate_tokens),1,entity_types_dict[entity_type]]\n",
        "        else:\n",
        "            candidateBase_dict_alt_ec[annotated_candidate+'||'+entity_type][2] += 1\n",
        "\n",
        "        #---------------------- When collecting mention phrase embeddings to train the entity classifier ----------------------\n",
        "        # !! must unsqueeze at test time with embedder else batchnorm throws error\n",
        "        candidate_tokens_avg_embedding = candidate_tokens_avg_embedding.unsqueeze(0)\n",
        "        assert torch.isnan(candidate_tokens_avg_embedding).any() == False\n",
        "        candidate_phrase_embedding = (entityPhraseEmbedder.getEmbedding(candidate_tokens_avg_embedding)).squeeze(0)\n",
        "        assert torch.isnan(candidate_phrase_embedding).any() == False\n",
        "        # print(candidate_tokens_avg_embedding.shape, candidate_phrase_embedding.shape)\n",
        "        candidate_embedding_dict[(annotated_candidate, entity_type)].append(candidate_phrase_embedding)\n",
        "\n",
        "        tweet_token_ind+=len(candidate_tokens)\n",
        "    #------------------------ positive candidate mining ------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-El0QCwZvRa8"
      },
      "outputs": [],
      "source": [
        "print(len(candidateBase_dict_alt_ec))\n",
        "print(candidateBase_dict_alt_ec.keys())\n",
        "# candidateBaseHeaders_alt\n",
        "candidate_featureBase_DF_alt = pd.DataFrame.from_dict(candidateBase_dict_alt_ec, orient='index')\n",
        "candidate_featureBase_DF_alt.columns = candidateBaseHeaders_alt[1:]\n",
        "candidate_featureBase_DF_alt.index.name = candidateBaseHeaders_alt[0]\n",
        "candidate_featureBase_DF_alt  = candidate_featureBase_DF_alt.reset_index(drop=False)\n",
        "print(candidate_featureBase_DF_alt.columns.tolist())\n",
        "print(len(candidateBase_dict_alt_ec),len(candidate_embedding_dict))\n",
        "print(candidate_featureBase_DF_alt.head())\n",
        "\n",
        "entity_types = set(candidate_featureBase_DF_alt['class'].tolist())\n",
        "for entity_type in entity_types:\n",
        "    print(len(candidate_featureBase_DF_alt[candidate_featureBase_DF_alt['class']==entity_type]))\n",
        "\n",
        "print('=============')\n",
        "\n",
        "\n",
        "# entity_types = set(candidate_featureBase_DF_alt_filtered['class'].tolist())\n",
        "# for entity_type in entity_types:\n",
        "#     print(len(candidate_featureBase_DF_alt_filtered[candidate_featureBase_DF_alt_filtered['class']==entity_type]))\n",
        "# print(len(candidate_featureBase_DF_alt_filtered))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT0n8825I_v2"
      },
      "outputs": [],
      "source": [
        "entity_classifier_alt = EntityClassifierAlt(True, device, candidate_featureBase_DF_alt, candidate_embedding_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wbn1mwrwIZX"
      },
      "outputs": [],
      "source": [
        "# p= 0.8297872340425532\n",
        "# r= 0.6933962264150944\n",
        "\n",
        "# f=2*p*r/(p+r)\n",
        "# print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQy_LjrMrhfw"
      },
      "outputs": [],
      "source": [
        "# import torch.nn.functional as F\n",
        "# data1=np.array([0.0,0.0,0.0])\n",
        "# x_data1 = torch.tensor(data1)\n",
        "\n",
        "# tensor_inter_norm = torch.norm(x_data1, p=2,dim=0)\n",
        "# print(tensor_inter_norm)\n",
        "# updated_norm = torch.add(tensor_inter_norm, 1e-6)\n",
        "# print(x_data1/tensor_inter_norm)\n",
        "# print(x_data1/updated_norm)\n",
        "\n",
        "# data2=np.array([2.0,3.0,4.0])\n",
        "# x_data2 = torch.tensor(data2)\n",
        "\n",
        "# diff = x_data1-x_data2\n",
        "# print(x_data1.shape,x_data2.shape,diff.shape)\n",
        "# print(diff)\n",
        "# losses=torch.relu(diff)\n",
        "# print(losses)\n",
        "# print(losses.mean())\n",
        "# print('=====')\n",
        "# # print(x_data.shape)\n",
        "# # stacked=torch.stack([x_data1,x_data2])\n",
        "# # print(stacked.shape)\n",
        "# tensorlist = [x_data1,x_data2]\n",
        "# # mean = torch.mean(torch.stack(tensorlist), axis=1)\n",
        "# # sd = torch.std(torch.stack(tensorlist), axis=1)\n",
        "# # print('mean:', mean)\n",
        "# # print('sd:' , sd)\n",
        "# for tensor in tensorlist:\n",
        "#     print(tensor)\n",
        "#     tensor_norm = torch.norm(tensor, p=2,dim=0)\n",
        "#     print(tensor_norm)\n",
        "#     print(tensor/tensor_norm)\n",
        "#     print('----')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6a02VCgGRl9"
      },
      "source": [
        "## **Phase I: Local NER to collect entity candidates and Token Contextual Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzwyqfxcHEF2"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import emoji\n",
        "from emoji import demojize\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas  as pd\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from collections import Iterable, OrderedDict\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "#from datasketch import MinHash, MinHashLSH\n",
        "# import NE_candidate_module as ne\n",
        "# import Mention\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import copy\n",
        "import trie as trie\n",
        "import ast\n",
        "\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, set_seed\n",
        "import copy\n",
        "\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "# import matplotlib.pyplot as plt\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tempList=[\"i\",\"and\",\"or\",\"other\",\"since\",\"hence\",\"onto\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"probably\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
        "for item in tempList:\n",
        "    if item not in cachedStopWords:\n",
        "        cachedStopWords.append(item)\n",
        "cachedStopWords.remove(\"don\")\n",
        "# cachedStopWords.remove(\"your\")\n",
        "# cachedStopWords.remove(\"up\")\n",
        "\n",
        "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
        "prep_list=[\"of\",\"v.\"]  #includes common conjunction as well\n",
        "# prep_list=[]\n",
        "# article_list=[]\n",
        "article_list=[\"a\",\"an\",\"the\"]\n",
        "conjoiner=[\"de\"]\n",
        "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
        "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "chat_word_list=[\"nope\",\"gee\",\"hmm\",\"httpurl\",\"n't\",\"pls\",\"bye\",\"€\",\"vs\",\"ouch\",\"am\",\"pm\",\"omw\",\"cbb\",\"tmi\",\"http\",\"https\",\"tv\",\"tmw\",\"og\",\"psst\",\"b.s\",\"thanku\",\"em\",\"ip\",\"qft\",\"ima\",\"icymi\",\"bdsm\",\"ah\",\"ive\",\"qt\",\"dj\",\"dm\",\"pts\",\"pt\",\"yrs\",\"congrat\",\"haueheuaeh\",\"ahushaush\",\"jr\",\"please\",\"retweet\",\"2mrw\",\"2moro\",\"4get\",\"ooh\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
        "string.punctuation=string.punctuation+'…‘’'\n",
        "\n",
        "\n",
        "class LocalNERModule():\n",
        "    def __init__(self, sentenceTokenizer, nerTokenizer, nerEngine, device):\n",
        "        self.counter=0\n",
        "\n",
        "        if(sentenceTokenizer):\n",
        "            self.my_sentence_tokenizer = sentenceTokenizer\n",
        "        else:\n",
        "            nltk.download('gutenberg')\n",
        "            gutenberg_text = \"\"\n",
        "            for file_id in gutenberg.fileids():\n",
        "                gutenberg_text += gutenberg.raw(file_id)\n",
        "            tokenizer_trainer = PunktTrainer()\n",
        "            tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "            tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "            self.my_sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('ret')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('rep')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('mr')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('ms')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('mrs')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('v')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('vs')\n",
        "\n",
        "        self.quickRegex=re.compile(\"[a-z]+\")\n",
        "\n",
        "        self.tweet_to_sentences_w_annotation = {}\n",
        "        self.device = device\n",
        "        self.apostrophe_list =[\"'s\",'’s','s']\n",
        "        # self.label_list = ['O','B','I']\n",
        "        self.label_list = ['O','B-ORG','I-ORG','B-MISC','I-MISC','B-LOC','I-LOC','B-PER','I-PER']\n",
        "\n",
        "        self.tweetTokenizer = TweetTokenizer()\n",
        "\n",
        "        self.contextual_embeddings = {}\n",
        "\n",
        "        print('Starting Local NER Engine!')\n",
        "        self.expanded_label_dict={0:'O', 1:'B-corporation', 2:'I-corporation', 3:'B-creative-work', 4:'I-creative-work', 5:'B-group', 6:'I-group', 7:'B-location', 8:'I-location', 9:'B-person', 10:'I-person', 11:'B-product', 12:'I-product'}\n",
        "        self.BIO_dict={'O':0,'B':1,'I':2}\n",
        "        self.BIO_type_dict={'O':0, 'B-ORG':1, 'I-ORG':2, 'B-MISC':3, 'I-MISC':4, 'B-LOC':5, 'I-LOC':6, 'B-PER':7, 'I-PER':8}\n",
        "        self.label_map_dict={'O':'O', 'B-corporation':'B-ORG', 'I-corporation':'I-ORG', 'B-creative-work':'B-MISC', 'I-creative-work':'I-MISC', 'B-group':'B-MISC', 'I-group':'I-MISC', 'B-location':'B-LOC', 'I-location':'I-LOC', 'B-person':'B-PER', 'I-person':'I-PER', 'B-product':'B-MISC', 'I-product':'I-MISC'}\n",
        "\n",
        "\n",
        "        if((nerTokenizer is not None)&(nerEngine is not None)):\n",
        "            self.nerTokenizer = nerTokenizer\n",
        "            self.localNEREngine = nerEngine\n",
        "        else:\n",
        "            self.train_engine()\n",
        "\n",
        "    def train_engine(self):\n",
        "        task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "        model_checkpoint = \"vinai/bertweet-base\"\n",
        "        batch_size = 16\n",
        "        # set_seed(42)\n",
        "        datasets = load_dataset(\"wnut_17\")\n",
        "        self.nerTokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
        "        label_all_tokens = True\n",
        "        tokenized_datasets = datasets.map(self.tokenize_and_align_labels)\n",
        "        data_collator = DataCollatorForTokenClassification(self.nerTokenizer)\n",
        "        self.metric = load_metric(\"seqeval\")\n",
        "        self.localNEREngine = AutoModelForTokenClassification.from_pretrained(\"vinai/bertweet-base\", output_hidden_states=True, num_labels=len(self.label_list))\n",
        "        alt_training_args = TrainingArguments(\n",
        "            f\"test-{task}\",\n",
        "            evaluation_strategy = \"epoch\",\n",
        "            learning_rate=1e-5,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "        )\n",
        "        alt_trainer = Trainer(\n",
        "        self.localNEREngine,\n",
        "        alt_training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=self.nerTokenizer,\n",
        "        compute_metrics=self.compute_metrics\n",
        "        )\n",
        "        alt_trainer.train()\n",
        "\n",
        "        # tokenizer.save_pretrained('test-ner/')\n",
        "        # alt_model.save_pretrained('test-ner/')\n",
        "\n",
        "    def tokenize_and_align_labels(self,example):\n",
        "        \n",
        "        tokenized_ds_input = self.nerTokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "        inputId_to_token_dict={}\n",
        "        for index, token in enumerate(example[\"tokens\"]):\n",
        "            values=self.nerTokenizer.encode(token, add_special_tokens=False, truncation=True)\n",
        "            for value in values:\n",
        "                try:\n",
        "                    inputId_to_token_dict[value].append(index)\n",
        "                except KeyError:\n",
        "                    inputId_to_token_dict[value]=[index]\n",
        "        labels=[]\n",
        "        for inputID in tokenized_ds_input['input_ids']:\n",
        "            try:\n",
        "                index_list=copy.deepcopy(inputId_to_token_dict[inputID])\n",
        "                index_to_address=index_list.pop(0)\n",
        "\n",
        "                label = self.BIO_type_dict[self.label_map_dict[self.expanded_label_dict[example['ner_tags'][index_to_address]]]]\n",
        "                # label = example['ner_tags'][index_to_address]\n",
        "\n",
        "                labels.append(label)\n",
        "                inputId_to_token_dict[inputID]=index_list\n",
        "            except KeyError:\n",
        "                labels.append(-100)\n",
        "\n",
        "        assert (len(tokenized_ds_input['input_ids']) == len(labels))\n",
        "        tokenized_ds_input['labels']=labels\n",
        "        \n",
        "        return tokenized_ds_input\n",
        "\n",
        "    def compute_metrics(self, p):\n",
        "        # print(p.shape)\n",
        "        output, labels = p\n",
        "\n",
        "        # print(len(predictions))\n",
        "        # print(predictions[0].shape)\n",
        "        # for elem in predictions[1]:\n",
        "        #   print(elem.shape)\n",
        "\n",
        "        predictions, _ = output\n",
        "        \n",
        "        predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "        # Remove ignored index (special tokens)\n",
        "        true_predictions = [\n",
        "            [self.label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "        true_labels = [\n",
        "            [self.label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "\n",
        "        results = self.metric.compute(predictions=true_predictions, references=true_labels)\n",
        "        return {\n",
        "            \"precision\": results[\"overall_precision\"],\n",
        "            \"recall\": results[\"overall_recall\"],\n",
        "            \"f1\": results[\"overall_f1\"],\n",
        "            \"accuracy\": results[\"overall_accuracy\"],\n",
        "        }\n",
        "\n",
        "    def is_float(self,string):\n",
        "        try:\n",
        "            f=float(string)\n",
        "            if(f==0.0):\n",
        "              return True\n",
        "            else:\n",
        "              return ((f) and (string.count(\".\")==1))\n",
        "      #return True# True if string is a number with a dot\n",
        "        except ValueError:  # if string is not a number\n",
        "          return False\n",
        "\n",
        "    def collate_token_label_embedding(self, tweetWordList, token_dict, prediction_labels, entity_embeddings):\n",
        "        counter=0\n",
        "        collated_labels=[]\n",
        "        collated_entity_embeddings=[]\n",
        "        for word in tweetWordList:\n",
        "            vals=token_dict[word]\n",
        "            # print(word,vals)\n",
        "            if(counter<len(prediction_labels)):\n",
        "                labels=prediction_labels[counter:counter+len(vals)]\n",
        "                token_entity_embeddings=entity_embeddings[counter:counter+len(vals)]\n",
        "        #         print(token_entity_embeddings.shape)\n",
        "                mean_tensor = torch.mean(token_entity_embeddings,dim=0)\n",
        "                mean_tensor[torch.isnan(mean_tensor)] = 0\n",
        "                collated_entity_embeddings.append(mean_tensor)\n",
        "        #         print(collated_entity_embeddings)\n",
        "                if('I-PER' in labels):\n",
        "                    collated_labels.append('I-PER')\n",
        "                elif('I-LOC' in labels):\n",
        "                    collated_labels.append('I-LOC')\n",
        "                elif('I-ORG' in labels):\n",
        "                    collated_labels.append('I-ORG')\n",
        "                elif('I-MISC' in labels):\n",
        "                    collated_labels.append('I-MISC')\n",
        "                elif('B-PER' in labels):\n",
        "                    collated_labels.append('B-PER')\n",
        "                elif('B-LOC' in labels):\n",
        "                    collated_labels.append('B-LOC')\n",
        "                elif('B-ORG' in labels):\n",
        "                    collated_labels.append('B-ORG')\n",
        "                elif('B-MISC' in labels):\n",
        "                    collated_labels.append('B-MISC')\n",
        "                else:\n",
        "                    collated_labels.append('O')\n",
        "                counter+=len(vals)\n",
        "            else:\n",
        "                collated_labels.append('O')\n",
        "                collated_entity_embeddings.append(torch.zeros(768).to(self.device))\n",
        "        assert len(collated_labels)==len(collated_entity_embeddings)\n",
        "        return collated_labels,collated_entity_embeddings\n",
        "\n",
        "\n",
        "    def get_entities(self, word_tag_tuples):\n",
        "        mentions=[]\n",
        "        candidateMention=''\n",
        "        positions=[]\n",
        "        type_tag=''\n",
        "        \n",
        "        #emoji.get_emoji_regexp().sub(u'', candidateMention)\n",
        "        for index, tup in enumerate(word_tag_tuples):\n",
        "            candidate=tup[0]\n",
        "            tag=tup[1]\n",
        "            if(tag=='O'):\n",
        "                if(candidateMention):\n",
        "                    if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                        mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                        if mention_to_add.endswith(\"'s\"):\n",
        "                            li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        elif mention_to_add.endswith(\"’s\"):\n",
        "                            li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        else:\n",
        "                            mention_to_add=mention_to_add\n",
        "                        if(mention_to_add!=''):\n",
        "                            try:\n",
        "                                assert len(mention_to_add.split()) == len(positions)\n",
        "                                mentions.append((mention_to_add,type_tag,positions))\n",
        "                            except AssertionError:\n",
        "                                print(word_tag_tuples)\n",
        "                                print(mention_to_add,type_tag,positions)\n",
        "                                return\n",
        "                candidateMention=''\n",
        "                positions=[]\n",
        "                type_tag=''\n",
        "            else:\n",
        "                boundary_tag = tag.split('-')[0]\n",
        "                type_tag = tag.split('-')[1]\n",
        "                if (boundary_tag=='B'):\n",
        "                    if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                        mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                        if mention_to_add.endswith(\"'s\"):\n",
        "                            li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        elif mention_to_add.endswith(\"’s\"):\n",
        "                            li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        else:\n",
        "                            mention_to_add=mention_to_add\n",
        "                        if(mention_to_add!=''):\n",
        "                            try:\n",
        "                                assert len(mention_to_add.split()) == len(positions)\n",
        "                                mentions.append((mention_to_add,type_tag,positions))\n",
        "                            except AssertionError:\n",
        "                                print(word_tag_tuples)\n",
        "                                print(mention_to_add,type_tag,positions)\n",
        "                                return\n",
        "                    if((candidate.strip() not in string.punctuation)&(emoji.get_emoji_regexp().sub(u'', candidate).strip(string.punctuation).lower().strip()!='')&(candidate.strip().strip(string.punctuation) not in self.apostrophe_list)):\n",
        "                        candidateMention=candidate\n",
        "                        positions=[index]\n",
        "                else:\n",
        "                    if((candidate.strip() not in string.punctuation)&(emoji.get_emoji_regexp().sub(u'', candidate).strip(string.punctuation).lower().strip()!='')&(candidate.strip().strip(string.punctuation) not in self.apostrophe_list)):\n",
        "                        candidateMention+=\" \"+candidate\n",
        "                        positions.append(index)\n",
        "            # if (tag=='B'):\n",
        "            #     if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))):\n",
        "            #         mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "            #         if(mention_to_add):\n",
        "            #             mentions.append(mention_to_add)\n",
        "            #     candidateMention=candidate\n",
        "            # else:\n",
        "            #     candidateMention+=\" \"+candidate\n",
        "        if(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).strip()):\n",
        "            if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))&(candidate.strip().strip(string.punctuation) not in self.apostrophe_list)):\n",
        "                mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                if(mention_to_add!=''):\n",
        "                    try:\n",
        "                        assert len(mention_to_add.split()) == len(positions)\n",
        "                        mentions.append((mention_to_add,type_tag,positions))\n",
        "                    except AssertionError:\n",
        "                        print(word_tag_tuples)\n",
        "                        print(mention_to_add,type_tag,positions)\n",
        "                        return\n",
        "            # mentions.append(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip())\n",
        "        # print('extracted mentions:', mentions)\n",
        "        return mentions\n",
        "\n",
        "    def normalize_to_sentences(self, text):\n",
        "        tweetSentences=list(filter (lambda sentence: len(sentence)>1, text.split('\\n')))\n",
        "        tweetSentenceList_inter=self.custom_flatten(list(map(lambda sentText: self.my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
        "        tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
        "        return tweetSentenceList\n",
        "\n",
        "    def custom_flatten(self, mylist, outlist, ignore_types=(str, bytes, int)):\n",
        "        \n",
        "        if (mylist !=[]):\n",
        "            for item in mylist:\n",
        "                #print not isinstance(item, ne.NE_candidate)\n",
        "                if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                    self.custom_flatten(item, outlist)\n",
        "                else:\n",
        "                    item=item.strip(' \\t\\n\\r')\n",
        "                    outlist.append(item)\n",
        "        return outlist\n",
        "\n",
        "    def getWords(self, sentence):\n",
        "        tempList=[]\n",
        "        tempWordList=sentence.split()\n",
        "        p_dots= re.compile(r'[.]{2,}')\n",
        "        #print(tempWordList)\n",
        "        for word in tempWordList:\n",
        "            temp=[]\n",
        "\n",
        "            if \"(\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: '('+elem, temp))\n",
        "            elif \")\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: elem+')', temp))\n",
        "                # temp.append(temp1[-1])\n",
        "    #         elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
        "    #             temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
        "    #             if(temp1):\n",
        "    #                 temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
        "    #             temp.append(temp1[-1])\n",
        "            elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
        "                #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif (list(p_dots.finditer(word))):\n",
        "                matched_spans= list(p_dots.finditer(word)) \n",
        "                temp=[]\n",
        "                next_string_start=0\n",
        "                for matched_span in matched_spans:\n",
        "                    matched_start=matched_span.span()[0]\n",
        "                    this_excerpt=word[next_string_start:matched_start]\n",
        "                    if(this_excerpt):\n",
        "                        temp.append(this_excerpt)\n",
        "                    next_string_start=matched_span.span()[1]\n",
        "                if(next_string_start<len(word)):\n",
        "                    last_excerpt=word[next_string_start:]\n",
        "                    if(last_excerpt):\n",
        "                        temp.append(last_excerpt)\n",
        "            elif \"…\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
        "                if(temp):\n",
        "                    if(word.endswith(\"…\")):\n",
        "                        temp=list(map(lambda elem: elem+'…', temp))\n",
        "                    else:\n",
        "                        temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
        "            else:\n",
        "                #if word not in string.punctuation:\n",
        "                temp=[word]\n",
        "            if(temp):\n",
        "                tempList.append(temp)\n",
        "        tweetWordList=self.custom_flatten(tempList,[])\n",
        "        return tweetWordList\n",
        "\n",
        "    def rreplace(self,s, old, new, occurrence):\n",
        "        if s.endswith(old):\n",
        "            li = s.rsplit(old, occurrence)\n",
        "            return new.join(li)\n",
        "        else:\n",
        "            return s\n",
        "\n",
        "    def remAmpersand(self,candidateStr):\n",
        "        candidateStr=candidateStr.replace('&amp;','')\n",
        "        return candidateStr\n",
        "\n",
        "    def normalizeToken(self,token):\n",
        "        lowercased_token = token.lower()\n",
        "        if token.startswith(\"@\"):\n",
        "            return \"@USER\"\n",
        "        elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
        "            return \"HTTPURL\"\n",
        "        elif len(token) == 1:\n",
        "            return demojize(token)\n",
        "        else:\n",
        "            if token == \"’\":\n",
        "                return \"'\"\n",
        "            elif token == \"…\":\n",
        "                return \"...\"\n",
        "            else:\n",
        "                return token\n",
        "\n",
        "    def normalizeTweet(self, tweet):\n",
        "        tokens = self.tweetTokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
        "        normTweet = \" \".join([self.normalizeToken(token) for token in tokens])\n",
        "\n",
        "        normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n",
        "        normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n",
        "        normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n",
        "\n",
        "        normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n",
        "        normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n",
        "        normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n",
        "        \n",
        "        return normTweet\n",
        "\n",
        "    #removing commonly used expletives, enunciated chat words and other common words (like days of the week, common expressions)\n",
        "    def slang_remove(self,ne_phrase):\n",
        "        phrase=ne_phrase.strip().strip(string.punctuation).lower()\n",
        "        p1= re.compile(r'([A-Za-z]+)\\1\\1{1,}')\n",
        "        match_lst = p1.findall(phrase)\n",
        "        if phrase in article_list:\n",
        "            return True\n",
        "        elif phrase in day_list:\n",
        "            return True\n",
        "        elif phrase in month_list:\n",
        "            return True\n",
        "        elif match_lst:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def all_slang(self, ne_phrase):\n",
        "        ne_words=ne_phrase.split()\n",
        "        combined=[]+cachedStopWords+cachedTitles+chat_word_list+day_list\n",
        "        is_invalid=0\n",
        "\n",
        "        for word in ne_words:\n",
        "            if(word.strip().strip(string.punctuation).lower() in combined):\n",
        "                is_invalid+=1\n",
        "        if(is_invalid==len(ne_words)):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def start_end_token_check(self, ne_candidate):\n",
        "        positions = ne_candidate[2]\n",
        "        entity_type = ne_candidate[1]\n",
        "        ne_words=ne_candidate[0].split()\n",
        "        combined=[]+cachedStopWords+cachedTitles+chat_word_list+day_list+prep_list+article_list\n",
        "        # print(combined)\n",
        "        # print(ne_words)\n",
        "        start_word = ne_words[0].strip().strip(string.punctuation).lower()\n",
        "        end_word = ne_words[-1].strip().strip(string.punctuation).lower()\n",
        "        while((len(ne_words)>1)&((start_word in combined)|(end_word in combined))):\n",
        "            if(start_word in combined):\n",
        "                ne_words.pop(0)\n",
        "                positions.pop(0)\n",
        "            if(end_word in combined):\n",
        "                ne_words.pop()\n",
        "                positions.pop()\n",
        "            if(len(ne_words)>1):\n",
        "                start_word = ne_words[0].strip().strip(string.punctuation).lower()\n",
        "                end_word = ne_words[-1].strip().strip(string.punctuation).lower()\n",
        "        ne_phrase = (' '.join(ne_words)).strip()\n",
        "        # print(ne_phrase)\n",
        "        return (ne_phrase,entity_type,positions)\n",
        "\n",
        "    def set_stopword_exceptions(self,words):\n",
        "        combined=cachedStopWords+prep_list+article_list+day_list\n",
        "        for word in words:\n",
        "            if word in combined:\n",
        "                self.swSet.add(word)\n",
        "\n",
        "    def extract(self, batch, batch_number):\n",
        "        print(\"Running Local NER now\")\n",
        "        time_in=time.time()\n",
        "        self.batch=batch\n",
        "        self.df_out= pd.DataFrame(columns=('tweetID', 'sentID', 'TweetSentence','tweetwordList', 'phase1Candidates','start_time','entry_batch'))\n",
        "        df_holder=[]\n",
        "        self.swSet= set()\n",
        "\n",
        "        if(self.counter==0):\n",
        "            #self.df_out= pd.DataFrame(columns=('tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates','correct_candidates_tweet'))\n",
        "            #dict1 = {'tweetID':0, 'sentID':0, 'hashtags':'first', 'user':'user', 'TweetSentence':'sentence', 'phase1Candidates':'phase1Out','start_time':'now','entry_batch':'batch_number'}\n",
        "            self.CTrie=trie.Trie(\"ROOT\")\n",
        "            self.phase2stopWordList=[]\n",
        "            self.sentenceID = 0\n",
        "            self.f=0\n",
        "            self.phaseIpredictions=[]\n",
        "\n",
        "        for row in self.batch.itertuples():\n",
        "\n",
        "            now = datetime.datetime.now()\n",
        "            tweetID=str(row.Index)\n",
        "            text=str(row.TweetText)\n",
        "            row_sentences = self.normalize_to_sentences(text)\n",
        "\n",
        "            annnotated_mentions=[]\n",
        "\n",
        "            #Comment out when no annotations available\n",
        "            annnotated_mentions=[]\n",
        "            annotations = str(row.mentions_other).strip()\n",
        "            print(tweetID, annotations)\n",
        "            if(annotations):\n",
        "                for sentence_level in annotations.split(';'):\n",
        "                    sentence_level=sentence_level.strip()\n",
        "                    if(sentence_level):\n",
        "                        for elem in sentence_level.split(','):\n",
        "                            elem = elem.strip()\n",
        "                            if(elem):\n",
        "                                mention_record = elem.split('|')\n",
        "                                if(mention_record):\n",
        "                                    mention, entity_type =  mention_record[0], mention_record[1].lower()\n",
        "                                    if(mention):\n",
        "                                        if((mention !='')&(mention !='nan')):\n",
        "                                            mention = mention.lower().strip(string.punctuation).strip()\n",
        "                                            if(mention.startswith('the ')):\n",
        "                                                mention = mention[4:]\n",
        "                                                mention = mention.lower().strip(string.punctuation).strip()\n",
        "                                            annnotated_mentions.append((mention, entity_type))\n",
        "\n",
        "            self.tweet_to_sentences_w_annotation[tweetID]=((self.sentenceID,self.sentenceID+len(row_sentences)),annnotated_mentions)\n",
        "            self.sentenceID+=len(row_sentences)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for sen_index, sentence in enumerate(row_sentences):\n",
        "\n",
        "                    # print('tuple index:',tweetID,sen_index)\n",
        "                    phase1Out=\"\"\n",
        "                    sentence = self.normalizeTweet(sentence)\n",
        "                    # tweetWordList=self.getWords(sentence)\n",
        "                    tweetWordList = sentence.split()\n",
        "                    tweetWordList = list(filter(lambda element: not element.startswith('http://'), tweetWordList))\n",
        "                    sentence=(' '.join(tweetWordList)).strip()\n",
        "                    enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "\n",
        "                    entities_from_sentence=[] #with positions\n",
        "                    entity_predictions=[]\n",
        "                    entity_aware_embeddings=[]\n",
        "\n",
        "                    if(len(tweetWordList)>0):\n",
        "\n",
        "                        # print(test_record)\n",
        "                        # tokenized_input=tokenizer(test_record)\n",
        "                        # initial_input_ids = torch.tensor([tokenizer.encode(test_record)])\n",
        "                        # token_dict = {x : tokenizer.encode(x, add_special_tokens=False) for x in test_record.split()}\n",
        "                        # input_ids = initial_input_ids.to(device)\n",
        "                        # tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "                        tokenized_input= self.nerTokenizer(sentence)\n",
        "                        initial_input_ids = torch.tensor([self.nerTokenizer.encode(sentence)])\n",
        "                        # num_tokens = initial_input_ids.shape[1]\n",
        "\n",
        "                        initial_input_ids = initial_input_ids[:,:128]\n",
        "                        token_dict = {x : self.nerTokenizer.encode(x, add_special_tokens=False) for x in sentence.split()} #token, add_special_tokens=False, truncation=True\n",
        "                        input_ids = initial_input_ids.to(self.device)\n",
        "                        tokens = self.nerTokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "                        output = self.localNEREngine(input_ids)\n",
        "                        token_embeddings=output.hidden_states[-1].squeeze()[1:-1] # we dont need embeddings for CLS and EOS\n",
        "\n",
        "                        assert torch.isnan(token_embeddings).any() == False\n",
        "\n",
        "                        prediction = (torch.argmax(output.logits, axis=2))\n",
        "                        prediction = prediction.cpu().numpy().reshape(-1)\n",
        "\n",
        "                        # prediction_labels=[self.label_list[l].split('-')[0] for l in prediction]\n",
        "                        prediction_labels=[self.label_list[l] for l in prediction]\n",
        "\n",
        "                        prediction_labels, entity_aware_embeddings=self.collate_token_label_embedding(tweetWordList, token_dict, prediction_labels[1:-1],token_embeddings)\n",
        "\n",
        "                        assert len(enumerated_tweetWordList)==len(entity_aware_embeddings)\n",
        "                        assert len(prediction_labels)==len(enumerated_tweetWordList)\n",
        "\n",
        "                        word_tag_tuples=list(zip(tweetWordList,prediction_labels))\n",
        "                        print(list(word_tag_tuples))\n",
        "                        entities_from_sentence=self.get_entities(word_tag_tuples)\n",
        "                        print('entities_from_sentence:',entities_from_sentence)\n",
        "\n",
        "                        if(self.f<5):\n",
        "                            print(len(tweetWordList),initial_input_ids.shape,len(prediction[1:-1]))\n",
        "                            print(tweetWordList)\n",
        "                            print(token_dict)\n",
        "                            print(initial_input_ids)\n",
        "                            print(input_ids)\n",
        "                            print(prediction_labels)\n",
        "                            print('entities_from_sentence:',entities_from_sentence)\n",
        "                            print('======')\n",
        "                            self.f+=1\n",
        "\n",
        "                    just_candidates=[]\n",
        "\n",
        "                    # place some necessary filters\n",
        "                    entities_from_sentence= list(filter(lambda element: not self.slang_remove(element[0]), entities_from_sentence))\n",
        "                    entities_from_sentence= list(map(lambda element: self.start_end_token_check(element), entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: not self.all_slang(element[0]), entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: len(element[0])>1, entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: element[0]!='', entities_from_sentence))\n",
        "\n",
        "                    \n",
        "\n",
        "                    for candidateTuple in entities_from_sentence:\n",
        "                        #self.insert_dict (candidate,self.NE_container,candidateBase,index,candidate.sen_index,batch_number)\n",
        "                        candidateText, entity_type, positions = candidateTuple\n",
        "\n",
        "                        entity_predictions.append((candidateText, entity_type))\n",
        "\n",
        "                        candidateText=(((candidateText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip(' \\t\\n\\r')).lower()\n",
        "                        candidateText=(self.remAmpersand(candidateText).lstrip('“‘’”')).rstrip('“‘’”')\n",
        "                        candidateText= self.rreplace(self.rreplace(self.rreplace(candidateText,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "                        candidateText= candidateText.strip()\n",
        "                        self.set_stopword_exceptions(candidateText.split())\n",
        "                        just_candidates.append(candidateText)\n",
        "                        # if(index==9423):\n",
        "                        #     print(candidateText)\n",
        "                        position = '*'+'*'.join(str(v) for v in positions)\n",
        "                        position=position+'*'\n",
        "\n",
        "                        phase1Out+=(((candidateText).lstrip(string.punctuation)).strip())+'::'+str(position)+\"::\"+entity_type+\"||\"\n",
        "\n",
        "                        combined=[]+cachedStopWords+cachedTitles+prep_list+chat_word_list+article_list+day_list\n",
        "                        if not ((candidateText in combined)|(len(candidateText)<=1)|(candidateText.isdigit())|(self.is_float(candidateText))):\n",
        "                            if(self.quickRegex.match(candidateText)):\n",
        "                                self.CTrie.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
        "                    \n",
        "                    print('candidate_strings_from_sentence:',just_candidates)\n",
        "                    self.phaseIpredictions.append(entity_predictions)\n",
        "                    #storing the outputs and token embeddings in dataframe tweetID,sen_index\n",
        "                    dict1 = {'tweetID':str(tweetID), 'sentID':str(sen_index), 'TweetSentence':sentence, 'tweetwordList': enumerated_tweetWordList,'phase1Candidates': just_candidates ,'phase1CandidatesWPositions':phase1Out,\n",
        "                             'contextual_embeddings':entity_aware_embeddings,\n",
        "                             'start_time':now,'entry_batch':batch_number}\n",
        "                    df_holder.append(dict1)\n",
        "                    # self.contextual_embeddings[(tweetID,sen_index)] = entity_aware_embeddings\n",
        "\n",
        "        time_out=time.time()\n",
        "        self.append_rows(df_holder)\n",
        "        self.phase2stopWordList=list(set(self.phase2stopWordList)|self.swSet)\n",
        "        self.counter=self.counter+1\n",
        "        \n",
        "        #return (copy.deepcopy(self.df_out),copy.deepcopy(freqs),time_in,time_out)\n",
        "        return (self.df_out,self.contextual_embeddings,self.CTrie,time_in,time_out,self.phase2stopWordList,self.tweet_to_sentences_w_annotation)\n",
        "\n",
        "    def append_rows(self,df_holder):\n",
        "    \n",
        "        df = pd.DataFrame(df_holder)\n",
        "        self.df_out=self.df_out.append(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_4GphEbGUen"
      },
      "source": [
        "## **Phase II: Global NER with Phrase Embedder to collect the Entity Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBKfLqJPHLcw"
      },
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "from nltk.corpus import stopwords\n",
        "import pandas  as pd\n",
        "# import NE_candidate_module as ne\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import string\n",
        "import copy\n",
        "import numpy\n",
        "import math\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from collections import Iterable, OrderedDict\n",
        "from scipy import stats\n",
        "import emoji\n",
        "import statistics\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "import trie as trie\n",
        "import re\n",
        "import ast\n",
        "import pickle\n",
        "import itertools\n",
        "from scipy import spatial\n",
        "\n",
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "# from sklearn import linear_model\n",
        "# from sklearn.cluster import KMeans, MeanShift\n",
        "# from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "# Clustering imports\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import scipy.cluster.hierarchy as shc\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "import torch\n",
        "\n",
        "#CLASSIFIER CLASS\n",
        "# import SVM as svm\n",
        "# import entityClassifier as entityClassifier\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tempList=[\"i\",\"and\",\"or\",\"since\",\"hence\",\"onto\",\"other\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"probably\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
        "for item in tempList:\n",
        "    if item not in cachedStopWords:\n",
        "        cachedStopWords.append(item)\n",
        "cachedStopWords.remove(\"don\")\n",
        "# cachedStopWords.remove(\"your\")\n",
        "# cachedStopWords.remove(\"us\")\n",
        "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
        "prep_list=[\"of\",\"&;\",\"v.\"] #includes common conjunction as well\n",
        "# prep_list=[]\n",
        "# article_list=[]\n",
        "article_list=[\"a\",\"an\",\"the\"]\n",
        "conjoiner=[\"de\"]\n",
        "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
        "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "chat_word_list=[\"nope\",\"httpurl\",\"n't\",\"&amp;\",\"gee\",\"€\",\"hmm\",\"bye\",\"pls\",\"please\",\"yrs\",\"4get\",\"cbb\",\"tmi\",\"ooh\",\"ouch\",\"am\",\"tv\",\"ima\",\"tmw\",\"og\",\"psst\",\"b.s\",\"thanku\",\"em\",\"qft\",\"ip\",\"icymi\",\"bdsm\",\"ah\",\"http\",\"https\",\"pm\",\"omw\",\"pts\",\"pt\",\"ive\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
        "string.punctuation=string.punctuation+'…‘’'\n",
        "punct = string.punctuation\n",
        "punct=punct.replace(\".\",\"\",1)\n",
        "# print(punct)\n",
        "\n",
        "\n",
        "\n",
        "class GlobalNERModule():\n",
        "\n",
        "\n",
        "    def executor(self,max_batch_value,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,raw_tweets_for_others):\n",
        "\n",
        "\n",
        "        # SET CB\n",
        "        # print(phase2stopwordList)\n",
        "        candidate_featureBase_DF,data_frame_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder = self.set_cb(TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold)\n",
        "        candidate_df = candidate_featureBase_DF[['candidate','class']]\n",
        "        for index, row in candidate_df.iterrows():\n",
        "            print(row['candidate']+' : '+str(row['class']))\n",
        "        candidate_df.to_csv(\"candidate_records.csv\", sep=',', encoding='utf-8',index=False)\n",
        "\n",
        "\n",
        "        # # SET TF \n",
        "        ner_arrays = self.set_tf(data_frame_holder, candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder)\n",
        "\n",
        "        # self.calculate_tp_fp_f1(z_score_threshold,untrashed_tweets,raw_tweets_for_others)\n",
        "\n",
        "        # # return candidate_featureBase_DF, self.complete_tweet_dataframe_grouped_df_sorted,time_out \n",
        "        return ner_arrays\n",
        "\n",
        "\n",
        "    def __init__(self,entity_phrase_embedder,device,filename):\n",
        "        self.counter=0\n",
        "        self.decay_factor=2**(-1/2)\n",
        "        self.decay_base_staggering=2\n",
        "        self.true_positive_count=0\n",
        "        self.false_positive_count=0\n",
        "        self.false_negative_count=0\n",
        "        self.device=device\n",
        "        self.save_file= filename\n",
        "\n",
        "        self.entity_phrase_embedder = entity_phrase_embedder\n",
        "\n",
        "        # context_feature_list=['cf_'+str(i) for i in range(768)]\n",
        "        context_feature_list=['cf_'+str(i) for i in range(300)]\n",
        "        self.candidateBaseHeaders=['candidate', 'batch', 'length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+context_feature_list+['cumulative']\n",
        "        self.candidateBaseHeaders_alt=['candidate', 'batch', 'length','cumulative','class']\n",
        "        self.complete_tweet_dataframe_grouped_df_sorted=pd.DataFrame([], columns=['tweetID', 'TweetSentence', 'ambiguous_candidates', 'annotation', 'candidates_with_label', 'completeness', 'current_minus_entry', 'entry_batch', 'hashtags', 'index', 'only_good_candidates', 'output_mentions', 'phase1Candidates', 'sentID', 'stanford_candidates', 'user'])\n",
        "\n",
        "\n",
        "    def classify_candidate_base(self,z_score_threshold,candidate_featureBase_DF):\n",
        "\n",
        "        # zscore_array1=stats.zscore(candidate_featureBase_DF['cumulative'])\n",
        "\n",
        "        # candidate_featureBase_DF['Z_ScoreUnweighted']=zscore_array1\n",
        "        # cumulative_threshold=1.0 #set threshold here\n",
        "        # z_score_threshold=candidate_featureBase_DF[candidate_featureBase_DF['cumulative']==cumulative_threshold].Z_ScoreUnweighted.tolist()[0]\n",
        "        # print(cumulative_threshold,z_score_threshold)\n",
        "        # #candidate_featureBase_DF.to_csv(\"cf_new_with_z_score.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        # #multi-word infrequent candidates ---> to be used for recall correction\n",
        "        # infrequent_candidates=candidate_featureBase_DF[(candidate_featureBase_DF['Z_ScoreUnweighted'] < z_score_threshold) & (candidate_featureBase_DF.length>1)].candidate.tolist()\n",
        "        # candidate_featureBase_DF = candidate_featureBase_DF[candidate_featureBase_DF['Z_ScoreUnweighted'] >= z_score_threshold]\n",
        "        return self.entity_classifier_alt.run(candidate_featureBase_DF,self.candidateEmbeddingDict)\n",
        "\n",
        "    # recall_correction\n",
        "    def set_partition_dict(self,candidate_featureBase_DF,infrequent_candidates):\n",
        "\n",
        "        #print(list(self.partition_dict.keys()))\n",
        "        ambiguous_bad_candidates=candidate_featureBase_DF[(((candidate_featureBase_DF.status==\"a\")|(candidate_featureBase_DF.status==\"b\"))&(candidate_featureBase_DF.length.astype(int)>1))]\n",
        "        good_candidates=candidate_featureBase_DF[(candidate_featureBase_DF.status==\"g\")].candidate.tolist()\n",
        "        flag1=False\n",
        "        flag2=False\n",
        "        if(len(ambiguous_bad_candidates)>0):\n",
        "            # ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1) \n",
        "            # ambiguous_bad_candidates_wFilter= ambiguous_bad_candidates[ambiguous_bad_candidates.max_column=='substring-cap']\n",
        "\n",
        "            #good_candidates=candidate_featureBase_DF[(candidate_featureBase_DF.status==\"g\")].candidate.tolist()\n",
        "            #print(ambiguous_bad_candidates_wFilter.candidate.tolist())\n",
        "\n",
        "            # for candidate in ambiguous_bad_candidates_wFilter.candidate.tolist():\n",
        "            for candidate in ambiguous_bad_candidates.candidate.tolist():\n",
        "                \n",
        "                #print(candidate)\n",
        "                if candidate not in self.partition_dict.keys():\n",
        "\n",
        "                    substring_candidates=self.get_substring_candidates(candidate.split(),good_candidates)\n",
        "                    if(len(substring_candidates)>0):\n",
        "                        self.partition_dict[candidate]=substring_candidates\n",
        "\n",
        "            flag1= True\n",
        "        if(len(infrequent_candidates)>0):\n",
        "            #print(len(ambiguous_bad_candidates_wFilter.candidate.tolist()))\n",
        "\n",
        "            for candidate in infrequent_candidates:\n",
        "                #print(candidate)\n",
        "                if candidate not in self.partition_dict.keys():\n",
        "                    substring_candidates=self.get_substring_candidates(candidate.split(),good_candidates)\n",
        "                    if(len(substring_candidates)>0):\n",
        "                        self.partition_dict[candidate]=substring_candidates\n",
        "            flag2= True\n",
        "        print(list(self.partition_dict.keys()))\n",
        "        return (flag1|flag2)\n",
        "\n",
        "    def update_Candidatedict(self,candidate,cluster_id,contextual_embedding_vector):\n",
        "        \n",
        "        alt_feature_list=[] # this is for the reduced CandidateBase for the Alt_Classifier\n",
        "        candidate_name = candidate+'||'+cluster_id\n",
        "\n",
        "        if(candidate_name in self.CandidateBase_dict_alt.keys()):\n",
        "            # feature_list=self.CandidateBase_dict[normalized_candidate]\n",
        "            alt_feature_list=self.CandidateBase_dict_alt[candidate_name]\n",
        "        else:\n",
        "            alt_feature_list=[0]*4\n",
        "            alt_feature_list[0]=self.counter\n",
        "            alt_feature_list[1]=len(candidate.split())\n",
        "            alt_feature_list[2]=0\n",
        "            alt_feature_list[3]=-1\n",
        "\n",
        "        alt_feature_list[2]+=1 #increase cumulative frequency\n",
        "        self.CandidateBase_dict_alt[candidate_name]=alt_feature_list\n",
        "\n",
        "        if((candidate,cluster_id) in self.candidateEmbeddingDict.keys()):\n",
        "            self.candidateEmbeddingDict[(candidate,cluster_id)].append(contextual_embedding_vector)\n",
        "        else:\n",
        "            self.candidateEmbeddingDict[(candidate,cluster_id)]=[contextual_embedding_vector]\n",
        "        return\n",
        "\n",
        "    def get_candidate_clusters(self, candidate, input_arr):\n",
        "        if(len(input_arr)>1):\n",
        "            print(candidate)\n",
        "            # X = np.nan_to_num(np.array(input_arr))\n",
        "            X = np.array(input_arr)\n",
        "            # print(X)\n",
        "            # clusterer = AgglomerativeClustering(n_clusters=5, affinity=\"cosine\", linkage=\"single\") #total 5 entity type clusters can be formed\n",
        "            # cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "            # clusterer = AgglomerativeClustering(n_clusters=None, affinity='cosine', linkage='complete', compute_full_tree=True, distance_threshold=1.0)\n",
        "            clusterer = AgglomerativeClustering(n_clusters=None, affinity='cosine', linkage='average', compute_full_tree=True, distance_threshold=0.999)\n",
        "            # clusterer = AgglomerativeClustering(n_clusters=None, affinity='cosine', linkage='single', compute_full_tree=True, distance_threshold=1.0)\n",
        "\n",
        "            # clusterer = AgglomerativeClustering(n_clusters=None, affinity='euclidean', linkage='ward', compute_full_tree=True, distance_threshold=1.0)\n",
        "\n",
        "            cluster_labels = clusterer.fit_predict(X)\n",
        "            # print(\"Number of clusters for \"+candidate+\" : \"+str(1+np.amax(cluster_labels)))\n",
        "\n",
        "            # range_n_clusters = [1, 2, 3, 4, 5]\n",
        "            # max_silhouette = float('-inf')\n",
        "            # for n_clusters in range_n_clusters:\n",
        "            #     if(n_clusters <= len(X)):\n",
        "            #         clusterer = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "            #         labels = clusterer.fit_predict(X)\n",
        "            #         silhouette = silhouette_score(X, cluster_labels)\n",
        "            #         if(silhouette>max_silhouette):\n",
        "            #             max_silhouette = silhouette\n",
        "            #             cluster_labels = labels\n",
        "        else:\n",
        "            cluster_labels = np.array([0])\n",
        "\n",
        "        # print(cluster_labels)\n",
        "        return cluster_labels\n",
        "    \n",
        "    def set_cb(self,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold):\n",
        "\n",
        "        #input new_tweets, z_score, Updated candidatebase of phase1\n",
        "        #output candidate_featureBase_DF, Incomplete_tweets\n",
        "        data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "        phase2_candidates_holder=[]\n",
        "        phase2_unnormalized_candidates_holder=[]\n",
        "        df_holder=[]\n",
        "\n",
        "        df_holder_extracted,phase2_candidates_holder_extracted,phase2_unnormalized_candidates_holder_extracted= self.extract(TweetBase,CTrie,phase2stopwordList,0)\n",
        "        phase2_candidates_holder.extend(phase2_candidates_holder_extracted)\n",
        "        phase2_unnormalized_candidates_holder.extend(phase2_unnormalized_candidates_holder_extracted)\n",
        "        df_holder.extend(df_holder_extracted)\n",
        "\n",
        "        data_frame_holder = pd.DataFrame(df_holder)\n",
        "\n",
        "        #perform clustering over mentions of each candidate in self.candidateEmbeddingPool\n",
        "        for candidate in self.candidateEmbeddingPool.keys():\n",
        "            labels = self.get_candidate_clusters(candidate, self.candidateEmbeddingPool[candidate])\n",
        "            for ind,int_label in enumerate(labels):\n",
        "                # print(self.candidateEmbedding_records[candidate][ind])\n",
        "                label = str(int_label)\n",
        "                mention_embedding_vector = self.candidateEmbedding_records[candidate][ind][0]\n",
        "                assert self.candidateEmbeddingPool[candidate][ind] == mention_embedding_vector.tolist()\n",
        "                self.candidateEmbedding_records[candidate][ind] += (label,)\n",
        "                # print(self.candidateEmbedding_records[candidate][ind])\n",
        "                self.update_Candidatedict(candidate,label,mention_embedding_vector)\n",
        "\n",
        "        # #self.CandidateBase_dict_alt and self.candidateEmbeddingDict are formed after candidate clusters have been separated\n",
        "        candidate_featureBase_DF_alt = pd.DataFrame.from_dict(self.CandidateBase_dict_alt, orient='index')\n",
        "\n",
        "        candidate_featureBase_DF_alt.columns = self.candidateBaseHeaders_alt[1:]\n",
        "        candidate_featureBase_DF_alt.index.name = self.candidateBaseHeaders_alt[0]\n",
        "        candidate_featureBase_DF_alt  = candidate_featureBase_DF_alt.reset_index(drop=False)\n",
        "\n",
        "        # print(candidate_featureBase_DF_alt.head(5))\n",
        "        # for elem in list(self.candidateEmbeddingDict.keys())[:5]:\n",
        "        #     print(elem, len(self.candidateEmbeddingDict[elem]))\n",
        "\n",
        "        #initialize and classify\n",
        "        self.entity_classifier_alt = EntityClassifierAlt(False,self.device,None,None)\n",
        "        candidate_featureBase_DF= self.classify_candidate_base(z_score_threshold,candidate_featureBase_DF_alt)\n",
        "\n",
        "        return candidate_featureBase_DF,data_frame_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder\n",
        "\n",
        "    def set_tf(self,data_frame_holder, candidate_featureBase_DF, phase2_candidates_holder,phase2_unnormalized_candidates_holder):\n",
        "        entity_mentions_holder = []\n",
        "        for phase2_mentions in phase2_candidates_holder:\n",
        "            row_entity_mentions=[]\n",
        "            for mention in phase2_mentions:\n",
        "                # print(mention)\n",
        "                mention_record = mention.split('||')\n",
        "                candidate, mention_id = mention_record[0], int(mention_record[1])\n",
        "                # print(self.candidateEmbedding_records[candidate][mention_id])\n",
        "                cluster_id = self.candidateEmbedding_records[candidate][mention_id][1]\n",
        "                class_value = candidate_featureBase_DF[candidate_featureBase_DF['candidate'] == candidate+'||'+cluster_id]['class'].tolist()[0]\n",
        "                class_label = self.entity_classifier_alt.entity_types[class_value]\n",
        "                if(class_label!='ne'):\n",
        "                    row_entity_mentions.append((candidate,class_label))\n",
        "            entity_mentions_holder.append(row_entity_mentions)\n",
        "        data_frame_holder[\"only_good_candidates\"]=entity_mentions_holder\n",
        "        return entity_mentions_holder\n",
        "\n",
        "    def get_incomplete_tf(self,untrashed_tweets):\n",
        "        return untrashed_tweets[untrashed_tweets.completeness==False]\n",
        "\n",
        "    def get_complete_tf(self,untrashed_tweets):\n",
        "        return untrashed_tweets[untrashed_tweets.completeness==True]\n",
        "\n",
        "    def compute_seen_tweets_so_far(self,start_batch,end_batch):\n",
        "        if(start_batch==end_batch):\n",
        "            sliced_seen_tweets=self.number_of_seen_tweets_per_batch[start_batch]\n",
        "\n",
        "\n",
        "        sliced_seen_tweets=self.number_of_seen_tweets_per_batch[start_batch:]\n",
        "\n",
        "\n",
        "        counter=0\n",
        "        for elem in sliced_seen_tweets:\n",
        "            counter=counter+elem\n",
        "\n",
        "        return counter\n",
        "\n",
        "\n",
        "    def rreplace(self,s, old, new, occurrence):\n",
        "        if s.endswith(old):\n",
        "            li = s.rsplit(old, occurrence)\n",
        "            return new.join(li)\n",
        "        else:\n",
        "            return s\n",
        "    #ME_EXTR=Mention.Mention_Extraction()\n",
        "\n",
        "\n",
        "\n",
        "    def set_column_for_candidates_in_incomplete_tweets(self,candidate_featureBase_DF,input_to_eval):\n",
        "\n",
        "        incomplete_candidates= input_to_eval['2nd Iteration Candidates'].tolist()\n",
        "\n",
        "        candidate_featureBase_DF= candidate_featureBase_DF.set_index('candidate')\n",
        "\n",
        "        candidate_with_label_holder=[]\n",
        "        one_level=[]\n",
        "        \n",
        "\n",
        "        for sentence_level_candidates in incomplete_candidates:\n",
        "\n",
        "            one_level.clear()\n",
        "\n",
        "            for candidate in sentence_level_candidates:\n",
        "                if candidate.lower() in candidate_featureBase_DF.index:\n",
        "                    # label=candidate_featureBase_DF.get_value(candidate.lower(),'status')\n",
        "                    label=candidate_featureBase_DF.at[candidate.lower(),'status']\n",
        "                    one_level.append((candidate,label))\n",
        "                else:\n",
        "                    one_level.append((candidate,\"na\"))\n",
        "\n",
        "            candidate_with_label_holder.append(copy.deepcopy(one_level))\n",
        "\n",
        "\n",
        "        input_to_eval[\"candidates_with_label\"]=candidate_with_label_holder\n",
        "        debug_candidates_label_list= input_to_eval['candidates_with_label'].tolist()\n",
        "        candidates_filtered_g_labeled=[]\n",
        "        row_level_candidates=[]\n",
        "        index_outer=0\n",
        "\n",
        "        candidates_filtered_a_labeled=[]\n",
        "        row_level_a_candidates=[]\n",
        "\n",
        "        for sentence_level in debug_candidates_label_list:\n",
        "\n",
        "            # sentence_level_candidates_unnormalized= incomplete_candidates_unnormalized[index_outer]\n",
        "            row_level_candidates.clear()\n",
        "            row_level_a_candidates.clear()\n",
        "            for candidate in sentence_level:\n",
        "                if(candidate[1]==\"g\"):\n",
        "                    candidate_str = self.erode_article(candidate[0])\n",
        "                    row_level_candidates.append(candidate_str)\n",
        "                if(((candidate[1]==\"b\")|(candidate[1]==\"a\"))&(candidate[0]==\"US\")):\n",
        "                    # print('here')\n",
        "                    row_level_candidates.append(candidate[0])\n",
        "                if(candidate[1]==\"a\"):\n",
        "                    row_level_a_candidates.append(candidate[0])\n",
        "\n",
        "            candidates_filtered_g_labeled.append(copy.deepcopy(row_level_candidates))\n",
        "            candidates_filtered_a_labeled.append(copy.deepcopy(row_level_a_candidates))\n",
        "            index_outer+=1\n",
        "\n",
        "\n",
        "        input_to_eval[\"only_good_candidates\"]=candidates_filtered_g_labeled\n",
        "        input_to_eval[\"ambiguous_candidates\"]=candidates_filtered_a_labeled\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_tp_fp_f1(self,z_score_threshold,input_to_eval,raw_tweets_for_others):\n",
        "\n",
        "        input_to_eval_grouped_df= (input_to_eval.groupby('tweetID', as_index=False).aggregate(lambda x: x.tolist()))\n",
        "        input_to_eval_grouped_df['tweetID']=input_to_eval_grouped_df['tweetID'].astype(int)\n",
        "        input_to_eval_df_sorted=(input_to_eval_grouped_df.sort_values(by='tweetID', ascending=True)).reset_index(drop=True)\n",
        "\n",
        "        print(len(input_to_eval_df_sorted),len(raw_tweets_for_others))\n",
        "        \n",
        "        print(set(input_to_eval_df_sorted['tweetID'].values.tolist())-set(raw_tweets_for_others['ID'].values.tolist()))\n",
        "\n",
        "        input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['mentions_other'].iloc[0])\n",
        "        # input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['mentions_limited_types'].iloc[0])\n",
        "        # input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['annotation_limited types'].iloc[0])\n",
        "\n",
        "        column_candidates_holder = input_to_eval_df_sorted['only_good_candidates'].tolist()\n",
        "        # column_candidates_holder = input_to_eval_df_sorted['phase1Candidates'].tolist()\n",
        "\n",
        "        file1 = open(self.save_file+\".txt\", \"w\")\n",
        "        \n",
        "\n",
        "        column_annot_holder= input_to_eval_df_sorted['annotation'].tolist()\n",
        "        \n",
        "        output_str=''\n",
        "        # print(column_candidates_holder)\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "        total_annotation=0\n",
        "\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        quickRegex=re.compile(\"[a-z]+\")\n",
        "\n",
        "        print('=========================BERTNER_candidates')\n",
        "\n",
        "        for idx in range(len(column_annot_holder)):\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            output_mentions_list=[]\n",
        "            tweet_level_candidate_list=str(column_annot_holder[idx]).split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: quickRegex.match(element), annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: ((element !='')&(element !='nan')), annotated_mention_list))\n",
        "\n",
        "            for lst in column_candidates_holder[idx]:\n",
        "                output_mentions_list.extend(lst)\n",
        "\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "            output_mentions_list=list(map(lambda element: element.lower(), output_mentions_list))\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            print(idx, annotated_mention_list,output_mentions_list)\n",
        "            output_str+=','.join(output_mentions_list)+'\\n'\n",
        "\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= self.normalize(annotated_mention_list.pop())\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            # print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
        "\n",
        "            self.true_positive_count+=tp_counter_inner\n",
        "            self.false_positive_count+=fp_counter_inner\n",
        "            self.false_negative_count+=fn_counter_inner\n",
        "\n",
        "        print('TP||||FP||||FN')\n",
        "        print(self.true_positive_count,self.false_positive_count,self.false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        precision=(self.true_positive_count)/(self.true_positive_count+self.false_positive_count)\n",
        "        recall=(self.true_positive_count)/(self.true_positive_count+self.false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "        \n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        # recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        # f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        file1.write(output_str)\n",
        "        file1.close()\n",
        "\n",
        "\n",
        "\n",
        "        self.accuracy_vals=(z_score_threshold,f_measure,precision,recall)\n",
        "\n",
        "        print('Precision:',precision)\n",
        "        print('Recall:',recall)\n",
        "        print('F1:',f_measure)\n",
        "\n",
        "        # print('z_score:', z_score_threshold , 'precision: ',precision,'recall: ',recall,'f measure: ',f_measure)\n",
        "        # print('trupe positive: ',tp_count, 'false positive: ',fp_count,'false negative: ', fn_count,'total mentions: ', tm_count)\n",
        "\n",
        "        return input_to_eval\n",
        "\n",
        "\n",
        "    def recall_correction(self,candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,data_frame_holder):\n",
        "\n",
        "        corrected_phase2_candidates_holder=[]\n",
        "        index_outer=0\n",
        "        for candidates in phase2_candidates_holder:\n",
        "            unnormalized_candidates=phase2_unnormalized_candidates_holder[index_outer]\n",
        "            corrected_phase2_candidates=[]\n",
        "            for idx, candidate in enumerate(candidates):\n",
        "                unnormalized_candidate=unnormalized_candidates[idx]\n",
        "                # if((candidate in self.partition_dict.keys())&((candidate in self.infrequent_candidates)|(candidate in self.bad_candidates))):\n",
        "                if((candidate in self.partition_dict.keys())&((candidate in self.infrequent_candidates)|(candidate in self.bad_candidates)|(candidate in self.ambiguous_candidates))):   #do this only for 3K tweets\n",
        "                    #print(candidate, self.partition_dict[candidate])\n",
        "                    corrected_phase2_candidates.extend(self.partition_dict[candidate])\n",
        "                else:\n",
        "                    if(((candidate in self.bad_candidates)|(candidate in self.ambiguous_candidates))&(candidate=='us')&(unnormalized_candidate=='US')):\n",
        "                        # print(index_outer)\n",
        "                        candidate=unnormalized_candidate\n",
        "                    # if((len(candidate.strip().strip(string.punctuation).split())>1)&(candidate.strip().strip(string.punctuation).split()[0].lower() in ['a','an','the'])):\n",
        "                    #     candidate = (' '.join(candidate.strip().strip(string.punctuation).split()[1:])).strip()\n",
        "                    corrected_phase2_candidates.append(candidate)\n",
        "            corrected_phase2_candidates_holder.append(copy.deepcopy(corrected_phase2_candidates))\n",
        "            index_outer+=1\n",
        "\n",
        "        \n",
        "        #print(corrected_phase2_candidates_holder)\n",
        "        data_frame_holder['2nd Iteration Candidates']=corrected_phase2_candidates_holder\n",
        "\n",
        "        return corrected_phase2_candidates_holder,data_frame_holder                  \n",
        "\n",
        "\n",
        "    def erode_article(self, entity_string):\n",
        "        if((len(entity_string.strip().strip(string.punctuation).split())>1)&(entity_string.lower().strip().strip(string.punctuation).split()[0] in ['a','an','the'])):\n",
        "            # print(entity_string)\n",
        "            entity_string = ' '.join(entity_string.strip().strip(string.punctuation).split()[1:])\n",
        "            # print(entity_string)\n",
        "        return entity_string.strip()\n",
        "\n",
        "\n",
        "    def set_completeness_in_tweet_frame(self,data_frame_holder,candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag):\n",
        "        #print(candidate_featureBase_DF.head())\n",
        "        good_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"g\"].candidate.tolist()\n",
        "        bad_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"b\"].candidate.tolist()\n",
        "\n",
        "        merged_g_b= bad_candidates+good_candidates\n",
        "\n",
        "        #candidate_featureBase_DF.to_csv(\"cf_before_labeling_comp.csv\", sep=',', encoding='utf-8')\n",
        "        ambiguous_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"a\"].candidate.tolist()\n",
        "\n",
        "        if(correction_flag):\n",
        "            phase2_candidates_holder,data_frame_holder=self.recall_correction(candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,data_frame_holder)\n",
        "\n",
        "         \n",
        "\n",
        "        \n",
        "        truth_vals=[False if any(x not in merged_g_b for x in list1) else True for list1 in phase2_candidates_holder]\n",
        "\n",
        "        intermediate_output_mentions=[list(filter(lambda candidate: ((candidate in good_candidates))|(candidate=='US'), list1)) for list1 in phase2_candidates_holder]\n",
        "\n",
        "        output_mentions=[list(map(lambda candidate: self.erode_article(candidate), list1)) for list1 in intermediate_output_mentions]\n",
        "\n",
        "        # truth_vals=[False if any(x in ambiguous_candidates for x in list1) else True for list1 in phase2_candidates_holder]\n",
        "        completeness_series = pd.Series( (v for v in truth_vals) )\n",
        "        output_mentions_series = pd.Series( (v for v in output_mentions) )\n",
        "\n",
        "\n",
        "        data_frame_holder['output_mentions']=output_mentions_series\n",
        "        data_frame_holder['completeness']=completeness_series\n",
        "        data_frame_holder[\"current_minus_entry\"]=self.counter-data_frame_holder['entry_batch']\n",
        "\n",
        "        return data_frame_holder\n",
        "\n",
        "\n",
        "\n",
        "    def normalize(self,word):\n",
        "        strip_op=word\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
        "        strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
        "        #strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "        if strip_op.endswith(\"'s\"):\n",
        "            li = strip_op.rsplit(\"'s\", 1)\n",
        "            return ''.join(li)\n",
        "        elif strip_op.endswith(\"’s\"):\n",
        "            li = strip_op.rsplit(\"’s\", 1)\n",
        "            return ''.join(li)\n",
        "        else:\n",
        "            return strip_op\n",
        "        #return strip_op\n",
        "\n",
        "    \n",
        "    def isSubstring(self,to_increase_element,id_to_incr,comparison_holder,phase1_holder_holder_copy):\n",
        "        combined_list=comparison_holder[id_to_incr]+phase1_holder_holder_copy[id_to_incr]\n",
        "\n",
        "        for idx,val in enumerate(comparison_holder[id_to_incr]):\n",
        "            if((to_increase_element[0] in val[0]) and to_increase_element[0] != val[0]):\n",
        "                if((to_increase_element[5] in val[5]) and to_increase_element[5] != val[5]):\n",
        "                    return True\n",
        "        for idx,val in enumerate(phase1_holder_holder_copy[id_to_incr]):\n",
        "            if((to_increase_element[0] in val[0]) and to_increase_element[0] != val[0]):\n",
        "                if((to_increase_element[5] in val[2]) and to_increase_element[5] != val[2]):\n",
        "                    return True   \n",
        "                \n",
        "        return False\n",
        "\n",
        "\n",
        "    def calculate_pmi(self,big,x1,x2,total):\n",
        "        big__= float(big/total)\n",
        "        x1__=float(x1/total)\n",
        "        x2__=float(x2/total)\n",
        "        pmi= math.log(big__/(x1__*x2__),2.71828182845)\n",
        "        pklv=big__*pmi\n",
        "        #return (1/(1+math.exp(-1*pmi)))\n",
        "        npmi= pmi/(-1.0*(math.log(big__,2.71828182845)))\n",
        "        return npmi,pklv\n",
        "        #return pklv\n",
        "\n",
        "    def multiSlice(self,s,cutpoints,good_candidates):\n",
        "        k = len(cutpoints)\n",
        "        multislices=[]\n",
        "        if k == 0:\n",
        "            curr_candidate=self.normalize(' '.join(s))\n",
        "\n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices = [curr_candidate]        \n",
        "        else:\n",
        "            \n",
        "            curr_candidate=self.normalize(' '.join(s[:cutpoints[0]]))\n",
        "            alt_list=[curr_candidate]\n",
        "            \n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices = [curr_candidate]\n",
        "\n",
        "            alt_list.extend(self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) for i in range(k-1))\n",
        "            multislices.extend(self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) for i in range(k-1) if self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) in good_candidates)\n",
        "\n",
        "            curr_candidate=self.normalize(' '.join(s[cutpoints[k-1]:]))\n",
        "            alt_list.append(curr_candidate)\n",
        "            \n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices.append(curr_candidate)\n",
        "            # print('::',alt_list)\n",
        "        return multislices\n",
        "\n",
        "\n",
        "\n",
        "    def get_substring_candidates(self,candidate_words,good_candidates):\n",
        "        n = len(candidate_words)\n",
        "        all_partitions=[]\n",
        "        all_partitions_length=[]\n",
        "        cuts = list(range(1,n))\n",
        "        for k in range(n):\n",
        "            # all_partitions_inner=[]\n",
        "            partition_list=[]\n",
        "            partition_length_list=[]\n",
        "            for cutpoints in itertools.combinations(cuts,k):\n",
        "                ret_list=self.multiSlice(candidate_words,cutpoints,good_candidates)\n",
        "                if(ret_list):\n",
        "                    partition_length=sum([len(elem.split()) for elem in ret_list])\n",
        "                    # print('==',ret_list,partition_length)\n",
        "                    if(partition_length==len(candidate_words)):\n",
        "                        return ret_list\n",
        "                    partition_list.append(ret_list)\n",
        "                    partition_length_list.append(partition_length)\n",
        "                    # yield ret_list\n",
        "            # print('------')\n",
        "            if(partition_length_list):\n",
        "                max_index=partition_length_list.index(max(partition_length_list))\n",
        "                all_partitions.append(partition_list[max_index])\n",
        "                all_partitions_length.append(partition_length_list[max_index])\n",
        "        # print(all_partitions)\n",
        "        if(all_partitions_length):\n",
        "            max_index=all_partitions_length.index(max(all_partitions_length))\n",
        "            # print(all_partitions[max_index])\n",
        "            return all_partitions[max_index]\n",
        "        else:\n",
        "            return []\n",
        "    \n",
        "\n",
        "    def verify(self, subsequence, CTrie):\n",
        "        return CTrie.__contains__(subsequence)\n",
        "\n",
        "\n",
        "\n",
        "    def check_sequence(self, sequence, l, CTrie):\n",
        "        result=[]\n",
        "        subsequence_length=l\n",
        "        while(subsequence_length>0):\n",
        "            shift=len(sequence)-subsequence_length\n",
        "            verified_subsequence=[]\n",
        "            verified=False\n",
        "            for i in range(0,shift+1):\n",
        "                list1=sequence[i:(i+subsequence_length)]\n",
        "                text=' '.join(str(e[0]) for e in list1)\n",
        "                subsequence=(self.normalize(text)).split()\n",
        "                #print(\"search for\", subsequence)\n",
        "                if self.verify(subsequence, CTrie):\n",
        "                    verified_subsequence.append(i)\n",
        "                    verified_subsequence.append(i+subsequence_length)\n",
        "                    #print(subsequence)\n",
        "                    #print(subsequence,[(verified_subsequence[0]-0),(int(sequence[-1][1])-verified_subsequence[1])])\n",
        "                    verified=True\n",
        "                    break\n",
        "            if(verified):\n",
        "                result.append(sequence[verified_subsequence[0]:verified_subsequence[1]])\n",
        "                if(verified_subsequence[0]-0)>0:\n",
        "                    subequence_to_check=sequence[0:verified_subsequence[0]]\n",
        "                    #since tokens before the starting position of the verified subsequence have already been checked for subsequences of this length\n",
        "                    partition_length=min(len(subequence_to_check),(subsequence_length-1))\n",
        "                    #print(subequence_to_check)\n",
        "                    lst=self.check_sequence(subequence_to_check,partition_length, CTrie)\n",
        "                    if(lst):\n",
        "                        result.extend(lst)\n",
        "                if(int(sequence[-1][1])-verified_subsequence[1])>0:\n",
        "                    subequence_to_check=sequence[(verified_subsequence[1]):]\n",
        "                    #since tokens following the end position of the verified subsequence have not been checked for subsequences of this length\n",
        "                    partition_length=min(len(subequence_to_check),(subsequence_length))\n",
        "                    #print(subequence_to_check)\n",
        "                    lst=self.check_sequence(subequence_to_check,partition_length, CTrie)\n",
        "                    if(lst):\n",
        "                        result.extend(lst)\n",
        "                return result\n",
        "            else:\n",
        "                subsequence_length-=1\n",
        "        return result\n",
        "\n",
        "    # def flatten(self,mylist, outlist,ignore_types=(str, bytes, int, ne.NE_candidate)):\n",
        "    def flatten(self,mylist, outlist,ignore_types=(str, bytes, int)):\n",
        "    \n",
        "        if mylist !=[]:\n",
        "            for item in mylist:\n",
        "                #print not isinstance(item, ne.NE_candidate)\n",
        "                if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                    self.flatten(item, outlist)\n",
        "                else:\n",
        "                    # if isinstance(item,ne.NE_candidate):\n",
        "                    #     item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
        "                    #     item.reset_length()\n",
        "                    # else:\n",
        "                    if type(item)!= int:\n",
        "                        item=item.strip(' \\t\\n\\r')\n",
        "                    outlist.append(item)\n",
        "        return outlist\n",
        "\n",
        "\n",
        "    def getWords(self, sentence):\n",
        "        tempList=[]\n",
        "        tempWordList=sentence.split()\n",
        "        p_dots= re.compile(r'[.]{2,}')\n",
        "        #print(tempWordList)\n",
        "        for word in tempWordList:\n",
        "            temp=[]\n",
        "            \n",
        "            if \"(\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: '('+elem, temp))\n",
        "            elif \")\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: elem+')', temp))\n",
        "                # temp.append(temp1[-1])\n",
        "            # elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
        "            #     temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
        "            #     if(temp1):\n",
        "            #         temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
        "            #     temp.append(temp1[-1])\n",
        "            elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
        "                #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "                #print(index, temp)\n",
        "            # elif \"...\" in word:\n",
        "            #     #print(\"here\")\n",
        "            #     temp=list(filter(lambda elem: elem!='',word.split(\"...\")))\n",
        "            #     if(temp):\n",
        "            #         if(word.endswith(\"...\")):\n",
        "            #             temp=list(map(lambda elem: elem+'...', temp))\n",
        "            #         else:\n",
        "            #            temp=list(map(lambda elem: elem+'...', temp[:-1]))+[temp[-1]]\n",
        "            #     # temp.append(temp1[-1])\n",
        "            # elif \"..\" in word:\n",
        "            #     temp=list(filter(lambda elem: elem!='',word.split(\"..\")))\n",
        "            #     if(temp):\n",
        "            #         if(word.endswith(\"..\")):\n",
        "            #             temp=list(map(lambda elem: elem+'..', temp))\n",
        "            #         else:\n",
        "            #             temp=list(map(lambda elem: elem+'..', temp[:-1]))+[temp[-1]]\n",
        "            #     #temp.append(temp1[-1])\n",
        "            elif (list(p_dots.finditer(word))):\n",
        "                matched_spans= list(p_dots.finditer(word)) \n",
        "                temp=[]\n",
        "                next_string_start=0\n",
        "                for matched_span in matched_spans:\n",
        "                    matched_start=matched_span.span()[0]\n",
        "                    this_excerpt=word[next_string_start:matched_start]\n",
        "                    if(this_excerpt):\n",
        "                        temp.append(this_excerpt)\n",
        "                    next_string_start=matched_span.span()[1]\n",
        "                if(next_string_start<len(word)):\n",
        "                    last_excerpt=word[next_string_start:]\n",
        "                    if(last_excerpt):\n",
        "                        temp.append(last_excerpt)\n",
        "            elif \"…\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
        "                if(temp):\n",
        "                    if(word.endswith(\"…\")):\n",
        "                        temp=list(map(lambda elem: elem+'…', temp))\n",
        "                    else:\n",
        "                        temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
        "            else:\n",
        "                #if word not in string.punctuation:\n",
        "                temp=[word]\n",
        "            if(temp):\n",
        "                tempList.append(temp)\n",
        "        tweetWordList=self.flatten(tempList,[])\n",
        "        return tweetWordList\n",
        "\n",
        "    def get_Candidates(self, sequence, CTrie,flag):\n",
        "        #flag: debug_flag\n",
        "        candidateList=[]\n",
        "        left=0\n",
        "        start_node=CTrie\n",
        "        last_cand=\"NAN\"\n",
        "        last_cand_substr=\"\"\n",
        "        reset=False\n",
        "        right=0\n",
        "        while (right < len(sequence)):\n",
        "            # if(flag):\n",
        "            #     print(right)\n",
        "            if(reset):\n",
        "                start_node=CTrie\n",
        "                last_cand_substr=\"\"\n",
        "                left=right\n",
        "            curr_text=sequence[right][0]\n",
        "            curr_pos=[sequence[right][1]]\n",
        "            #normalized curr_text\n",
        "            curr=self.normalize(sequence[right][0])\n",
        "            cand_str=self.normalize(last_cand_substr+\" \"+curr)\n",
        "            cand_str_wPunct=(last_cand_substr+\" \"+curr_text).lower()\n",
        "            last_cand_sequence=sequence[left:(right+1)]\n",
        "            last_cand_text=' '.join(str(e[0]) for e in last_cand_sequence)\n",
        "            last_cand_text_norm=self.normalize(' '.join(str(e[0]) for e in last_cand_sequence))\n",
        "            if(flag):\n",
        "                print(\"==>\",cand_str,last_cand_text_norm)\n",
        "            if((cand_str==last_cand_text_norm)&((curr in start_node.path.keys())|(curr_text.lower() in start_node.path.keys()))):\n",
        "            #if (((curr in start_node.path.keys())&(cand_str==last_cand_text_norm))|(curr_text.lower() in start_node.path.keys())):\n",
        "                if flag:\n",
        "                    print(\"=>\",cand_str,last_cand_text)\n",
        "                reset=False\n",
        "                if (curr_text.lower() in start_node.path.keys()):\n",
        "                    if (start_node.path[curr_text.lower()].value_valid):\n",
        "                        last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                        last_cand_batch=start_node.path[curr_text.lower()].feature_list[-1]\n",
        "                        last_cand=last_cand_text\n",
        "                    elif(curr in start_node.path.keys()):\n",
        "                        if ((start_node.path[curr].value_valid)):\n",
        "                            last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                            last_cand=last_cand_text\n",
        "                            last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                        else:\n",
        "                            if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                                #print(\"hehe\",cand_str)\n",
        "                                right=left\n",
        "                                reset=True\n",
        "                    else:\n",
        "                        if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                            #print(\"hehe\",cand_str)\n",
        "                            right=left\n",
        "                            reset=True\n",
        "                elif ((start_node.path[curr].value_valid)&(cand_str==last_cand_text_norm)):\n",
        "                    # if flag:\n",
        "                    #     print(\"==\",last_cand_text)\n",
        "                    last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                    last_cand=last_cand_text\n",
        "                    last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                else:\n",
        "                    if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                        #print(\"hehe\",cand_str)\n",
        "                        right=left\n",
        "                        reset=True\n",
        "                if((curr_text.lower() in start_node.path.keys())&(cand_str==last_cand_text_norm)):\n",
        "                    start_node=start_node.path[curr_text.lower()]\n",
        "                    last_cand_substr=cand_str_wPunct\n",
        "                else:\n",
        "                    start_node=start_node.path[curr]\n",
        "                    last_cand_substr=cand_str\n",
        "            else:\n",
        "                #print(\"=>\",cand_str,last_cand_text)\n",
        "                if(last_cand!=\"NAN\"):\n",
        "                    candidateList.append((last_cand,last_cand_pos,last_cand_batch))\n",
        "                    last_cand=\"NAN\"\n",
        "                    if(start_node!=CTrie):\n",
        "                        start_node=CTrie\n",
        "                        last_cand_substr=\"\"\n",
        "                        if curr in start_node.path.keys():\n",
        "                            # if(flag):\n",
        "                            #     print(\"here\",curr)\n",
        "                            reset=False\n",
        "                            if start_node.path[curr].value_valid:\n",
        "                                last_cand_text=curr_text\n",
        "                                last_cand_pos=curr_pos\n",
        "                                last_cand=last_cand_text\n",
        "                                last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                            left=right\n",
        "                            start_node=start_node.path[curr]\n",
        "                            last_cand_substr=curr\n",
        "                        else:\n",
        "                            reset=True\n",
        "                    else:\n",
        "                        reset=True\n",
        "                else:\n",
        "                    if(left<right):\n",
        "                        # if(flag):\n",
        "                        #     print(sequence[(left+1):(right+1)])\n",
        "                        #candidateList.extend(self.get_Candidates(sequence[(left+1):(right+1)], CTrie, flag))\n",
        "                        right=left\n",
        "                        # if(flag):\n",
        "                        #     print(\"++\",right)\n",
        "                    reset=True\n",
        "            right+=1\n",
        "        # if(flag):\n",
        "        #     print(last_cand)\n",
        "        if(last_cand!=\"NAN\"):\n",
        "            candidateList.append((last_cand,last_cand_pos,last_cand_batch))\n",
        "        return candidateList\n",
        "\n",
        "\n",
        "    def append_rows(self,df_holder):\n",
        "    \n",
        "        df = pd.DataFrame(df_holder)\n",
        "        #self.data_frame_holder=self.data_frame_holder.append(df,ignore_index=True)\n",
        "        #self.data_frame_holder=self.data_frame_holder.reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def join_token_tuples(self,list_of_tuples):\n",
        "        #print(string.punctuation)\n",
        "        combined_str=(' '.join(tuple[0] for tuple in list_of_tuples)).lstrip(string.punctuation).rstrip(string.punctuation).strip()\n",
        "        combined_pos='*'.join(str(tuple[1]) for tuple in list_of_tuples)\n",
        "        combined_tuple=(combined_str,combined_pos,list_of_tuples[0][2],list_of_tuples[0][3],list_of_tuples[0][4],list_of_tuples[0][5],list_of_tuples[0][6])\n",
        "        return combined_tuple\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def all_capitalized(self,candidate):\n",
        "        strip_op=candidate\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip())\n",
        "        strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
        "        strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "        prep_article_list=prep_list+article_list+self.phase2stopwordList\n",
        "        word_list=strip_op.split()\n",
        "        for i in range(len(word_list)):\n",
        "            word=word_list[i]\n",
        "            if((word[0].isupper())|(word[0].isdigit())):\n",
        "                continue\n",
        "            else:\n",
        "                if(word in prep_article_list):\n",
        "                    if (i!=0):\n",
        "                        continue\n",
        "                    else:\n",
        "                        return False\n",
        "                elif(word in conjoiner):\n",
        "                    continue\n",
        "                else:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "\n",
        "    def check_feature_update(self, candidate_tuple,non_discriminative_flag):\n",
        "        #print(candidate_tuple)\n",
        "        if(non_discriminative_flag):\n",
        "            return 7\n",
        "        candidateText=candidate_tuple[0]\n",
        "        position=candidate_tuple[1]\n",
        "        word_list=candidateText.split()\n",
        "        if candidateText.islower():\n",
        "            return 6\n",
        "        elif candidateText.isupper():\n",
        "            return 5\n",
        "        elif (len(word_list)==1):\n",
        "            #start-of-sentence-check\n",
        "            if self.all_capitalized(candidateText):\n",
        "                if(int(position[0])==0):\n",
        "                    return 4\n",
        "                else:\n",
        "                    return 2\n",
        "            else:\n",
        "                return 3\n",
        "        else:\n",
        "            if(self.all_capitalized(candidateText)):\n",
        "                return 2\n",
        "            else:\n",
        "                return 3\n",
        "\n",
        "    def addEmbedding(self, candidate_tuple,contextual_embedding_vector):\n",
        "\n",
        "        candidateText=candidate_tuple[0]\n",
        "        normalized_candidate=self.normalize(candidateText)\n",
        "\n",
        "        if(normalized_candidate in self.candidateEmbedding_records.keys()):\n",
        "            curr_id = self.candidate_string_freq_dict[normalized_candidate]\n",
        "            self.candidateEmbedding_records[normalized_candidate][curr_id]=(contextual_embedding_vector,)\n",
        "            self.candidateEmbeddingPool[normalized_candidate].append(contextual_embedding_vector.tolist())\n",
        "        else:\n",
        "            curr_id = 0\n",
        "            self.candidateEmbedding_records[normalized_candidate]={curr_id:(contextual_embedding_vector,)}\n",
        "            self.candidateEmbeddingPool[normalized_candidate]=[contextual_embedding_vector.tolist()]\n",
        "        self.candidate_string_freq_dict[normalized_candidate]=curr_id+1\n",
        "        \n",
        "        return curr_id\n",
        "\n",
        "    def extract(self,tweetBaseInput,CTrie,phase2stopwordList,new_or_old):\n",
        "\n",
        "\n",
        "        if(self.counter==0):\n",
        "            #output_queue\n",
        "            self.candidate_string_freq_dict= {} #not mindful of clusters\n",
        "\n",
        "            self.CandidateBase_dict_alt={}\n",
        "            self.candidateEmbeddingDict={}\n",
        "\n",
        "            #new data structures for clustering\n",
        "            self.candidateEmbeddingPool={}\n",
        "            self.candidateEmbedding_records={} # {candidate:{mention_id: <contextual embedding,cluster_id>...},....} multi level dictionary\n",
        "\n",
        "            self.ambiguous_candidate_distanceDict_prev={}\n",
        "            self.partition_dict={}\n",
        "            self.good_candidates=[]\n",
        "            self.bad_candidates=[]\n",
        "            self.ambiguous_candidates=[]\n",
        "\n",
        "            self.aggregator_incomplete_tweets=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.just_converted_tweets=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            #self.data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates'])\n",
        "            self.raw_tweets_for_others=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "\n",
        "            self.accuracy_tuples_prev_batch=[]\n",
        "            self.accuracy_vals=[]\n",
        "            \n",
        "            #frequency_w_decay related information\n",
        "            self.ambiguous_candidates_reintroduction_dict={}\n",
        "\n",
        "            #### other systems\n",
        "            self.accuracy_vals_stanford=[]\n",
        "            self.accuracy_vals_opencalai=[]\n",
        "            self.accuracy_vals_ritter=[]\n",
        "            self.accuracy_vals_neuroner=[]\n",
        "\n",
        "            self.number_of_seen_tweets_per_batch=[]\n",
        "        self.phase2stopwordList=phase2stopwordList\n",
        "        self.number_of_seen_tweets_per_batch.append(len(tweetBaseInput))\n",
        "\n",
        "\n",
        "        #data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "        phase1_holder_holder=[]\n",
        "        phase2_candidates_holder=[]\n",
        "        phase2_unnormalized_candidates_holder=[]\n",
        "        df_holder=[]\n",
        "\n",
        "\n",
        "        combined_list_here=([]+list(cachedStopWords)+chat_word_list+day_list+month_list+article_list+prep_list)\n",
        "        combined_list_filtered=list(filter(lambda word: word not in (prep_list+article_list+month_list+self.phase2stopwordList), combined_list_here))\n",
        "\n",
        "        self.entity_phrase_embedder.eval()\n",
        "        #--------------------------------------PHASE II---------------------------------------------------\n",
        "        for index, row in tweetBaseInput.iterrows():\n",
        "\n",
        "            #phase 1 candidates for one sentence\n",
        "            phase1_holder=[]\n",
        "\n",
        "            tweetText=str(row['TweetSentence'])\n",
        "            tweetWordList = row['tweetwordList']\n",
        "            sentID=str(row['sentID'])\n",
        "            tweetID=str(row['tweetID'])\n",
        "            phase1Candidates=str(row['phase1CandidatesWPositions'])\n",
        "            batch=int(row['entry_batch'])\n",
        "            contextual_embeddings_dict= {ind: embedding for ind, embedding in enumerate(row['contextual_embeddings'])}\n",
        "            \n",
        "            # print('====',tweetID,sentID)\n",
        "            # print('tweetWordList:',tweetWordList)\n",
        "            \n",
        "            non_discriminative_flag=False\n",
        "            phase1CandidatesList=[]\n",
        "\n",
        "            # print('phase1Candidates:',phase1Candidates)\n",
        "\n",
        "            if (phase1Candidates !='nan'):\n",
        "                phase1Raw=phase1Candidates.split(\"||\")\n",
        "                phase1Raw = list(filter(None, phase1Raw))\n",
        "\n",
        "\n",
        "                for entities_with_loc in phase1Raw:\n",
        "                    candidateData=entities_with_loc.split(\"::\")\n",
        "                    entity_to_store=candidateData[0]\n",
        "                    #print(entity_to_store)\n",
        "                    position=candidateData[1]\n",
        "                    #print(position)\n",
        "                    entityType=candidateData[2]\n",
        "                    phase1_holder.append((entity_to_store,entityType,position))\n",
        "                    phase1_holder.clear()\n",
        "                    phase1CandidatesList.append(entity_to_store.lower())\n",
        "\n",
        "                phase1_holder_holder.append(copy.deepcopy(phase1_holder))\n",
        "                \n",
        "\n",
        "            else:\n",
        "                non_discriminative_flag=True\n",
        "                phase1_holder_holder.append([])\n",
        "\n",
        "            \n",
        "            # tweetWordList=self.getWords(tweetText)\n",
        "            # tweetWordList= [(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "            \n",
        "            tweetWordList_stopWords=list(filter (lambda word: ((((word[0].strip()).strip(punct)).lower() in combined_list_filtered)|(word[0].strip() in punct)|(word[0].startswith('@'))|(word[0].startswith('#'))), tweetWordList))\n",
        "\n",
        "            # phase 2 candidate tuples without stopwords for a sentence\n",
        "            c=[(y[0],str(y[1]),tweetID,sentID,'ne',batch,time) for y  in tweetWordList if y not in tweetWordList_stopWords ]\n",
        "            #c=[(y[0],str(y[1])) for y  in tweetWordList if y not in tweetWordList_stopWords ]\n",
        "\n",
        "            \n",
        "            sequences=[]\n",
        "            for k, g in groupby(enumerate(c), lambda element: element[0]-int(element[1][1])):\n",
        "                sequences.append(list(map(itemgetter(1), g)))\n",
        "\n",
        "            # print('phaseII candidates:')\n",
        "            phase2_candidates=[]\n",
        "            phase2_candidates_unnormalized=[]\n",
        "            for sequence in sequences:\n",
        "                seq_candidate_list=self.get_Candidates(sequence, CTrie,False)\n",
        "                if(seq_candidate_list):\n",
        "                    for candidate_tuple in seq_candidate_list:\n",
        "\n",
        "                        # extract candidate token embeddings\n",
        "                        candidate_token_embeddings = torch.stack([contextual_embeddings_dict[int(position)] for position in candidate_tuple[1]])\n",
        "                        assert torch.isnan(candidate_token_embeddings).any() == False\n",
        "                        tensor_inter = torch.mean(candidate_token_embeddings,dim=0)\n",
        "                        assert torch.isnan(tensor_inter).any() == False\n",
        "                        tensor_inter_norm = torch.norm(tensor_inter, p=2,dim=0)\n",
        "                        \n",
        "                        if (not torch.is_nonzero(tensor_inter_norm)):\n",
        "                            print(candidate_tuple, 'adjustement needed')\n",
        "                            tensor_inter = torch.add(tensor_inter,1e-7)\n",
        "                            tensor_inter_norm = torch.norm(tensor_inter, p=2,dim=0)\n",
        "                        \n",
        "\n",
        "                        candidate_tokens_avg_embedding = tensor_inter/tensor_inter_norm\n",
        "                        assert torch.isnan(candidate_tokens_avg_embedding).any() == False\n",
        "\n",
        "                        # !! must unsqueeze at test time with embedder else batchnorm throws error\n",
        "                        candidate_tokens_avg_embedding = candidate_tokens_avg_embedding.unsqueeze(0)\n",
        "                        \n",
        "                        candidate_embedding = (self.entity_phrase_embedder.getEmbedding(candidate_tokens_avg_embedding)).squeeze(0)\n",
        "                        \n",
        "                        assert torch.isnan(candidate_embedding).any() == False\n",
        "\n",
        "                        # print(candidate_tuple,candidate_token_embeddings.shape)\n",
        "                        # print('candidate_embedding:',candidate_embedding.shape)\n",
        "\n",
        "                        #inserts into CandidatePool: pre-clustering\n",
        "                        # if not ((float(batch)<self.counter)&(candidate_tuple[-1]<self.counter)):\n",
        "                        mention_id = self.addEmbedding(candidate_tuple,candidate_embedding)\n",
        "                        # self.update_Candidatedict(candidate_tuple,non_discriminative_flag,candidate_embedding.tolist())\n",
        "\n",
        "                        phase2_candidates.append(self.normalize(candidate_tuple[0])+'||'+str(mention_id))\n",
        "                        phase2_candidates_unnormalized.append(candidate_tuple[0]+'||'+str(mention_id))\n",
        "\n",
        "            phase2_candidates_holder.append(phase2_candidates)\n",
        "            phase2_unnormalized_candidates_holder.append(phase2_candidates_unnormalized)\n",
        "\n",
        "            dict1 = {'entry_batch':batch, 'tweetID':tweetID, 'sentID':sentID, 'TweetSentence':tweetText, 'phase1Candidates':phase1CandidatesList,'2nd Iteration Candidates':phase2_candidates,'2nd Iteration Candidates Unnormalized':phase2_candidates_unnormalized}\n",
        "\n",
        "            df_holder.append(dict1)\n",
        "\n",
        "        # sanity check\n",
        "        for candidate in self.candidate_string_freq_dict.keys():\n",
        "            assert self.candidate_string_freq_dict[candidate] == len(self.candidateEmbedding_records[candidate])\n",
        "            assert self.candidate_string_freq_dict[candidate] == len(self.candidateEmbeddingPool[candidate])\n",
        "\n",
        "        return df_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQix93xPFfAf"
      },
      "source": [
        "## **Running the NER Globalizer engine**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufF8xR7ffYN2"
      },
      "outputs": [],
      "source": [
        "def calculate_f1_ner_engine(tweet_to_sentences_w_annotation, ner_arrays):\n",
        "    \n",
        "    entity_types = ['org','misc', 'loc', 'per']\n",
        "    confusion_matrices = {entity_type: {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0} for entity_type in entity_types}\n",
        "    scores = {entity_type: {'p': 0, 'r': 0, 'f1': 0} for entity_type in entity_types}\n",
        "\n",
        "    out_str = ''\n",
        "    mistyping_count = 0\n",
        "    total_mention_count = 0\n",
        "\n",
        "    annotation_dict={}\n",
        "    output_dict={}\n",
        "\n",
        "    for tweetID in tweet_to_sentences_w_annotation.keys():\n",
        "        unrecovered_annotated_mention_list=[]\n",
        "        annotated_mention_list=tweet_to_sentences_w_annotation[tweetID][1]\n",
        "        output_mentions_list=[]\n",
        "        true_positive_list=[]\n",
        "        idRange=tweet_to_sentences_w_annotation[tweetID][0]\n",
        "\n",
        "        for sentID in range(idRange[0],idRange[1]):\n",
        "            output_mentions_list+=ner_arrays[sentID]\n",
        "        output_mentions_list=[(elem[0],elem[1].lower()) for elem in output_mentions_list]\n",
        "        print(tweetID,annotated_mention_list,output_mentions_list)\n",
        "        out_str+=str(tweetID)+str(annotated_mention_list)+str(output_mentions_list)+'\\n'\n",
        "\n",
        "        total_mention_count+=len(annotated_mention_list)\n",
        "\n",
        "        for annotated_entity in annotated_mention_list:\n",
        "            try:\n",
        "                annotation_dict[annotated_entity]+=1\n",
        "            except KeyError:\n",
        "                annotation_dict[annotated_entity]=1\n",
        "\n",
        "        while(annotated_mention_list):\n",
        "            if(len(output_mentions_list)):\n",
        "                annotated_candidate= annotated_mention_list.pop()\n",
        "                if(annotated_candidate in output_mentions_list):\n",
        "                    output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                    # tp_counter_inner+=1 # for emd\n",
        "                    true_positive_list.append(annotated_candidate)\n",
        "                    # #for ner\n",
        "                    # entity_type=annotated_candidate[1]\n",
        "                    # confusion_matrices[entity_type]['TP']+=1\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "            else:\n",
        "                unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                break\n",
        "        # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "\n",
        "        # fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "        # fp_counter_inner=all_postitive_counter_inner - tp_counter_inner\n",
        "        # print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
        "\n",
        "        print('true positives:',true_positive_list)\n",
        "        print('false positives:',output_mentions_list)\n",
        "        print('false negatives:',unrecovered_annotated_mention_list)\n",
        "\n",
        "        out_str+= 'true positives:'+str(true_positive_list)+'\\n'\n",
        "        out_str+= 'false positives:'+str(output_mentions_list)+'\\n'\n",
        "        out_str+= 'false negatives:'+str(unrecovered_annotated_mention_list)+'\\n'\n",
        "\n",
        "        mistyped_candidates = []\n",
        "        false_positive_candidate_strings = [elem[0] for elem in output_mentions_list]\n",
        "\n",
        "        for mention_tup in true_positive_list:\n",
        "            output_dict[mention_tup] = True\n",
        "            entity_type=mention_tup[1]\n",
        "            if(mention_tup[0]):\n",
        "                confusion_matrices[entity_type]['TP']+=1\n",
        "\n",
        "        for mention_tup in unrecovered_annotated_mention_list:\n",
        "            entity_type=mention_tup[1]\n",
        "            if(mention_tup[0]):\n",
        "                confusion_matrices[entity_type]['FN']+=1\n",
        "\n",
        "        for mention_tup in output_mentions_list:\n",
        "            entity_type=mention_tup[1]\n",
        "            if(mention_tup[0]):\n",
        "                confusion_matrices[entity_type]['FP']+=1\n",
        "\n",
        "        while(unrecovered_annotated_mention_list):\n",
        "            if(false_positive_candidate_strings):\n",
        "                fn_candidate = unrecovered_annotated_mention_list.pop()\n",
        "                if(fn_candidate[0] in false_positive_candidate_strings):\n",
        "                    false_positive_candidate_strings.pop(false_positive_candidate_strings.index(fn_candidate[0]))\n",
        "                    mistyped_candidates.append(fn_candidate)\n",
        "            else:\n",
        "                break\n",
        "        print('mistypings:',mistyped_candidates)\n",
        "        out_str+= 'mistypings:'+str(mistyped_candidates)+'\\n'\n",
        "        mistyping_count+= len(mistyped_candidates)\n",
        "    \n",
        "    # file1 = open(\"btc.txt\", \"w\")\n",
        "    file1 = open(\"tweets_3K.txt\", \"w\")\n",
        "    file1.write(out_str)\n",
        "    file1.close()\n",
        "\n",
        "    freq_bucket = {} \n",
        "    for candidate in annotation_dict.keys():\n",
        "        candidate_freq = annotation_dict[candidate]\n",
        "        flag = False\n",
        "        if(candidate in output_dict):\n",
        "            flag = True\n",
        "        try:\n",
        "            old_tup = freq_bucket[candidate_freq]\n",
        "            if flag:\n",
        "                freq_bucket[candidate_freq] = (old_tup[0]+1,old_tup[1])\n",
        "            else:\n",
        "                freq_bucket[candidate_freq] = (old_tup[0],old_tup[1]+1)\n",
        "        except KeyError:\n",
        "            if flag:\n",
        "                freq_bucket[candidate_freq] = (1,0)\n",
        "            else:\n",
        "                freq_bucket[candidate_freq] = (0,1)\n",
        "    \n",
        "    x_axis=[]\n",
        "    y_axis =[]\n",
        "    cumulative_tp=0\n",
        "    cumulative_annotated=0\n",
        "    freq_bucket_sorted = {k : freq_bucket[k] for k in sorted(freq_bucket)}\n",
        "\n",
        "    maxKey = list(freq_bucket_sorted.keys())[-1]\n",
        "    print(freq_bucket_sorted.keys(),maxKey)\n",
        "\n",
        "    step=0\n",
        "    for key in range(maxKey):\n",
        "        try:\n",
        "            tup = freq_bucket_sorted[key]\n",
        "        except KeyError:\n",
        "            tup = (0,0)\n",
        "\n",
        "        # freq_recall= tup[0]/(tup[0]+tup[1])\n",
        "\n",
        "        if(step==5):\n",
        "            if((cumulative_tp==0)&(cumulative_annotated==0)):\n",
        "                freq_recall= y_axis[-1]\n",
        "            else:\n",
        "                freq_recall= cumulative_tp/cumulative_annotated\n",
        "            # print(key, freq_recall)\n",
        "            x_axis.append(key)\n",
        "            y_axis.append(freq_recall)\n",
        "            step=0\n",
        "            cumulative_tp=0\n",
        "            cumulative_annotated=0\n",
        "\n",
        "\n",
        "        cumulative_tp+=tup[0]\n",
        "        cumulative_annotated+=(tup[0]+tup[1])\n",
        "        # freq_recall= cumulative_tp/cumulative_annotated\n",
        "        step+=1\n",
        "\n",
        "    print(x_axis)\n",
        "    print(y_axis)\n",
        "\n",
        "    # print20=20\n",
        "    for ind, elem in enumerate(x_axis):\n",
        "        # if(print20>0):\n",
        "        print(x_axis[ind],':',y_axis[ind])\n",
        "\n",
        "    print('========Entity Mention Detection========')\n",
        "    print('Total mentions:',str(total_mention_count))\n",
        "    print('Mistyped mentions:',str(mistyping_count))\n",
        "    for entity_type in entity_types:\n",
        "        print(entity_type.upper())\n",
        "        try:\n",
        "            precision = confusion_matrices[entity_type]['TP']/(confusion_matrices[entity_type]['TP']+confusion_matrices[entity_type]['FP'])\n",
        "        except ZeroDivisionError:\n",
        "            precision = 0\n",
        "        try:\n",
        "            recall = confusion_matrices[entity_type]['TP']/(confusion_matrices[entity_type]['TP']+confusion_matrices[entity_type]['FN'])\n",
        "        except ZeroDivisionError:\n",
        "            recall = 0\n",
        "        try:\n",
        "            f_measure = 2*precision*recall/(precision+recall)\n",
        "        except ZeroDivisionError:\n",
        "            f_measure = 0\n",
        "\n",
        "        scores[entity_type]['p'] = precision\n",
        "        scores[entity_type]['r'] = recall\n",
        "        scores[entity_type]['f1'] = f_measure\n",
        "\n",
        "        print('true positives: ',confusion_matrices[entity_type]['TP'])\n",
        "        print('false positives: ',confusion_matrices[entity_type]['FP'])\n",
        "        print('false negatives: ',confusion_matrices[entity_type]['FN'])\n",
        "\n",
        "        print('precision: ',precision)\n",
        "        print('recall: ',recall)\n",
        "        print('f_measure: ',f_measure)\n",
        "\n",
        "        print('------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCdNPuGhrrah"
      },
      "outputs": [],
      "source": [
        "# tweets_unpartitoned=pd.read_csv('data/tweets_3k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_6k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/venezuela.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/covid_2K.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/broad_twitter_corpus.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/billdeblasio.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/roevwade.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/pikapika.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/ripcity.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/billnye.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/wnut17test_ner.csv',sep =',',keep_default_na=False)\n",
        "tweets_unpartitoned=pd.read_csv('data/btctest_ner.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/venezuela_ner.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/covid_2K_ner.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_3k_annotated_ner.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/roevwade_ner.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/billnye_ner.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/pikapika_ner.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/billdeblasio_ner.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/ripcity_ner.csv',sep =',',keep_default_na=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0U9BbL2FhYd"
      },
      "outputs": [],
      "source": [
        "#tokenizer here is the BERT model's tokenizer\n",
        "local_NER_Module= LocalNERModule(sentence_tokenizer, tokenizer, alt_model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAsb7l-0Fly9"
      },
      "outputs": [],
      "source": [
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'wnut17')\n",
        "\n",
        "global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'btc')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'tweets_3k_annotated')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'tweets_6k_annotated')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'venezuela')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'covid_2K')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'billdeblasio')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'ripcity')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'pikapika')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'roevwade')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'billnye')\n",
        "\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'classifier-train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwCB8GpCFqXU"
      },
      "outputs": [],
      "source": [
        "print('Tweets are in memory...')\n",
        "\n",
        "length=len(tweets_unpartitoned)\n",
        "batch_size=length\n",
        "print(length, batch_size)\n",
        "val=math.ceil(length/batch_size)-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7sD_5LtFxVk"
      },
      "outputs": [],
      "source": [
        "tweet_batch = tweets_unpartitoned\n",
        "print(len(tweet_batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5VfP7sdFzu8"
      },
      "outputs": [],
      "source": [
        "tweet_to_sentences_w_annotation={}\n",
        "# df_out_holder_Phase1=[]\n",
        "total_time=0\n",
        "reintroduction_threshold_dummy=0\n",
        "max_batch_value=112\n",
        "z_score=1\n",
        "\n",
        "# for g, tweet_batch in tweets_unpartitoned.groupby(np.arange(length) //batch_size):\n",
        "phaseI_timein=time.time()\n",
        "g=0\n",
        "tuple_of= local_NER_Module.extract(tweet_batch,g)\n",
        "phaseI_timeout=time.time()\n",
        "\n",
        "print('local emd time',(phaseI_timeout-phaseI_timein))\n",
        "\n",
        "tweet_base=tuple_of[0]\n",
        "contextual_embeddings=tuple_of[1]\n",
        "candidate_base=tuple_of[2]\n",
        "elapsedTime= tuple_of[4] - tuple_of[3]\n",
        "phase2stopwordList=tuple_of[5]\n",
        "# print('len of tweet_base = '  len(tweet_base))\n",
        "tweet_to_sentences_w_annotation=tuple_of[6]\n",
        "total_time+=elapsedTime\n",
        "print(elapsedTime,total_time)\n",
        "\n",
        "# df_out_holder_Phase1.append(tweet_base)\n",
        "\n",
        "print ('Produced', g)\n",
        "print(\"**********************************************************\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am_WGnH5F21o"
      },
      "outputs": [],
      "source": [
        "# # #PHASE I CHECKING:\n",
        "# calculate_f1_ner_engine(tweet_to_sentences_w_annotation, local_NER_Module.phaseIpredictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wI-xf3eKfXi"
      },
      "outputs": [],
      "source": [
        "phaseI_candidates = candidate_base.displayTrie(\"\",[])\n",
        "print(str(len(phaseI_candidates))+' candidates from phaseI')\n",
        "print(phaseI_candidates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwzJoGq6F7fy"
      },
      "outputs": [],
      "source": [
        "phaseII_timein=time.time()\n",
        "global_ner_predictions = global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch)\n",
        "phaseII_timeout=time.time()\n",
        "\n",
        "print('global emd time',(phaseII_timeout-phaseII_timein))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMJgl1zFEhBq"
      },
      "outputs": [],
      "source": [
        "calculate_f1_ner_engine(tweet_to_sentences_w_annotation, global_ner_predictions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TZKMubBHEgZM",
        "usDw-ReUiklS",
        "bhnAJCnbjKIm",
        "M5qm2iIc8zzq",
        "UDEYZT668rNp",
        "mK4fkrxjLQZj",
        "47Do-RR4nsYt",
        "Mmj4tewsyPCv",
        "i6a02VCgGRl9",
        "ZQix93xPFfAf",
        "s8D38z3wO5Ia",
        "yZkJ9wBwUYZJ"
      ],
      "machine_shape": "hm",
      "name": "BERTweet-Collective-NER.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}